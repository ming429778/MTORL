{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1481,"status":"ok","timestamp":1690970114664,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"SHePlV70OBS2"},"outputs":[],"source":["# if torch.cuda.is_available():\n","#     print('CUDNN VERSION:',torch.backends.cudnn.version())\n","#     print('Number CUDA Devices:',torch.cuda.device_count())\n","#     print('CUDA Device Name:',torch.cuda.get_device_name(0))\n","#     print('CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6052,"status":"ok","timestamp":1690970120708,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"kUqW9r2qOBS2","outputId":"606cfa3e-2965-43bb-9f91-818746accbc6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on the GPU\n"]}],"source":["import torch\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")  # you can continue going on here, like cuda:1 cuda:2....etc.\n","    print(\"Running on the GPU\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"Running on the CPU\")\n","# torch.cuda.device_count()"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1690973614130,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"iZBy7-2rOBS3"},"outputs":[{"name":"stdout","output_type":"stream","text":["126184\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","Data = pd.read_csv('kuairand_sequence.csv',index_col=0)\n","print(len(Data))\n","# Data = pd.read_csv('kuairand_sequence_user=8000.csv',index_col=0)\n","# data['user_id'].value_counts()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1690973619591,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"3HzbfAYVOBS3","outputId":"2e5a6756-eb74-4e81-f782-0caddeae1081"},"outputs":[{"name":"stdout","output_type":"stream","text":["(6051,)\n"]}],"source":["import torch\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import random\n","from torch.utils.data import Dataset, DataLoader,TensorDataset\n","\n","max_length = 100\n","num_user = len(Data['user_id'].unique())\n","state_feature = len(Data.columns) - 1\n","action_feature = 3\n","\n","uid = Data['user_id'].unique()\n","print(uid.shape)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2991,"status":"ok","timestamp":1690973742240,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"HdF3rl_LOBS4","outputId":"9a25cd7c-7681-4521-a5f4-db7dc0f9d462"},"outputs":[],"source":["State = np.zeros(((num_user,max_length,state_feature)))\n","Action = np.zeros((num_user,max_length,action_feature))\n","Click = np.zeros((num_user,max_length))\n","\n","j = 0\n","for i in uid:\n","    state_sequence = np.array(Data[Data['user_id']==i])[:-1]\n","    # state_sequence = np.array(data[data['user_id']==i].drop(['short','mid','long'],axis=1))\n","    action_sequence = np.array(Data[Data['user_id']==i][['short','mid','long']])[1:]\n","    click_sequence = np.array(Data[Data['user_id']==i]['is_click'])[1:]\n","    # action_sequence = np.array(data[data['user_id']==i][['short','mid','long']])\n","    # print(action_sequence)\n","    state = np.pad(state_sequence,((0,max_length-len(state_sequence)),(0,0)))[:,1:]\n","    action = np.pad(action_sequence,((0,max_length-len(action_sequence)),(0,0)))\n","    click = np.pad(click_sequence,((0,max_length-len(click_sequence))))\n","    # print(click)\n","    State[j] = state\n","    Action[j] = action\n","    Click[j] = click\n","    j += 1\n","\n","# print(len(State))\n","# print(len(Action))\n","# print(len(Click))\n","# print(Click.shape)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":822,"status":"ok","timestamp":1690976535806,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"tPsLnRdtOBS4","outputId":"798634a6-86b6-4797-8422-298b81c046ca"},"outputs":[],"source":["length = 20\n","max_number_traj = 4000\n","\n","number_traj = 0\n","def Select_trajectory(X,Y,Z,number_traj):\n","    Select_states = []\n","    Select_actions = []\n","    Select_clicks = []\n","    # for s in range(number_traj):\n","    while True:\n","        i = random.randint(0,len(X)-1)\n","        select_state = X[i]\n","        select_action = Y[i]\n","        select_click = Z[i]\n","        # print(select_state)\n","        # print(select_action)\n","        for k in range(0,max_length):\n","            # print(select_episode[k])\n","            if select_state[k].any() == 0:\n","                max_episode_length = k\n","                break\n","        # print(k)\n","        # print(max_episode_length)\n","        if max_episode_length >= length + 1:\n","            j = random.randint(0,max_episode_length-length-1)\n","            select_state = select_state[j:j+length]\n","            select_action = select_action[j:j+length]\n","            select_click = select_click[j:j+length]\n","            Select_states.append(select_state)\n","            Select_actions.append(select_action)\n","            Select_clicks.append(select_click)\n","            number_traj += 1\n","        # print(select_trajectory)\n","        if number_traj == max_number_traj:\n","            break\n","    return np.array(Select_states),np.array(Select_actions),np.array(Select_clicks)\n","\n","X , Y , Z = Select_trajectory(State,Action,Click,number_traj)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1690976536378,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"P1xlyTewOBS5"},"outputs":[],"source":["np.random.seed(2023)\n","per = np.random.permutation(X.shape[0])\n","# print(per)\n","X = X[per]\n","Y = Y[per]\n","Z = Z[per]\n","X,Y,Z = torch.from_numpy(X).to(torch.float32).to(device),torch.from_numpy(Y).to(torch.float32).to(device), torch.from_numpy(Z).to(torch.float32).to(device)\n","# print(sum(Y))"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690976536378,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"YB3Wa9SaOBS5","outputId":"c094f597-85a0-49b2-adee-00ee3d42b5d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["3200\n"]}],"source":["def split_data(State, Action, Reward, timestep, input_size, output_size):\n","\n","    # Training set\n","    train_size = int(np.round(0.8 * X.shape[0]))\n","    val_size = int(np.round(0.1 * X.shape[0]))\n","    print(train_size)\n","\n","    # Split training and test set 8:1:1\n","    x_train = X[: train_size, :].reshape(-1, timestep,input_size)\n","    y_train = Y[: train_size].reshape(-1,timestep, output_size)\n","    z_train = Z[: train_size].reshape(-1,timestep, 1)\n","\n","    # x_test = X[train_size:, :].reshape(-1, timestep,input_size)\n","    # y_test = Y[train_size:].reshape(-1,timestep, output_size)\n","    # z_test = Z[train_size:].reshape(-1,timestep, 1)\n","\n","    x_val = X[train_size:train_size+val_size, :].reshape(-1, timestep,input_size)\n","    y_val = Y[train_size:train_size+val_size].reshape(-1,timestep, output_size)\n","    z_val = Z[train_size:train_size+val_size].reshape(-1,timestep, 1) \n","\n","    x_test = X[train_size+val_size:, :].reshape(-1, timestep,input_size)\n","    y_test = Y[train_size+val_size:].reshape(-1,timestep, output_size)\n","    z_test = Z[train_size+val_size:].reshape(-1,timestep, 1)\n","\n","    return [x_train, y_train, z_train, x_val, y_val, z_val, x_test, y_test, z_test]\n","\n","# State, Action, Reward(is_click)\n","X1,Y1,Z1,X2,Y2,Z2,X3,Y3,Z3 = split_data(X,Y,Z,length,state_feature,action_feature)\n","# print(X1.shape)\n","# print(Y1.shape)\n","# print(Z1.shape)\n","# X1,Y1=torch.from_numpy(X1).to(torch.float32).to(device),torch.from_numpy(Y1).to(torch.float32).to(device)\n","# X2,Y2=torch.from_numpy(X2).to(torch.float32).to(device),torch.from_numpy(Y2).to(torch.float32).to(device)\n","# Z1,Z2=torch.from_numpy(Z1).to(torch.float32).to(device),torch.from_numpy(Z2).to(torch.float32).to(device)\n","train_ids = TensorDataset(X1,Y1,Z1)\n","val_ids = TensorDataset(X2,Y2,Z2)\n","test_ids = TensorDataset(X3,Y3,Z3)\n","# print(test_ids[0])"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([512, 512, 20])\n"]}],"source":["\n","from pytorch_tcn.tcn import TCN\n","x = torch.ones(512,state_feature,length)\n","model = TCN(state_feature,[512,512],causal=True)\n","print(model(x).shape)\n","# model = TCN(\n","#     num_inputs: int,\n","#     num_channels: ArrayLike,\n","#     kernel_size: int = 4,\n","#     dilations: Optional[ ArrayLike ] = None,\n","#     dilaton_reset: Optional[ int ] = None,\n","#     dropout: float = 0.1,\n","#     causal: bool = True,\n","#     use_norm: str = 'weight_norm',\n","#     activation: str = 'relu',\n","#     kernel_initializer: str = 'xavier_uniform',\n","#     use_skip_connections: bool = False,\n","#     input_shape: str = 'NCL',\n","# )"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690976536379,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"gxuxFJ5sOBS6"},"outputs":[],"source":["class MTORL(nn.Module):\n","    def __init__(self, feature_size, hidden_size, num_layers, output_size):\n","        super(MTORL, self).__init__()\n","        self.hidden_size = hidden_size  # hiddensize\n","        self.num_layers = num_layers  # gru layer\n","        # self.embedding = nn.Linear(feature_size,hidden_size)\n","        # self.gru = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)\n","        self.gru = nn.GRU(feature_size, hidden_size, num_layers, batch_first=True)\n","        # self.lstm = nn.LSTM(feature_size, hidden_size, num_layers, batch_first=True)\n","        self.hidden = nn.Linear(hidden_size, hidden_size)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","        self.fc_click = nn.Linear(hidden_size, 1)\n","        # self.fc1 = nn.Linear(hidden_size, fc_hidden_size)\n","        # self.fc2 = nn.Linear(fc_hidden_size, output_size)\n","        self.sigmoid = nn.Sigmoid()\n","        self.ReLU = nn.ReLU()\n","        self.LeakyReLU = nn.LeakyReLU()\n","        self.BN = nn.BatchNorm1d(8)\n","        self.LayerNorm = nn.LayerNorm(hidden_size)\n","        self.query = nn.Linear(hidden_size, hidden_size)\n","        self.key = nn.Linear(hidden_size, hidden_size)\n","        self.value = nn.Linear(hidden_size, hidden_size)\n","        self.sqrt_size = np.sqrt(hidden_size)\n","        self.softmax = nn.Softmax(dim=-1)\n","        self.tcn = TCN(state_feature,[hidden_size,hidden_size])\n","        # self.multihead = nn.MultiheadAttention(hidden_size,num_heads,dropout=0.1)\n","    def forward(self, x, hidden=None):\n","        batch_size = x.shape[0] #  batchsize\n","\n","        # # initial hidden state\n","        # if hidden is None:\n","        #     h_0 = x.data.new(self.num_layers, batch_size, self.hidden_size).fill_(0).float()\n","        # else:\n","        #     h_0 = hidden\n","        # # GRU operation\n","        # output, h_0 = self.gru(x, h_0)\n","\n","        # TCN\n","        x = x.permute(0, 2, 1)\n","        output = self.tcn(x)\n","        output = output.permute(0, 2, 1)\n","\n","        # # Batch Normalization\n","        # output = self.BN(output)\n","\n","        # click_prob = self.LeakyReLU(GRU_output)\n","        # click_prob = click_prob.transpose(0,1)\n","        click_prob = output.transpose(0,1)\n","\n","        click_prob = self.hidden(click_prob)\n","        click_prob = self.LeakyReLU(click_prob)\n","\n","        click_prob = self.fc_click(click_prob)\n","        click_prob = self.sigmoid(click_prob)\n","\n","        # Attention Layer\n","        query_layer = self.query(output)\n","        key_layer = self.key(output).permute(0, 2, 1)\n","        value_layer = self.value(output)\n","        attention_scores = torch.matmul(query_layer, key_layer)\n","        attention_scores = attention_scores / self.sqrt_size\n","\n","        # Mask\n","        mask = (torch.triu(torch.ones(length, length)) == 0).transpose(0,1).to(device)\n","        attention_scores = attention_scores.masked_fill(mask, value=torch.tensor(-1e9))\n","        # print(attention_scores)\n","\n","        attention_scores = F.dropout(attention_scores, p=0.2)\n","        attention_probs = self.softmax(attention_scores)\n","        output = torch.matmul(attention_probs, value_layer)\n","\n","        # ReLU/LeakyReLU activation\n","        # output = self.ReLU(output)\n","        output = self.LeakyReLU(output)\n","        # output = self.LeakyReLU(GRU_output)\n","\n","\n","        # # Multihead attention\n","        # query_layer = self.query(GRU_output)\n","        # key_layer = self.key(GRU_output)\n","        # value_layer = self.value(GRU_output)\n","        # # print(query_layer.shape)\n","        # output, _ = self.multihead(query_layer,key_layer,value_layer)\n","\n","        # transpose dim 1,2 to get shape (timestep, batch_size, hidden_dim)\n","        output = output.transpose(0,1)\n","\n","        # # Add&Norm (1)\n","        output = F.dropout(self.LeakyReLU(self.hidden(output)), p=0.2) + output\n","        output = self.LayerNorm(output)\n","        output = F.dropout(self.LeakyReLU(self.hidden(output)), p=0.2) + output\n","        output = self.LayerNorm(output)\n","\n","        # # Add&Norm (2)\n","        # output = self.LeakyReLU(self.hidden(output)) + output\n","        # output = self.LayerNorm(output)\n","        # output = F.dropout(output, p=0.2)\n","        # output = self.LeakyReLU(self.hidden(output)) + output\n","        # output = self.LayerNorm(output)\n","    \n","        # # Add&Norm (3)\n","        # output = F.dropout(output, p=0.2) + GRU_output.transpose(0,1)\n","        # output = self.LayerNorm(output)\n","        # output = F.dropout(self.hidden(self.LeakyReLU(self.hidden(output))), p=0.2) + output\n","        # output = self.LayerNorm(output)\n","\n","        output = self.fc(output)  # (batch_size, timestep, output_size)\n","\n","        # Softmax convert to [0,1]\n","        action_prob = F.softmax(output,dim=2)\n","\n","        # all timestep output\n","        return action_prob, click_prob\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690976536379,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"PwFFlRoiOBS6"},"outputs":[],"source":["Batchsize = 512\n","# data_train_loader = DataLoader(dataset=train_ids, batch_size=Batchsize, shuffle=True)\n","data_train_loader = DataLoader(dataset=train_ids, batch_size=Batchsize, shuffle=False)\n","data_val_loader = DataLoader(dataset=val_ids, batch_size=1, shuffle=False,drop_last=False)\n","data_test_loader = DataLoader(dataset=test_ids, batch_size=1, shuffle=False,drop_last=False)\n","# data_full_loader = DataLoader(dataset=data_ids, batch_size=1, shuffle=True)\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690976536379,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"5ulgrq9e6DgS"},"outputs":[],"source":["def shannon_entropy(x):\n","  p = x\n","  logp = torch.log2(p)\n","  # print(p)\n","  # print(logp)\n","  entropy = - torch.sum(p*logp,dim=-1)\n","  return entropy"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":122170,"status":"ok","timestamp":1690978204312,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"3qztBjLjOBS6","outputId":"f7e713e5-0d66-4c29-a488-d4a2910d4512"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\39223\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n","  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"]},{"name":"stdout","output_type":"stream","text":["Loss of episode 0 = 23.706306\n","Loss of episode 1 = 23.068487\n","Loss of episode 2 = 22.574545\n","Loss of episode 3 = 22.121094\n","Loss of episode 4 = 21.679228\n","Loss of episode 5 = 21.578398\n","Loss of episode 6 = 21.542824\n","Loss of episode 7 = 21.52562\n","Loss of episode 8 = 21.514656\n","Loss of episode 9 = 21.508896\n","Loss of episode 10 = 21.505146\n","Loss of episode 11 = 21.504684\n","Loss of episode 12 = 21.504375\n","Loss of episode 13 = 21.50322\n","Loss of episode 14 = 21.50302\n","Loss of episode 15 = 21.502083\n","Loss of episode 16 = 21.500923\n","Loss of episode 17 = 21.501053\n","Loss of episode 18 = 21.499077\n","Loss of episode 19 = 21.499954\n","Loss of episode 20 = 21.50008\n","Loss of episode 21 = 21.499344\n","Loss of episode 22 = 21.498684\n","Loss of episode 23 = 21.498083\n","Loss of episode 24 = 21.497536\n","Loss of episode 25 = 21.497753\n","Loss of episode 26 = 21.49742\n","Loss of episode 27 = 21.497437\n","Loss of episode 28 = 21.496603\n","Loss of episode 29 = 21.49691\n","Loss of episode 30 = 21.496372\n","Loss of episode 31 = 21.496344\n","Loss of episode 32 = 21.49745\n","Loss of episode 33 = 21.496664\n","Loss of episode 34 = 21.49656\n","Loss of episode 35 = 21.496717\n","Loss of episode 36 = 21.496014\n","Loss of episode 37 = 21.496052\n","Loss of episode 38 = 21.495365\n","Loss of episode 39 = 21.496208\n","Loss of episode 40 = 21.496237\n","Loss of episode 41 = 21.495968\n","Loss of episode 42 = 21.495077\n","Loss of episode 43 = 21.49553\n","Loss of episode 44 = 21.495785\n","Loss of episode 45 = 21.495296\n","Loss of episode 46 = 21.495361\n","Loss of episode 47 = 21.495275\n","Loss of episode 48 = 21.495464\n","Loss of episode 49 = 21.49542\n","Loss of episode 50 = 21.494795\n","Loss of episode 51 = 21.4951\n","Loss of episode 52 = 21.49482\n","Loss of episode 53 = 21.495129\n","Loss of episode 54 = 21.495167\n","Loss of episode 55 = 21.49498\n","Loss of episode 56 = 21.494764\n","Loss of episode 57 = 21.49496\n","Loss of episode 58 = 21.495121\n","Loss of episode 59 = 21.494743\n","Loss of episode 60 = 21.494919\n","Loss of episode 61 = 21.494911\n","Loss of episode 62 = 21.495062\n","Loss of episode 63 = 21.494968\n","Loss of episode 64 = 21.494492\n","Loss of episode 65 = 21.494884\n","Loss of episode 66 = 21.49487\n","Loss of episode 67 = 21.494665\n","Loss of episode 68 = 21.494436\n","Loss of episode 69 = 21.494686\n","Loss of episode 70 = 21.4949\n","Loss of episode 71 = 21.494688\n","Loss of episode 72 = 21.494766\n","Loss of episode 73 = 21.494558\n","Loss of episode 74 = 21.494406\n","Loss of episode 75 = 21.49446\n","Loss of episode 76 = 21.49426\n","Loss of episode 77 = 21.494911\n","Loss of episode 78 = 21.494526\n","Loss of episode 79 = 21.494392\n","Loss of episode 80 = 21.49487\n","Loss of episode 81 = 21.494503\n","Loss of episode 82 = 21.494566\n","Loss of episode 83 = 21.494545\n","Loss of episode 84 = 21.49452\n","Loss of episode 85 = 21.494452\n","Loss of episode 86 = 21.494568\n","Loss of episode 87 = 21.494755\n","Loss of episode 88 = 21.494549\n","Loss of episode 89 = 21.494484\n","Loss of episode 90 = 21.494461\n","Loss of episode 91 = 21.494488\n","Loss of episode 92 = 21.49451\n","Loss of episode 93 = 21.49437\n","Loss of episode 94 = 21.49438\n","Loss of episode 95 = 21.494255\n","Loss of episode 96 = 21.494282\n","Loss of episode 97 = 21.494427\n","Loss of episode 98 = 21.494528\n","Loss of episode 99 = 21.49464\n","Loss of episode 100 = 21.494352\n","Loss of episode 101 = 21.494087\n","Loss of episode 102 = 21.494453\n","Loss of episode 103 = 21.494621\n","Loss of episode 104 = 21.494373\n","Loss of episode 105 = 21.494246\n","Loss of episode 106 = 21.49419\n","Loss of episode 107 = 21.494223\n","Loss of episode 108 = 21.494354\n","Loss of episode 109 = 21.494438\n","Loss of episode 110 = 21.4943\n","Loss of episode 111 = 21.494293\n","Loss of episode 112 = 21.494167\n","Loss of episode 113 = 21.494223\n","Loss of episode 114 = 21.494293\n","Loss of episode 115 = 21.494175\n","Loss of episode 116 = 21.494175\n","Loss of episode 117 = 21.49422\n","Loss of episode 118 = 21.494047\n","Loss of episode 119 = 21.494041\n","Loss of episode 120 = 21.493927\n","Loss of episode 121 = 21.494015\n","Loss of episode 122 = 21.494015\n","Loss of episode 123 = 21.49401\n","Loss of episode 124 = 21.493841\n","Loss of episode 125 = 21.493711\n","Loss of episode 126 = 21.493668\n","Loss of episode 127 = 21.493725\n","Loss of episode 128 = 21.49371\n","Loss of episode 129 = 21.493315\n","Loss of episode 130 = 21.493093\n","Loss of episode 131 = 21.493187\n","Loss of episode 132 = 21.493134\n","Loss of episode 133 = 21.492847\n","Loss of episode 134 = 21.493696\n","Loss of episode 135 = 21.493366\n","Loss of episode 136 = 21.492928\n","Loss of episode 137 = 21.492603\n","Loss of episode 138 = 21.49226\n","Loss of episode 139 = 21.492102\n","Loss of episode 140 = 21.49172\n","Loss of episode 141 = 21.491552\n","Loss of episode 142 = 21.491262\n","Loss of episode 143 = 21.49115\n","Loss of episode 144 = 21.491573\n","Loss of episode 145 = 21.491898\n","Loss of episode 146 = 21.49147\n","Loss of episode 147 = 21.49088\n","Loss of episode 148 = 21.490154\n","Loss of episode 149 = 21.489508\n","Loss of episode 150 = 21.489422\n","Loss of episode 151 = 21.488503\n","Loss of episode 152 = 21.488197\n","Loss of episode 153 = 21.4885\n","Loss of episode 154 = 21.48846\n","Loss of episode 155 = 21.488594\n","Loss of episode 156 = 21.489063\n","Loss of episode 157 = 21.488285\n","Loss of episode 158 = 21.48798\n","Loss of episode 159 = 21.487396\n","Loss of episode 160 = 21.48764\n","Loss of episode 161 = 21.486729\n","Loss of episode 162 = 21.486065\n","Loss of episode 163 = 21.486378\n","Loss of episode 164 = 21.486866\n","Loss of episode 165 = 21.487814\n","Loss of episode 166 = 21.485065\n","Loss of episode 167 = 21.483337\n","Loss of episode 168 = 21.48243\n","Loss of episode 169 = 21.481756\n","Loss of episode 170 = 21.48129\n","Loss of episode 171 = 21.48001\n","Loss of episode 172 = 21.479347\n","Loss of episode 173 = 21.4786\n","Loss of episode 174 = 21.478302\n","Loss of episode 175 = 21.477806\n","Loss of episode 176 = 21.47665\n","Loss of episode 177 = 21.47537\n","Loss of episode 178 = 21.47725\n","Loss of episode 179 = 21.48238\n","Loss of episode 180 = 21.486464\n","Loss of episode 181 = 21.479055\n","Loss of episode 182 = 21.474762\n","Loss of episode 183 = 21.471241\n","Loss of episode 184 = 21.467625\n","Loss of episode 185 = 21.462322\n","Loss of episode 186 = 21.460318\n","Loss of episode 187 = 21.458641\n","Loss of episode 188 = 21.456137\n","Loss of episode 189 = 21.454807\n","Loss of episode 190 = 21.44872\n","Loss of episode 191 = 21.445486\n","Loss of episode 192 = 21.442017\n","Loss of episode 193 = 21.444965\n","Loss of episode 194 = 21.438904\n","Loss of episode 195 = 21.435247\n","Loss of episode 196 = 21.431583\n","Loss of episode 197 = 21.429775\n","Loss of episode 198 = 21.428062\n","Loss of episode 199 = 21.425854\n","Loss of episode 200 = 21.425276\n","Loss of episode 201 = 21.422789\n","Loss of episode 202 = 21.421484\n","Loss of episode 203 = 21.420057\n","Loss of episode 204 = 21.417265\n","Loss of episode 205 = 21.40949\n","Loss of episode 206 = 21.40335\n","Loss of episode 207 = 21.39346\n","Loss of episode 208 = 21.386488\n","Loss of episode 209 = 21.3759\n","Loss of episode 210 = 21.367682\n","Loss of episode 211 = 21.361265\n","Loss of episode 212 = 21.355045\n","Loss of episode 213 = 21.346737\n","Loss of episode 214 = 21.346024\n","Loss of episode 215 = 21.338223\n","Loss of episode 216 = 21.323153\n","Loss of episode 217 = 21.31155\n","Loss of episode 218 = 21.32087\n","Loss of episode 219 = 21.306055\n","Loss of episode 220 = 21.29309\n","Loss of episode 221 = 21.28046\n","Loss of episode 222 = 21.295181\n","Loss of episode 223 = 21.291924\n","Loss of episode 224 = 21.260448\n","Loss of episode 225 = 21.28426\n","Loss of episode 226 = 21.250212\n","Loss of episode 227 = 21.22241\n","Loss of episode 228 = 21.218618\n","Loss of episode 229 = 21.185093\n","Loss of episode 230 = 21.16912\n","Loss of episode 231 = 21.174076\n","Loss of episode 232 = 21.184664\n","Loss of episode 233 = 21.231054\n","Loss of episode 234 = 21.127277\n","Loss of episode 235 = 21.106335\n","Loss of episode 236 = 21.075418\n","Loss of episode 237 = 21.046505\n","Loss of episode 238 = 21.009583\n","Loss of episode 239 = 21.019196\n","Loss of episode 240 = 21.04742\n","Loss of episode 241 = 20.939083\n","Loss of episode 242 = 20.930069\n","Loss of episode 243 = 20.945057\n","Loss of episode 244 = 20.896378\n","Loss of episode 245 = 20.862953\n","Loss of episode 246 = 20.825306\n","Loss of episode 247 = 20.795607\n","Loss of episode 248 = 20.764242\n","Loss of episode 249 = 20.75593\n","Loss of episode 250 = 20.74887\n","Loss of episode 251 = 20.788641\n","Loss of episode 252 = 20.744213\n","Loss of episode 253 = 20.67898\n","Loss of episode 254 = 20.637459\n","Loss of episode 255 = 20.623806\n","Loss of episode 256 = 20.605446\n","Loss of episode 257 = 20.594564\n","Loss of episode 258 = 20.59628\n","Loss of episode 259 = 20.606506\n","Loss of episode 260 = 20.578682\n","Loss of episode 261 = 20.54657\n","Loss of episode 262 = 20.504086\n","Loss of episode 263 = 20.48228\n","Loss of episode 264 = 20.457668\n","Loss of episode 265 = 20.436562\n","Loss of episode 266 = 20.417793\n","Loss of episode 267 = 20.409262\n","Loss of episode 268 = 20.406677\n","Loss of episode 269 = 20.383558\n","Loss of episode 270 = 20.364723\n","Loss of episode 271 = 20.354774\n","Loss of episode 272 = 20.32817\n","Loss of episode 273 = 20.327112\n","Loss of episode 274 = 20.330786\n","Loss of episode 275 = 20.309261\n","Loss of episode 276 = 20.287077\n","Loss of episode 277 = 20.278652\n","Loss of episode 278 = 20.26279\n","Loss of episode 279 = 20.257208\n","Loss of episode 280 = 20.239618\n","Loss of episode 281 = 20.233027\n","Loss of episode 282 = 20.221092\n","Loss of episode 283 = 20.20849\n","Loss of episode 284 = 20.1859\n","Loss of episode 285 = 20.175106\n","Loss of episode 286 = 20.176939\n","Loss of episode 287 = 20.174904\n","Loss of episode 288 = 20.161552\n","Loss of episode 289 = 20.152702\n","Loss of episode 290 = 20.144552\n","Loss of episode 291 = 20.139128\n","Loss of episode 292 = 20.12872\n","Loss of episode 293 = 20.12022\n","Loss of episode 294 = 20.138859\n","Loss of episode 295 = 20.146496\n","Loss of episode 296 = 20.142391\n","Loss of episode 297 = 20.112343\n","Loss of episode 298 = 20.107292\n","Loss of episode 299 = 20.089792\n","Loss of episode 300 = 20.07794\n","Loss of episode 301 = 20.068958\n","Loss of episode 302 = 20.071007\n","Loss of episode 303 = 20.067245\n","Loss of episode 304 = 20.061712\n","Loss of episode 305 = 20.050608\n","Loss of episode 306 = 20.053387\n","Loss of episode 307 = 20.100391\n","Loss of episode 308 = 20.084358\n","Loss of episode 309 = 20.052715\n","Loss of episode 310 = 20.045101\n","Loss of episode 311 = 20.011908\n","Loss of episode 312 = 19.995857\n","Loss of episode 313 = 19.987755\n","Loss of episode 314 = 19.986975\n","Loss of episode 315 = 19.977524\n","Loss of episode 316 = 19.97905\n","Loss of episode 317 = 19.981178\n","Loss of episode 318 = 19.97435\n","Loss of episode 319 = 19.965439\n","Loss of episode 320 = 19.962936\n","Loss of episode 321 = 19.96131\n","Loss of episode 322 = 19.957066\n","Loss of episode 323 = 19.953642\n","Loss of episode 324 = 19.96062\n","Loss of episode 325 = 19.942253\n","Loss of episode 326 = 19.940998\n","Loss of episode 327 = 19.937708\n","Loss of episode 328 = 19.93525\n","Loss of episode 329 = 19.961452\n","Loss of episode 330 = 19.94251\n","Loss of episode 331 = 19.921799\n","Loss of episode 332 = 19.921585\n","Loss of episode 333 = 19.916004\n","Loss of episode 334 = 19.916752\n","Loss of episode 335 = 19.905014\n","Loss of episode 336 = 19.90682\n","Loss of episode 337 = 19.90341\n","Loss of episode 338 = 19.900436\n","Loss of episode 339 = 19.892553\n","Loss of episode 340 = 19.89605\n","Loss of episode 341 = 19.89022\n","Loss of episode 342 = 19.885048\n","Loss of episode 343 = 19.88881\n","Loss of episode 344 = 19.885778\n","Loss of episode 345 = 19.890514\n","Loss of episode 346 = 19.878584\n","Loss of episode 347 = 19.873013\n","Loss of episode 348 = 19.868149\n","Loss of episode 349 = 19.862186\n","Loss of episode 350 = 19.860584\n","Loss of episode 351 = 19.858433\n","Loss of episode 352 = 19.855803\n","Loss of episode 353 = 19.85627\n","Loss of episode 354 = 19.854378\n","Loss of episode 355 = 19.848381\n","Loss of episode 356 = 19.859642\n","Loss of episode 357 = 19.850256\n","Loss of episode 358 = 19.855001\n","Loss of episode 359 = 19.855896\n","Loss of episode 360 = 19.878399\n","Loss of episode 361 = 19.851482\n","Loss of episode 362 = 19.862112\n","Loss of episode 363 = 19.855684\n","Loss of episode 364 = 19.846472\n","Loss of episode 365 = 19.871643\n","Loss of episode 366 = 19.896225\n","Loss of episode 367 = 19.94097\n","Loss of episode 368 = 19.95585\n","Loss of episode 369 = 20.027496\n","Loss of episode 370 = 20.087818\n","Loss of episode 371 = 20.03962\n","Loss of episode 372 = 19.969658\n","Loss of episode 373 = 19.927702\n","Loss of episode 374 = 19.89804\n","Loss of episode 375 = 19.859863\n","Loss of episode 376 = 19.840805\n","Loss of episode 377 = 19.82277\n","Loss of episode 378 = 19.816021\n","Loss of episode 379 = 19.823044\n","Loss of episode 380 = 19.826206\n","Loss of episode 381 = 19.812653\n","Loss of episode 382 = 19.815216\n","Loss of episode 383 = 19.80325\n","Loss of episode 384 = 19.784996\n","Loss of episode 385 = 19.7784\n","Loss of episode 386 = 19.775806\n","Loss of episode 387 = 19.776571\n","Loss of episode 388 = 19.782776\n","Loss of episode 389 = 19.792828\n","Loss of episode 390 = 19.804268\n","Loss of episode 391 = 19.792324\n","Loss of episode 392 = 19.796911\n","Loss of episode 393 = 19.79165\n","Loss of episode 394 = 19.806423\n","Loss of episode 395 = 19.798807\n","Loss of episode 396 = 19.78146\n","Loss of episode 397 = 19.771948\n","Loss of episode 398 = 19.763014\n","Loss of episode 399 = 19.766302\n","Loss of episode 400 = 19.766134\n","Loss of episode 401 = 19.767462\n","Loss of episode 402 = 19.769093\n","Loss of episode 403 = 19.760607\n","Loss of episode 404 = 19.763153\n","Loss of episode 405 = 19.78579\n","Loss of episode 406 = 19.791237\n","Loss of episode 407 = 19.802452\n","Loss of episode 408 = 19.801342\n","Loss of episode 409 = 19.807829\n","Loss of episode 410 = 19.821999\n","Loss of episode 411 = 19.838362\n","Loss of episode 412 = 19.836462\n","Loss of episode 413 = 19.86079\n","Loss of episode 414 = 19.880022\n","Loss of episode 415 = 19.889479\n","Loss of episode 416 = 19.891542\n","Loss of episode 417 = 19.856848\n","Loss of episode 418 = 19.799131\n","Loss of episode 419 = 19.760086\n","Loss of episode 420 = 19.7387\n","Loss of episode 421 = 19.73629\n","Loss of episode 422 = 19.73665\n","Loss of episode 423 = 19.734358\n","Loss of episode 424 = 19.732925\n","Loss of episode 425 = 19.72081\n","Loss of episode 426 = 19.71182\n","Loss of episode 427 = 19.71183\n","Loss of episode 428 = 19.708494\n","Loss of episode 429 = 19.706306\n","Loss of episode 430 = 19.704035\n","Loss of episode 431 = 19.700974\n","Loss of episode 432 = 19.704166\n","Loss of episode 433 = 19.700222\n","Loss of episode 434 = 19.705894\n","Loss of episode 435 = 19.709227\n","Loss of episode 436 = 19.71594\n","Loss of episode 437 = 19.713165\n","Loss of episode 438 = 19.733227\n","Loss of episode 439 = 19.748882\n","Loss of episode 440 = 19.751328\n","Loss of episode 441 = 19.728897\n","Loss of episode 442 = 19.72216\n","Loss of episode 443 = 19.725126\n","Loss of episode 444 = 19.7204\n","Loss of episode 445 = 19.708548\n","Loss of episode 446 = 19.70261\n","Loss of episode 447 = 19.696873\n","Loss of episode 448 = 19.69857\n","Loss of episode 449 = 19.692944\n","Loss of episode 450 = 19.691029\n","Loss of episode 451 = 19.69558\n","Loss of episode 452 = 19.6874\n","Loss of episode 453 = 19.683784\n","Loss of episode 454 = 19.68724\n","Loss of episode 455 = 19.694298\n","Loss of episode 456 = 19.706512\n","Loss of episode 457 = 19.706562\n","Loss of episode 458 = 19.694885\n","Loss of episode 459 = 19.684711\n","Loss of episode 460 = 19.68569\n","Loss of episode 461 = 19.694294\n","Loss of episode 462 = 19.68843\n","Loss of episode 463 = 19.685537\n","Loss of episode 464 = 19.690506\n","Loss of episode 465 = 19.686232\n","Loss of episode 466 = 19.688488\n","Loss of episode 467 = 19.692282\n","Loss of episode 468 = 19.69461\n","Loss of episode 469 = 19.6861\n","Loss of episode 470 = 19.671488\n","Loss of episode 471 = 19.66827\n","Loss of episode 472 = 19.668606\n","Loss of episode 473 = 19.676495\n","Loss of episode 474 = 19.670351\n","Loss of episode 475 = 19.66869\n","Loss of episode 476 = 19.67038\n","Loss of episode 477 = 19.669094\n","Loss of episode 478 = 19.667103\n","Loss of episode 479 = 19.664446\n","Loss of episode 480 = 19.672104\n","Loss of episode 481 = 19.66944\n","Loss of episode 482 = 19.678968\n","Loss of episode 483 = 19.676115\n","Loss of episode 484 = 19.680332\n","Loss of episode 485 = 19.682194\n","Loss of episode 486 = 19.702951\n","Loss of episode 487 = 19.715263\n","Loss of episode 488 = 19.689327\n","Loss of episode 489 = 19.679516\n","Loss of episode 490 = 19.67846\n","Loss of episode 491 = 19.65696\n","Loss of episode 492 = 19.645065\n","Loss of episode 493 = 19.644272\n","Loss of episode 494 = 19.643776\n","Loss of episode 495 = 19.641026\n","Loss of episode 496 = 19.641361\n","Loss of episode 497 = 19.640078\n","Loss of episode 498 = 19.643597\n","Loss of episode 499 = 19.63514\n","Loss of episode 500 = 19.642538\n","Loss of episode 501 = 19.645662\n","Loss of episode 502 = 19.643307\n","Loss of episode 503 = 19.644161\n","Loss of episode 504 = 19.63591\n","Loss of episode 505 = 19.639322\n","Loss of episode 506 = 19.631802\n","Loss of episode 507 = 19.634026\n","Loss of episode 508 = 19.636345\n","Loss of episode 509 = 19.637217\n","Loss of episode 510 = 19.641266\n","Loss of episode 511 = 19.629719\n","Loss of episode 512 = 19.629755\n","Loss of episode 513 = 19.631151\n","Loss of episode 514 = 19.63234\n","Loss of episode 515 = 19.630634\n","Loss of episode 516 = 19.636059\n","Loss of episode 517 = 19.632214\n","Loss of episode 518 = 19.63773\n","Loss of episode 519 = 19.64167\n","Loss of episode 520 = 19.645775\n","Loss of episode 521 = 19.646732\n","Loss of episode 522 = 19.65736\n","Loss of episode 523 = 19.658718\n","Loss of episode 524 = 19.648865\n","Loss of episode 525 = 19.652338\n","Loss of episode 526 = 19.652906\n","Loss of episode 527 = 19.65108\n","Loss of episode 528 = 19.65512\n","Loss of episode 529 = 19.64785\n","Loss of episode 530 = 19.652174\n","Loss of episode 531 = 19.654549\n","Loss of episode 532 = 19.644764\n","Loss of episode 533 = 19.631842\n","Loss of episode 534 = 19.625889\n","Loss of episode 535 = 19.62714\n","Loss of episode 536 = 19.62437\n","Loss of episode 537 = 19.613903\n","Loss of episode 538 = 19.62207\n","Loss of episode 539 = 19.616814\n","Loss of episode 540 = 19.619534\n","Loss of episode 541 = 19.613728\n","Loss of episode 542 = 19.609385\n","Loss of episode 543 = 19.60443\n","Loss of episode 544 = 19.605429\n","Loss of episode 545 = 19.604214\n","Loss of episode 546 = 19.606014\n","Loss of episode 547 = 19.604685\n","Loss of episode 548 = 19.605843\n","Loss of episode 549 = 19.603386\n","Loss of episode 550 = 19.606865\n","Loss of episode 551 = 19.603703\n","Loss of episode 552 = 19.600765\n","Loss of episode 553 = 19.610899\n","Loss of episode 554 = 19.604809\n","Loss of episode 555 = 19.604733\n","Loss of episode 556 = 19.60772\n","Loss of episode 557 = 19.609055\n","Loss of episode 558 = 19.615997\n","Loss of episode 559 = 19.615808\n","Loss of episode 560 = 19.622892\n","Loss of episode 561 = 19.621525\n","Loss of episode 562 = 19.62562\n","Loss of episode 563 = 19.618263\n","Loss of episode 564 = 19.616545\n","Loss of episode 565 = 19.621298\n","Loss of episode 566 = 19.609968\n","Loss of episode 567 = 19.611082\n","Loss of episode 568 = 19.605766\n","Loss of episode 569 = 19.609571\n","Loss of episode 570 = 19.609985\n","Loss of episode 571 = 19.618902\n","Loss of episode 572 = 19.603245\n","Loss of episode 573 = 19.60501\n","Loss of episode 574 = 19.604134\n","Loss of episode 575 = 19.60009\n","Loss of episode 576 = 19.597942\n","Loss of episode 577 = 19.59323\n","Loss of episode 578 = 19.591461\n","Loss of episode 579 = 19.590948\n","Loss of episode 580 = 19.587908\n","Loss of episode 581 = 19.588932\n","Loss of episode 582 = 19.585005\n","Loss of episode 583 = 19.584942\n","Loss of episode 584 = 19.586174\n","Loss of episode 585 = 19.583038\n","Loss of episode 586 = 19.589386\n","Loss of episode 587 = 19.591164\n","Loss of episode 588 = 19.587997\n","Loss of episode 589 = 19.58839\n","Loss of episode 590 = 19.59009\n","Loss of episode 591 = 19.589539\n","Loss of episode 592 = 19.592989\n","Loss of episode 593 = 19.589947\n","Loss of episode 594 = 19.589092\n","Loss of episode 595 = 19.588446\n","Loss of episode 596 = 19.59758\n","Loss of episode 597 = 19.597052\n","Loss of episode 598 = 19.605455\n","Loss of episode 599 = 19.601038\n","Loss of episode 600 = 19.605913\n","Loss of episode 601 = 19.601856\n","Loss of episode 602 = 19.604465\n","Loss of episode 603 = 19.598986\n","Loss of episode 604 = 19.596973\n","Loss of episode 605 = 19.597073\n","Loss of episode 606 = 19.599768\n","Loss of episode 607 = 19.604172\n","Loss of episode 608 = 19.5959\n","Loss of episode 609 = 19.596554\n","Loss of episode 610 = 19.593502\n","Loss of episode 611 = 19.58832\n","Loss of episode 612 = 19.591816\n","Loss of episode 613 = 19.589542\n","Loss of episode 614 = 19.591003\n","Loss of episode 615 = 19.59068\n","Loss of episode 616 = 19.586088\n","Loss of episode 617 = 19.58103\n","Loss of episode 618 = 19.57716\n","Loss of episode 619 = 19.581802\n","Loss of episode 620 = 19.577663\n","Loss of episode 621 = 19.571064\n","Loss of episode 622 = 19.573769\n","Loss of episode 623 = 19.576456\n","Loss of episode 624 = 19.572966\n","Loss of episode 625 = 19.572063\n","Loss of episode 626 = 19.576462\n","Loss of episode 627 = 19.575401\n","Loss of episode 628 = 19.573849\n","Loss of episode 629 = 19.580732\n","Loss of episode 630 = 19.572104\n","Loss of episode 631 = 19.579945\n","Loss of episode 632 = 19.575035\n","Loss of episode 633 = 19.57922\n","Loss of episode 634 = 19.577723\n","Loss of episode 635 = 19.589165\n","Loss of episode 636 = 19.58371\n","Loss of episode 637 = 19.579159\n","Loss of episode 638 = 19.580734\n","Loss of episode 639 = 19.578743\n","Loss of episode 640 = 19.576336\n","Loss of episode 641 = 19.582047\n","Loss of episode 642 = 19.58524\n","Loss of episode 643 = 19.578062\n","Loss of episode 644 = 19.577621\n","Loss of episode 645 = 19.57672\n","Loss of episode 646 = 19.576572\n","Loss of episode 647 = 19.570614\n","Loss of episode 648 = 19.5716\n","Loss of episode 649 = 19.569344\n","Loss of episode 650 = 19.569498\n","Loss of episode 651 = 19.569439\n","Loss of episode 652 = 19.562567\n","Loss of episode 653 = 19.565262\n","Loss of episode 654 = 19.569975\n","Loss of episode 655 = 19.561726\n","Loss of episode 656 = 19.563984\n","Loss of episode 657 = 19.561699\n","Loss of episode 658 = 19.563549\n","Loss of episode 659 = 19.55932\n","Loss of episode 660 = 19.561056\n","Loss of episode 661 = 19.565138\n","Loss of episode 662 = 19.565882\n","Loss of episode 663 = 19.56203\n","Loss of episode 664 = 19.559263\n","Loss of episode 665 = 19.56498\n","Loss of episode 666 = 19.561575\n","Loss of episode 667 = 19.561018\n","Loss of episode 668 = 19.557392\n","Loss of episode 669 = 19.558573\n","Loss of episode 670 = 19.557518\n","Loss of episode 671 = 19.55862\n","Loss of episode 672 = 19.55244\n","Loss of episode 673 = 19.552395\n","Loss of episode 674 = 19.557934\n","Loss of episode 675 = 19.554213\n","Loss of episode 676 = 19.5492\n","Loss of episode 677 = 19.551605\n","Loss of episode 678 = 19.554\n","Loss of episode 679 = 19.554127\n","Loss of episode 680 = 19.558636\n","Loss of episode 681 = 19.556171\n","Loss of episode 682 = 19.555082\n","Loss of episode 683 = 19.56086\n","Loss of episode 684 = 19.561295\n","Loss of episode 685 = 19.5622\n","Loss of episode 686 = 19.565296\n","Loss of episode 687 = 19.566238\n","Loss of episode 688 = 19.56894\n","Loss of episode 689 = 19.573109\n","Loss of episode 690 = 19.571608\n","Loss of episode 691 = 19.56245\n","Loss of episode 692 = 19.560102\n","Loss of episode 693 = 19.563541\n","Loss of episode 694 = 19.565313\n","Loss of episode 695 = 19.570879\n","Loss of episode 696 = 19.56316\n","Loss of episode 697 = 19.55927\n","Loss of episode 698 = 19.55917\n","Loss of episode 699 = 19.555914\n","Loss of episode 700 = 19.554445\n","Loss of episode 701 = 19.554874\n","Loss of episode 702 = 19.548836\n","Loss of episode 703 = 19.551432\n","Loss of episode 704 = 19.5486\n","Loss of episode 705 = 19.55026\n","Loss of episode 706 = 19.551855\n","Loss of episode 707 = 19.550385\n","Loss of episode 708 = 19.548925\n","Loss of episode 709 = 19.550331\n","Loss of episode 710 = 19.54977\n","Loss of episode 711 = 19.547516\n","Loss of episode 712 = 19.547379\n","Loss of episode 713 = 19.549103\n","Loss of episode 714 = 19.546862\n","Loss of episode 715 = 19.553759\n","Loss of episode 716 = 19.552631\n","Loss of episode 717 = 19.549084\n","Loss of episode 718 = 19.549564\n","Loss of episode 719 = 19.543257\n","Loss of episode 720 = 19.544544\n","Loss of episode 721 = 19.545864\n","Loss of episode 722 = 19.54847\n","Loss of episode 723 = 19.54744\n","Loss of episode 724 = 19.55339\n","Loss of episode 725 = 19.553787\n","Loss of episode 726 = 19.550344\n","Loss of episode 727 = 19.551994\n","Loss of episode 728 = 19.555542\n","Loss of episode 729 = 19.55718\n","Loss of episode 730 = 19.558405\n","Loss of episode 731 = 19.563187\n","Loss of episode 732 = 19.567709\n","Loss of episode 733 = 19.574915\n","Loss of episode 734 = 19.576187\n","Loss of episode 735 = 19.577194\n","Loss of episode 736 = 19.570871\n","Loss of episode 737 = 19.56984\n","Loss of episode 738 = 19.574385\n","Loss of episode 739 = 19.591553\n","Loss of episode 740 = 19.579868\n","Loss of episode 741 = 19.566542\n","Loss of episode 742 = 19.556026\n","Loss of episode 743 = 19.54802\n","Loss of episode 744 = 19.55076\n","Loss of episode 745 = 19.551117\n","Loss of episode 746 = 19.548607\n","Loss of episode 747 = 19.544392\n","Loss of episode 748 = 19.556587\n","Loss of episode 749 = 19.55679\n","Loss of episode 750 = 19.549135\n","Loss of episode 751 = 19.542736\n","Loss of episode 752 = 19.54739\n","Loss of episode 753 = 19.542173\n","Loss of episode 754 = 19.543839\n","Loss of episode 755 = 19.54851\n","Loss of episode 756 = 19.541481\n","Loss of episode 757 = 19.53925\n","Loss of episode 758 = 19.541733\n","Loss of episode 759 = 19.542715\n","Loss of episode 760 = 19.540485\n","Loss of episode 761 = 19.541534\n","Loss of episode 762 = 19.540802\n","Loss of episode 763 = 19.539026\n","Loss of episode 764 = 19.535994\n","Loss of episode 765 = 19.53746\n","Loss of episode 766 = 19.536907\n","Loss of episode 767 = 19.538296\n","Loss of episode 768 = 19.536268\n","Loss of episode 769 = 19.540512\n","Loss of episode 770 = 19.53752\n","Loss of episode 771 = 19.534693\n","Loss of episode 772 = 19.542433\n","Loss of episode 773 = 19.53973\n","Loss of episode 774 = 19.536076\n","Loss of episode 775 = 19.541431\n","Loss of episode 776 = 19.541397\n","Loss of episode 777 = 19.546337\n","Loss of episode 778 = 19.545715\n","Loss of episode 779 = 19.539627\n","Loss of episode 780 = 19.536865\n","Loss of episode 781 = 19.54259\n","Loss of episode 782 = 19.542736\n","Loss of episode 783 = 19.542587\n","Loss of episode 784 = 19.545391\n","Loss of episode 785 = 19.54385\n","Loss of episode 786 = 19.541441\n","Loss of episode 787 = 19.544147\n","Loss of episode 788 = 19.539284\n","Loss of episode 789 = 19.53484\n","Loss of episode 790 = 19.539215\n","Loss of episode 791 = 19.534195\n","Loss of episode 792 = 19.530529\n","Loss of episode 793 = 19.53358\n","Loss of episode 794 = 19.533442\n","Loss of episode 795 = 19.539116\n","Loss of episode 796 = 19.537407\n","Loss of episode 797 = 19.542013\n","Loss of episode 798 = 19.539717\n","Loss of episode 799 = 19.535118\n","\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# feature_size, hidden_size, num_layers, output_size, (number_heads)\n","model = MTORL(state_feature,512,2,3).to(device)\n","num_epoch = 800     # 800\n","Lambda = 1.5        # 1.5\n","Mu = 0.08           # 0.08\n","\n","# optimizer = torch.optim.SGD(net.parameters(),lr=0.5)\n","# optimizer = torch.optim.Adam(gru.parameters(),lr=0.001)\n","optimizer = torch.optim.AdamW(model.parameters(),lr=0.001)\n","\n","# loss_func = torch.nn.MSELoss()            #MSE for regression\n","loss_func = torch.nn.CrossEntropyLoss()     #CE for classification\n","Loss=[]\n","\n","for t in range(num_epoch):\n","    aver_loss = 0\n","    model.train()\n","    for batch, data in enumerate(data_train_loader):\n","        x, y ,z = data\n","        action_prediction, click_prediction = model(x)\n","        # print(action_prediction.shape)\n","        action_prediction = torch.transpose(action_prediction, dim0=0, dim1=1)\n","        click_prediction = torch.transpose(click_prediction, dim0=0, dim1=1)\n","        Entropy = torch.mean(shannon_entropy(action_prediction))\n","        # loss = loss_func(action_prediction,y) - Lambda * Entropy #//2-loss\n","        loss = loss_func(action_prediction,y) + Mu * loss_func(click_prediction,z) \\\n","        - Lambda * Entropy \n","        optimizer.zero_grad() \n","        loss.backward()    \n","        optimizer.step() \n","        aver_loss += loss\n","    aver_loss /= batch\n","    aver_loss=aver_loss.cpu().detach().numpy()\n","    print('Loss of episode %s ='%t,aver_loss)\n","    Loss.append(aver_loss)\n","plt.plot(Loss,color='r')\n","print()"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690978204313,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"lxxPbtYyOBS6","outputId":"d2b4bdae-3248-45d5-9a2c-25bb1d8fc299"},"outputs":[{"name":"stdout","output_type":"stream","text":["6680084\n"]}],"source":["device = torch.device(\"cuda\")\n","model_test = MTORL(state_feature,512,2,3).to(device)\n","total = sum([param.nelement() for param in model_test.parameters()])\n","print(total)\n","# gru_test"]},{"cell_type":"markdown","metadata":{"id":"Qiqn4DAJOBS7"},"source":["## Training accuracy"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690978204313,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"BUoPP1KjOBS7","outputId":"4ec4f842-dd61-42d5-e7f4-01bea8ab6944"},"outputs":[{"name":"stdout","output_type":"stream","text":["acc = 0.97834375\n"]}],"source":["Truth = []\n","Pred = []\n","aver_loss = 0\n","total_acc = 0\n","Entropy = 0\n","model.train()\n","for batch, data in enumerate(data_train_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    x, y, z = data\n","    # print(x)\n","    # prediction =net(x)\n","    test, _ = model(x)\n","    test = torch.transpose(test, dim0=0, dim1=1)\n","    # print(test)\n","    # print(y)\n","    # print(shannon_entropy(test).shape)\n","    aver_entropy = sum(torch.mean(shannon_entropy(test),dim=1))\n","    # print(sum(aver_entropy))\n","    Entropy += aver_entropy\n","\n","    a = torch.argmax(test,dim = 2).cpu().data.numpy()\n","    b = torch.argmax(y,dim = 2).cpu().data.numpy()\n","    # print(a)\n","    # print(b)\n","    # print(sum(a==b))\n","    aver_acc = sum(sum(a==b))/length\n","    # print(aver_acc)\n","    total_acc += aver_acc\n","    # print(total_acc)\n","# print(batch)\n","print('acc =', total_acc/(0.8*max_number_traj))\n","# print('Entropy = ', Entropy/(0.8*max_number_traj))"]},{"cell_type":"markdown","metadata":{},"source":["## Validation accuracy (Hyperparameter tuning)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Action acc = 0.6735\n","CTR acc = 0.8239999999999986\n","Entropy =  tensor(1.0022, grad_fn=<DivBackward0>)\n"]}],"source":["from sklearn.metrics import roc_curve, auc\n","\n","\n","Loss=[]\n","Truth = []\n","Pred = []\n","aver_loss = 0\n","total_acc = 0\n","total_ctr = 0\n","total_auc = 0\n","Entropy = 0\n","num = 0\n","\n","# Click_pred = np.zeros((int(0.2*max_number_traj),length))\n","# Click_true = np.zeros((int(0.2*max_number_traj),length))\n","\n","for batch, data in enumerate(data_val_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    model.eval()\n","    x, y, z = data\n","    test, click = model(x)\n","    click_pred = torch.squeeze(click).cpu().data.numpy()\n","    click_true = torch.squeeze(z).cpu().data.numpy()\n","    click_pred[click_pred<1/2] = 0\n","    click_pred[click_pred>=1/2] = 1\n","    # Click_pred[batch] = click_pred\n","    # Click_true[batch] = click_true\n","\n","    p = test[:,0].cpu()\n","    aver_entropy = torch.mean(shannon_entropy(p))\n","    # print('aver_entropy =', aver_entropy)\n","    Entropy += aver_entropy\n","    # print(test)\n","    a = torch.argmax(test,dim = 2).cpu().data.numpy().T[0]\n","    b = torch.argmax(y,dim = 2).cpu().data.numpy()[0]\n","    # print('-----------')\n","    # print('pred =',a.T[0])\n","    # print('true =',b[0])\n","    aver_acc = sum(a==b)/length\n","    total_acc += aver_acc\n","\n","    # print('-----------')\n","    # print('pred =', click_pred)\n","    # print('true =', click_true) \n","    aver_ctr = sum(click_pred==click_true)/length\n","    total_ctr += aver_ctr\n","\n","\n","# fpr, tpr, thresholds = roc_curve(Click_true.reshape(-1), Click_pred.reshape(-1),pos_label=1)\n","# total_auc = auc(fpr, tpr)\n","# print('CTR AUC =', total_auc)\n","print('Action acc =', total_acc/(0.1*max_number_traj))\n","print('CTR acc =', total_ctr/(0.1*max_number_traj))\n","print('Entropy = ', Entropy/(0.1*max_number_traj))\n"]},{"cell_type":"markdown","metadata":{"id":"6Hi558JBOBS8"},"source":["## Inference accuracy"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1541,"status":"ok","timestamp":1690978205851,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"dH6BC5O2OBS8","outputId":"f0746245-4215-490b-c3b9-ee67948cc407"},"outputs":[{"name":"stdout","output_type":"stream","text":["Action acc = 0.7073749999999993\n","CTR acc = 0.8371249999999997\n","Entropy =  tensor(0.9891, grad_fn=<DivBackward0>)\n"]}],"source":["from sklearn.metrics import roc_curve, auc\n","\n","\n","Loss=[]\n","Truth = []\n","Pred = []\n","aver_loss = 0\n","total_acc = 0\n","total_ctr = 0\n","total_auc = 0\n","Entropy = 0\n","num = 0\n","\n","# Click_pred = np.zeros((int(0.2*max_number_traj),length))\n","# Click_true = np.zeros((int(0.2*max_number_traj),length))\n","\n","for batch, data in enumerate(data_test_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    model.eval()\n","    x, y, z = data\n","    test, click = model(x)\n","    click_pred = torch.squeeze(click).cpu().data.numpy()\n","    click_true = torch.squeeze(z).cpu().data.numpy()\n","    click_pred[click_pred<1/2] = 0\n","    click_pred[click_pred>=1/2] = 1\n","    # Click_pred[batch] = click_pred\n","    # Click_true[batch] = click_true\n","\n","    p = test[:,0].cpu()\n","    aver_entropy = torch.mean(shannon_entropy(p))\n","    # print('aver_entropy =', aver_entropy)\n","    Entropy += aver_entropy\n","    # print(test)\n","    a = torch.argmax(test,dim = 2).cpu().data.numpy().T[0]\n","    b = torch.argmax(y,dim = 2).cpu().data.numpy()[0]\n","    # print('-----------')\n","    # print('pred =',a.T[0])\n","    # print('true =',b[0])\n","    aver_acc = sum(a==b)/length\n","    total_acc += aver_acc\n","\n","    # print('-----------')\n","    # print('pred =', click_pred)\n","    # print('true =', click_true) \n","    aver_ctr = sum(click_pred==click_true)/length\n","    total_ctr += aver_ctr\n","\n","\n","# fpr, tpr, thresholds = roc_curve(Click_true.reshape(-1), Click_pred.reshape(-1),pos_label=1)\n","# total_auc = auc(fpr, tpr)\n","# print('CTR AUC =', total_auc)\n","print('Action acc =', total_acc/(0.1*max_number_traj))\n","print('CTR acc =', total_ctr/(0.1*max_number_traj))\n","print('Entropy = ', Entropy/(0.1*max_number_traj))\n"]},{"cell_type":"markdown","metadata":{"id":"n6ecXL0POBS8"},"source":["## AUC"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":736,"status":"ok","timestamp":1690978206585,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"45g3DhluOBS8","outputId":"9e567364-e555-4ef1-be01-fdcdc6de971b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Average AUC = [0.82043774 0.81501743 0.81813702]\n"]}],"source":["from sklearn.metrics import precision_score, roc_curve, auc\n","n_classes = 3\n","# length = 8\n","aver_auc = np.zeros(n_classes)\n","total_precision = 0\n","Y_score = np.zeros(((length, int(0.1*max_number_traj), n_classes)))\n","# print(Y_score.shape)\n","Y_label = np.zeros(((length, int(0.1*max_number_traj), n_classes)))\n","for batch, data in enumerate(data_test_loader):\n","    model.eval()\n","    x, y, z = data\n","    # print(x)\n","    test, _ = model(x)\n","    y_score = test[:,0,:].cpu().data.numpy()\n","    y_label = y[0].cpu().data.numpy()\n","    # print(batch)\n","    for i in range(length):\n","        Y_score[i][batch] = y_score[i]\n","        Y_label[i][batch] = y_label[i]\n","        # Y_score[i][batch] = [1/3,1/3,1/3]\n","        # Y_label[i][batch] = y_label[i]\n","        # print(y_score[i])\n","        # print(y_label[i])\n","fpr = dict()\n","tpr = dict()\n","roc_auc = dict()\n","aver_auc = np.zeros(n_classes)\n","for t in range(length):\n","    for i in range(n_classes):\n","        fpr[i], tpr[i], _ = roc_curve(Y_label[t][:, i], Y_score[t][:, i])\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","        # aver_auc[i] += auc(fpr[i], tpr[i])\n","        aver_auc[i] += roc_auc[i]/length\n","    # print('step %s AUC ='%(t+1),roc_auc)\n","print('Average AUC =', aver_auc)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1690978206585,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"Q-5eCoFCOBS8","outputId":"1fe76151-c1a2-4ced-8acf-4a37d2df9e6e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Aver_F1 = 0.7071 Aver_Precision = 0.7079 Aver_Recall = 0.7078\n"]}],"source":["from sklearn.metrics import f1_score, precision_score, recall_score\n","\n","aver_f1 = 0\n","aver_p = 0\n","aver_r = 0\n","\n","# print(y_pred)\n","# print(y_true)\n","for i in range(length):\n","  y_true = np.argmax(Y_label[i],axis = 1)\n","  y_pred = np.argmax(Y_score[i],axis = 1)\n","  f1 = round(f1_score(y_true, y_pred, average='macro' ),4)\n","  p = round(precision_score(y_true, y_pred, average='macro'),4)\n","  r = round(recall_score(y_true, y_pred, average='macro'),4)\n","  aver_f1 += f1/length\n","  aver_p += p/length\n","  aver_r += r/length\n","print('Aver_F1 =', round(aver_f1,4), 'Aver_Precision =', round(aver_p,4), 'Aver_Recall =', round(aver_r,4))"]},{"cell_type":"markdown","metadata":{},"source":["## Budget Allocation"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["test_number = int(0.1*max_number_traj)\n","True_Click_matrix = np.zeros((test_number,length))\n","True_Action_matrix = np.zeros((test_number,length))\n","Click_pred_matrix = np.zeros((test_number,length))\n","Action_pred_matrix = np.zeros((test_number,length))\n","\n","for batch, data in enumerate(data_test_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    model.eval()\n","    x, y, z = data\n","    action_true = torch.squeeze(torch.argmax(y,dim = 2)).cpu().data.numpy()\n","    click_true = torch.squeeze(z).cpu().data.numpy()\n","    True_Click_matrix[batch] = click_true\n","    # True_Action_count += np.sum(action_true, axis = 0)\n","    \n","    action, click = model(x)\n","    action_pred = torch.squeeze(torch.argmax(action,dim = 2)).cpu().data.numpy()\n","    # print(action_true == action_pred)\n","    Action_pred_matrix[batch] = (action_true == action_pred)\n","    click_pred = torch.squeeze(click).cpu().data.numpy()\n","    Click_pred_matrix[batch] = click_pred\n","\n","# print(True_Click_matrix)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True click number =  1513.0\n","Ture_CTR = 0.189125\n","Exposure number =  3000.0\n","Click number =  913.0\n","Policy_CTR = 0.30433333333333334\n"]}],"source":["choose_number = 150\n","True_cost = 0.1 * max_number_traj * length\n","True_Click_number = sum(sum(True_Click_matrix))\n","User_choose = np.zeros((test_number,length))\n","\n","print('True click number = ', True_Click_number)\n","True_CTR = True_Click_number/(0.1 * max_number_traj * length) \n","print('Ture_CTR =',True_CTR)\n","for i in range(length):\n","    tmp = Click_pred_matrix[:,i]\n","    idx = np.argsort(tmp)\n","    # print(idx[400:])\n","    # print(tmp[idx[400:]])\n","    User_choose[:,i][idx[test_number-choose_number:]] = 1\n","\n","Exposure = sum(sum(User_choose))\n","Click = sum(sum(User_choose*Action_pred_matrix*True_Click_matrix))\n","Policy_ctr = Click/Exposure\n","print('Exposure number = ', Exposure)\n","print('Click number = ', Click) \n","print('Policy_CTR =', Policy_ctr)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
