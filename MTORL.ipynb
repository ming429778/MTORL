{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1481,"status":"ok","timestamp":1690970114664,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"SHePlV70OBS2"},"outputs":[],"source":["# if torch.cuda.is_available():\n","#     print('CUDNN VERSION:',torch.backends.cudnn.version())\n","#     print('Number CUDA Devices:',torch.cuda.device_count())\n","#     print('CUDA Device Name:',torch.cuda.get_device_name(0))\n","#     print('CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6052,"status":"ok","timestamp":1690970120708,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"kUqW9r2qOBS2","outputId":"606cfa3e-2965-43bb-9f91-818746accbc6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on the GPU\n"]}],"source":["import torch\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")  # you can continue going on here, like cuda:1 cuda:2....etc.\n","    print(\"Running on the GPU\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"Running on the CPU\")\n","# torch.cuda.device_count()"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1690973614130,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"iZBy7-2rOBS3"},"outputs":[{"name":"stdout","output_type":"stream","text":["126184\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","Data = pd.read_csv('kuairand_sequence.csv',index_col=0)\n","print(len(Data))\n","# Data = pd.read_csv('kuairand_sequence_user=8000.csv',index_col=0)\n","# data['user_id'].value_counts()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1690973619591,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"3HzbfAYVOBS3","outputId":"2e5a6756-eb74-4e81-f782-0caddeae1081"},"outputs":[{"name":"stdout","output_type":"stream","text":["(6051,)\n"]}],"source":["import torch\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import random\n","from torch.utils.data import Dataset, DataLoader,TensorDataset\n","\n","max_length = 100\n","num_user = len(Data['user_id'].unique())\n","state_feature = len(Data.columns) - 1\n","action_feature = 3\n","\n","uid = Data['user_id'].unique()\n","print(uid.shape)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2991,"status":"ok","timestamp":1690973742240,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"HdF3rl_LOBS4","outputId":"9a25cd7c-7681-4521-a5f4-db7dc0f9d462"},"outputs":[],"source":["State = np.zeros(((num_user,max_length,state_feature)))\n","Action = np.zeros((num_user,max_length,action_feature))\n","Click = np.zeros((num_user,max_length))\n","\n","j = 0\n","for i in uid:\n","    state_sequence = np.array(Data[Data['user_id']==i])[:-1]\n","    # state_sequence = np.array(data[data['user_id']==i].drop(['short','mid','long'],axis=1))\n","    action_sequence = np.array(Data[Data['user_id']==i][['short','mid','long']])[1:]\n","    click_sequence = np.array(Data[Data['user_id']==i]['is_click'])[1:]\n","    # action_sequence = np.array(data[data['user_id']==i][['short','mid','long']])\n","    # print(action_sequence)\n","    state = np.pad(state_sequence,((0,max_length-len(state_sequence)),(0,0)))[:,1:]\n","    action = np.pad(action_sequence,((0,max_length-len(action_sequence)),(0,0)))\n","    click = np.pad(click_sequence,((0,max_length-len(click_sequence))))\n","    # print(click)\n","    State[j] = state\n","    Action[j] = action\n","    Click[j] = click\n","    j += 1\n","\n","# print(len(State))\n","# print(len(Action))\n","# print(len(Click))\n","# print(Click.shape)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":822,"status":"ok","timestamp":1690976535806,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"tPsLnRdtOBS4","outputId":"798634a6-86b6-4797-8422-298b81c046ca"},"outputs":[],"source":["length = 20\n","max_number_traj = 4000\n","\n","number_traj = 0\n","def Select_trajectory(X,Y,Z,number_traj):\n","    Select_states = []\n","    Select_actions = []\n","    Select_clicks = []\n","    # for s in range(number_traj):\n","    while True:\n","        i = random.randint(0,len(X)-1)\n","        select_state = X[i]\n","        select_action = Y[i]\n","        select_click = Z[i]\n","        # print(select_state)\n","        # print(select_action)\n","        for k in range(0,max_length):\n","            # print(select_episode[k])\n","            if select_state[k].any() == 0:\n","                max_episode_length = k\n","                break\n","        # print(k)\n","        # print(max_episode_length)\n","        if max_episode_length >= length + 1:\n","            j = random.randint(0,max_episode_length-length-1)\n","            select_state = select_state[j:j+length]\n","            select_action = select_action[j:j+length]\n","            select_click = select_click[j:j+length]\n","            Select_states.append(select_state)\n","            Select_actions.append(select_action)\n","            Select_clicks.append(select_click)\n","            number_traj += 1\n","        # print(select_trajectory)\n","        if number_traj == max_number_traj:\n","            break\n","    return np.array(Select_states),np.array(Select_actions),np.array(Select_clicks)\n","\n","X , Y , Z = Select_trajectory(State,Action,Click,number_traj)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1690976536378,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"P1xlyTewOBS5"},"outputs":[],"source":["np.random.seed(2023)\n","per = np.random.permutation(X.shape[0])\n","# print(per)\n","X = X[per]\n","Y = Y[per]\n","Z = Z[per]\n","X,Y,Z = torch.from_numpy(X).to(torch.float32).to(device),torch.from_numpy(Y).to(torch.float32).to(device), torch.from_numpy(Z).to(torch.float32).to(device)\n","# print(sum(Y))"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690976536378,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"YB3Wa9SaOBS5","outputId":"c094f597-85a0-49b2-adee-00ee3d42b5d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["3200\n"]}],"source":["def split_data(State, Action, Reward, timestep, input_size, output_size):\n","\n","    # Training set\n","    train_size = int(np.round(0.8 * X.shape[0]))\n","    val_size = int(np.round(0.1 * X.shape[0]))\n","    print(train_size)\n","\n","    # Split training and test set 8:1:1\n","    x_train = X[: train_size, :].reshape(-1, timestep,input_size)\n","    y_train = Y[: train_size].reshape(-1,timestep, output_size)\n","    z_train = Z[: train_size].reshape(-1,timestep, 1)\n","\n","    # x_test = X[train_size:, :].reshape(-1, timestep,input_size)\n","    # y_test = Y[train_size:].reshape(-1,timestep, output_size)\n","    # z_test = Z[train_size:].reshape(-1,timestep, 1)\n","\n","    x_val = X[train_size:train_size+val_size, :].reshape(-1, timestep,input_size)\n","    y_val = Y[train_size:train_size+val_size].reshape(-1,timestep, output_size)\n","    z_val = Z[train_size:train_size+val_size].reshape(-1,timestep, 1) \n","\n","    x_test = X[train_size+val_size:, :].reshape(-1, timestep,input_size)\n","    y_test = Y[train_size+val_size:].reshape(-1,timestep, output_size)\n","    z_test = Z[train_size+val_size:].reshape(-1,timestep, 1)\n","\n","    return [x_train, y_train, z_train, x_val, y_val, z_val, x_test, y_test, z_test]\n","\n","# State, Action, Reward(is_click)\n","X1,Y1,Z1,X2,Y2,Z2,X3,Y3,Z3 = split_data(X,Y,Z,length,state_feature,action_feature)\n","# print(X1.shape)\n","# print(Y1.shape)\n","# print(Z1.shape)\n","# X1,Y1=torch.from_numpy(X1).to(torch.float32).to(device),torch.from_numpy(Y1).to(torch.float32).to(device)\n","# X2,Y2=torch.from_numpy(X2).to(torch.float32).to(device),torch.from_numpy(Y2).to(torch.float32).to(device)\n","# Z1,Z2=torch.from_numpy(Z1).to(torch.float32).to(device),torch.from_numpy(Z2).to(torch.float32).to(device)\n","train_ids = TensorDataset(X1,Y1,Z1)\n","val_ids = TensorDataset(X2,Y2,Z2)\n","test_ids = TensorDataset(X3,Y3,Z3)\n","# print(test_ids[0])"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([512, 512, 20])\n"]}],"source":["\n","from pytorch_tcn.tcn import TCN\n","x = torch.ones(512,state_feature,length)\n","model = TCN(state_feature,[512,512],causal=True)\n","print(model(x).shape)\n","# model = TCN(\n","#     num_inputs: int,\n","#     num_channels: ArrayLike,\n","#     kernel_size: int = 4,\n","#     dilations: Optional[ ArrayLike ] = None,\n","#     dilaton_reset: Optional[ int ] = None,\n","#     dropout: float = 0.1,\n","#     causal: bool = True,\n","#     use_norm: str = 'weight_norm',\n","#     activation: str = 'relu',\n","#     kernel_initializer: str = 'xavier_uniform',\n","#     use_skip_connections: bool = False,\n","#     input_shape: str = 'NCL',\n","# )"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690976536379,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"gxuxFJ5sOBS6"},"outputs":[],"source":["class MTORL(nn.Module):\n","    def __init__(self, feature_size, hidden_size, num_layers, output_size):\n","        super(MTORL, self).__init__()\n","        self.hidden_size = hidden_size  # hiddensize\n","        self.num_layers = num_layers  # gru layer\n","        # self.embedding = nn.Linear(feature_size,hidden_size)\n","        # self.gru = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)\n","        self.gru = nn.GRU(feature_size, hidden_size, num_layers, batch_first=True)\n","        # self.lstm = nn.LSTM(feature_size, hidden_size, num_layers, batch_first=True)\n","        self.hidden = nn.Linear(hidden_size, hidden_size)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","        self.fc_click = nn.Linear(hidden_size, 1)\n","        # self.fc1 = nn.Linear(hidden_size, fc_hidden_size)\n","        # self.fc2 = nn.Linear(fc_hidden_size, output_size)\n","        self.sigmoid = nn.Sigmoid()\n","        self.ReLU = nn.ReLU()\n","        self.LeakyReLU = nn.LeakyReLU()\n","        self.BN = nn.BatchNorm1d(8)\n","        self.LayerNorm = nn.LayerNorm(hidden_size)\n","        self.query = nn.Linear(hidden_size, hidden_size)\n","        self.key = nn.Linear(hidden_size, hidden_size)\n","        self.value = nn.Linear(hidden_size, hidden_size)\n","        self.sqrt_size = np.sqrt(hidden_size)\n","        self.softmax = nn.Softmax(dim=-1)\n","        self.tcn = TCN(state_feature,[hidden_size,hidden_size])\n","        # self.multihead = nn.MultiheadAttention(hidden_size,num_heads,dropout=0.1)\n","    def forward(self, x, hidden=None):\n","        batch_size = x.shape[0] #  batchsize\n","\n","        # # initial hidden state\n","        # if hidden is None:\n","        #     h_0 = x.data.new(self.num_layers, batch_size, self.hidden_size).fill_(0).float()\n","        # else:\n","        #     h_0 = hidden\n","        # # GRU operation\n","        # output, h_0 = self.gru(x, h_0)\n","\n","        # TCN\n","        x = x.permute(0, 2, 1)\n","        output = self.tcn(x)\n","        output = output.permute(0, 2, 1)\n","\n","        # # Batch Normalization\n","        # output = self.BN(output)\n","\n","        # click_prob = self.LeakyReLU(GRU_output)\n","        # click_prob = click_prob.transpose(0,1)\n","        click_prob = output.transpose(0,1)\n","\n","        click_prob = self.hidden(click_prob)\n","        click_prob = self.LeakyReLU(click_prob)\n","\n","        click_prob = self.fc_click(click_prob)\n","        click_prob = self.sigmoid(click_prob)\n","\n","        # Attention Layer\n","        query_layer = self.query(output)\n","        key_layer = self.key(output).permute(0, 2, 1)\n","        value_layer = self.value(output)\n","        attention_scores = torch.matmul(query_layer, key_layer)\n","        attention_scores = attention_scores / self.sqrt_size\n","\n","        # Mask\n","        mask = (torch.triu(torch.ones(length, length)) == 0).transpose(0,1).to(device)\n","        attention_scores = attention_scores.masked_fill(mask, value=torch.tensor(-1e9))\n","        # print(attention_scores)\n","\n","        attention_scores = F.dropout(attention_scores, p=0.2)\n","        attention_probs = self.softmax(attention_scores)\n","        output = torch.matmul(attention_probs, value_layer)\n","\n","        # ReLU/LeakyReLU activation\n","        # output = self.ReLU(output)\n","        output = self.LeakyReLU(output)\n","        # output = self.LeakyReLU(GRU_output)\n","\n","\n","        # # Multihead attention\n","        # query_layer = self.query(GRU_output)\n","        # key_layer = self.key(GRU_output)\n","        # value_layer = self.value(GRU_output)\n","        # # print(query_layer.shape)\n","        # output, _ = self.multihead(query_layer,key_layer,value_layer)\n","\n","        # transpose dim 1,2 to get shape (timestep, batch_size, hidden_dim)\n","        output = output.transpose(0,1)\n","\n","        # # Add&Norm (1)\n","        output = F.dropout(self.LeakyReLU(self.hidden(output)), p=0.2) + output\n","        output = self.LayerNorm(output)\n","        output = F.dropout(self.LeakyReLU(self.hidden(output)), p=0.2) + output\n","        output = self.LayerNorm(output)\n","\n","        # # Add&Norm (2)\n","        # output = self.LeakyReLU(self.hidden(output)) + output\n","        # output = self.LayerNorm(output)\n","        # output = F.dropout(output, p=0.2)\n","        # output = self.LeakyReLU(self.hidden(output)) + output\n","        # output = self.LayerNorm(output)\n","    \n","        # # Add&Norm (3)\n","        # output = F.dropout(output, p=0.2) + GRU_output.transpose(0,1)\n","        # output = self.LayerNorm(output)\n","        # output = F.dropout(self.hidden(self.LeakyReLU(self.hidden(output))), p=0.2) + output\n","        # output = self.LayerNorm(output)\n","\n","        output = self.fc(output)  # (batch_size, timestep, output_size)\n","\n","        # Softmax convert to [0,1]\n","        action_prob = F.softmax(output,dim=2)\n","\n","        # all timestep output\n","        return action_prob, click_prob\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690976536379,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"PwFFlRoiOBS6"},"outputs":[],"source":["Batchsize = 512  # 512\n","# data_train_loader = DataLoader(dataset=train_ids, batch_size=Batchsize, shuffle=True)\n","data_train_loader = DataLoader(dataset=train_ids, batch_size=Batchsize, shuffle=False)\n","data_val_loader = DataLoader(dataset=val_ids, batch_size=1, shuffle=False,drop_last=False)\n","data_test_loader = DataLoader(dataset=test_ids, batch_size=1, shuffle=False,drop_last=False)\n","# data_full_loader = DataLoader(dataset=data_ids, batch_size=1, shuffle=True)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690976536379,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"5ulgrq9e6DgS"},"outputs":[],"source":["# def shannon_entropy(x):\n","#   p = x\n","#   logp = torch.log2(p)\n","#   # print(p)\n","#   # print(logp)\n","#   entropy = - torch.sum(p*logp,dim=-1)\n","#   return entropy"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","\n","def DPO_loss_func(pi_win,pi_lose):\n","    # loss = 0\n","    # num = 0\n","    # for p_win,p_lose in zip(pi_win,pi_lose):\n","    #     p_div = F.kl_div(p_win, p_lose)\n","    #     loss += p_div\n","    #     num += 1\n","    div = F.kl_div(pi_lose, pi_win, reduction='batchmean')\n","\n","    return -torch.log(0.1*F.sigmoid(div))/Batchsize"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":122170,"status":"ok","timestamp":1690978204312,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"3qztBjLjOBS6","outputId":"f7e713e5-0d66-4c29-a488-d4a2910d4512"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\39223\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n","  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"]},{"name":"stdout","output_type":"stream","text":["Loss of episode 0 = 24.263517\n","Loss of episode 1 = 24.184614\n","Loss of episode 2 = 24.136242\n","Loss of episode 3 = 24.113409\n","Loss of episode 4 = 24.092392\n","Loss of episode 5 = 24.075771\n","Loss of episode 6 = 24.070318\n","Loss of episode 7 = 24.070501\n","Loss of episode 8 = 24.054907\n","Loss of episode 9 = 24.03629\n","Loss of episode 10 = 24.007381\n","Loss of episode 11 = 23.961119\n","Loss of episode 12 = 23.944418\n","Loss of episode 13 = 23.926962\n","Loss of episode 14 = 23.908056\n","Loss of episode 15 = 23.906414\n","Loss of episode 16 = 23.891663\n","Loss of episode 17 = 23.851109\n","Loss of episode 18 = 23.820768\n","Loss of episode 19 = 23.805822\n","Loss of episode 20 = 23.81053\n","Loss of episode 21 = 23.813513\n","Loss of episode 22 = 23.756277\n","Loss of episode 23 = 23.717833\n","Loss of episode 24 = 23.703566\n","Loss of episode 25 = 23.718763\n","Loss of episode 26 = 23.735916\n","Loss of episode 27 = 23.76248\n","Loss of episode 28 = 23.696568\n","Loss of episode 29 = 23.642843\n","Loss of episode 30 = 23.698166\n","Loss of episode 31 = 23.624554\n","Loss of episode 32 = 23.548689\n","Loss of episode 33 = 23.55741\n","Loss of episode 34 = 23.509205\n","Loss of episode 35 = 23.526207\n","Loss of episode 36 = 23.590353\n","Loss of episode 37 = 23.586088\n","Loss of episode 38 = 23.539045\n","Loss of episode 39 = 23.434875\n","Loss of episode 40 = 23.398827\n","Loss of episode 41 = 23.409384\n","Loss of episode 42 = 23.408627\n","Loss of episode 43 = 23.384726\n","Loss of episode 44 = 23.395987\n","Loss of episode 45 = 23.35398\n","Loss of episode 46 = 23.37377\n","Loss of episode 47 = 23.268509\n","Loss of episode 48 = 23.23727\n","Loss of episode 49 = 23.216423\n","Loss of episode 50 = 23.305134\n","Loss of episode 51 = 23.380842\n","Loss of episode 52 = 23.224964\n","Loss of episode 53 = 23.214417\n","Loss of episode 54 = 23.151937\n","Loss of episode 55 = 23.107794\n","Loss of episode 56 = 23.093956\n","Loss of episode 57 = 23.09928\n","Loss of episode 58 = 23.093046\n","Loss of episode 59 = 23.14871\n","Loss of episode 60 = 23.153515\n","Loss of episode 61 = 23.104269\n","Loss of episode 62 = 23.127247\n","Loss of episode 63 = 23.01158\n","Loss of episode 64 = 22.9805\n","Loss of episode 65 = 23.014164\n","Loss of episode 66 = 22.96218\n","Loss of episode 67 = 22.943817\n","Loss of episode 68 = 22.891602\n","Loss of episode 69 = 22.918655\n","Loss of episode 70 = 22.870922\n","Loss of episode 71 = 22.895142\n","Loss of episode 72 = 22.800999\n","Loss of episode 73 = 22.896404\n","Loss of episode 74 = 22.871634\n","Loss of episode 75 = 22.931992\n","Loss of episode 76 = 22.885925\n","Loss of episode 77 = 22.894073\n","Loss of episode 78 = 23.017088\n","Loss of episode 79 = 23.145315\n","Loss of episode 80 = 22.963247\n","Loss of episode 81 = 22.774426\n","Loss of episode 82 = 22.78971\n","Loss of episode 83 = 22.840582\n","Loss of episode 84 = 22.825104\n","Loss of episode 85 = 22.77123\n","Loss of episode 86 = 22.751608\n","Loss of episode 87 = 22.736523\n","Loss of episode 88 = 22.653997\n","Loss of episode 89 = 22.62106\n","Loss of episode 90 = 22.579212\n","Loss of episode 91 = 22.536726\n","Loss of episode 92 = 22.526129\n","Loss of episode 93 = 22.493916\n","Loss of episode 94 = 22.480694\n","Loss of episode 95 = 22.488905\n","Loss of episode 96 = 22.506813\n","Loss of episode 97 = 22.531643\n","Loss of episode 98 = 22.604725\n","Loss of episode 99 = 22.641737\n","Loss of episode 100 = 22.653156\n","Loss of episode 101 = 22.571074\n","Loss of episode 102 = 22.518087\n","Loss of episode 103 = 22.495062\n","Loss of episode 104 = 22.422491\n","Loss of episode 105 = 22.386223\n","Loss of episode 106 = 22.310629\n","Loss of episode 107 = 22.315746\n","Loss of episode 108 = 22.28582\n","Loss of episode 109 = 22.280823\n","Loss of episode 110 = 22.267548\n","Loss of episode 111 = 22.235767\n","Loss of episode 112 = 22.22689\n","Loss of episode 113 = 22.212624\n","Loss of episode 114 = 22.20059\n","Loss of episode 115 = 22.203983\n","Loss of episode 116 = 22.210875\n","Loss of episode 117 = 22.187338\n","Loss of episode 118 = 22.202536\n","Loss of episode 119 = 22.159515\n","Loss of episode 120 = 22.212719\n","Loss of episode 121 = 22.24195\n","Loss of episode 122 = 22.224396\n","Loss of episode 123 = 22.188581\n","Loss of episode 124 = 22.164497\n","Loss of episode 125 = 22.181087\n","Loss of episode 126 = 22.21323\n","Loss of episode 127 = 22.24937\n","Loss of episode 128 = 22.327812\n","Loss of episode 129 = 22.449509\n","Loss of episode 130 = 22.335615\n","Loss of episode 131 = 22.141476\n","Loss of episode 132 = 22.047407\n","Loss of episode 133 = 21.993065\n","Loss of episode 134 = 21.979902\n","Loss of episode 135 = 21.992435\n","Loss of episode 136 = 21.982098\n","Loss of episode 137 = 21.99378\n","Loss of episode 138 = 22.034805\n","Loss of episode 139 = 22.080017\n","Loss of episode 140 = 22.00785\n","Loss of episode 141 = 22.011732\n","Loss of episode 142 = 21.957497\n","Loss of episode 143 = 21.93127\n","Loss of episode 144 = 21.933187\n","Loss of episode 145 = 21.966816\n","Loss of episode 146 = 21.955307\n","Loss of episode 147 = 21.906666\n","Loss of episode 148 = 21.927282\n","Loss of episode 149 = 21.886478\n","Loss of episode 150 = 21.920416\n","Loss of episode 151 = 21.932098\n","Loss of episode 152 = 21.910137\n","Loss of episode 153 = 21.968817\n","Loss of episode 154 = 21.98656\n","Loss of episode 155 = 22.080402\n","Loss of episode 156 = 22.206055\n","Loss of episode 157 = 22.511686\n","Loss of episode 158 = 22.240835\n","Loss of episode 159 = 22.029661\n","Loss of episode 160 = 21.96687\n","Loss of episode 161 = 21.954956\n","Loss of episode 162 = 21.997753\n","Loss of episode 163 = 21.96903\n","Loss of episode 164 = 21.889362\n","Loss of episode 165 = 21.821087\n","Loss of episode 166 = 21.738003\n","Loss of episode 167 = 21.689186\n","Loss of episode 168 = 21.660948\n","Loss of episode 169 = 21.655308\n","Loss of episode 170 = 21.642235\n","Loss of episode 171 = 21.656017\n","Loss of episode 172 = 21.676046\n","Loss of episode 173 = 21.642426\n","Loss of episode 174 = 21.64537\n","Loss of episode 175 = 21.622616\n","Loss of episode 176 = 21.678534\n","Loss of episode 177 = 21.67677\n","Loss of episode 178 = 21.690777\n","Loss of episode 179 = 21.67918\n","Loss of episode 180 = 21.689068\n","Loss of episode 181 = 21.672062\n","Loss of episode 182 = 21.686348\n","Loss of episode 183 = 21.61589\n","Loss of episode 184 = 21.553905\n","Loss of episode 185 = 21.541258\n","Loss of episode 186 = 21.541748\n","Loss of episode 187 = 21.55133\n","Loss of episode 188 = 21.527458\n","Loss of episode 189 = 21.524582\n","Loss of episode 190 = 21.509083\n","Loss of episode 191 = 21.494751\n","Loss of episode 192 = 21.51295\n","Loss of episode 193 = 21.499838\n","Loss of episode 194 = 21.506819\n","Loss of episode 195 = 21.486504\n","Loss of episode 196 = 21.509146\n","Loss of episode 197 = 21.5297\n","Loss of episode 198 = 21.494213\n","Loss of episode 199 = 21.46079\n","Loss of episode 200 = 21.46698\n","Loss of episode 201 = 21.459381\n","Loss of episode 202 = 21.450672\n","Loss of episode 203 = 21.452782\n","Loss of episode 204 = 21.464233\n","Loss of episode 205 = 21.504942\n","Loss of episode 206 = 21.531258\n","Loss of episode 207 = 21.503263\n","Loss of episode 208 = 21.49052\n","Loss of episode 209 = 21.448605\n","Loss of episode 210 = 21.4529\n","Loss of episode 211 = 21.461113\n","Loss of episode 212 = 21.51141\n","Loss of episode 213 = 21.513319\n","Loss of episode 214 = 21.524954\n","Loss of episode 215 = 21.509918\n","Loss of episode 216 = 21.497747\n","Loss of episode 217 = 21.497143\n","Loss of episode 218 = 21.432907\n","Loss of episode 219 = 21.40165\n","Loss of episode 220 = 21.414242\n","Loss of episode 221 = 21.3969\n","Loss of episode 222 = 21.369812\n","Loss of episode 223 = 21.323547\n","Loss of episode 224 = 21.304478\n","Loss of episode 225 = 21.283163\n","Loss of episode 226 = 21.274834\n","Loss of episode 227 = 21.269997\n","Loss of episode 228 = 21.275978\n","Loss of episode 229 = 21.272808\n","Loss of episode 230 = 21.267128\n","Loss of episode 231 = 21.25231\n","Loss of episode 232 = 21.247374\n","Loss of episode 233 = 21.240986\n","Loss of episode 234 = 21.24079\n","Loss of episode 235 = 21.261127\n","Loss of episode 236 = 21.266998\n","Loss of episode 237 = 21.23284\n","Loss of episode 238 = 21.217388\n","Loss of episode 239 = 21.194248\n","Loss of episode 240 = 21.20484\n","Loss of episode 241 = 21.187523\n","Loss of episode 242 = 21.228703\n","Loss of episode 243 = 21.235935\n","Loss of episode 244 = 21.25324\n","Loss of episode 245 = 21.263197\n","Loss of episode 246 = 21.236696\n","Loss of episode 247 = 21.247528\n","Loss of episode 248 = 21.26039\n","Loss of episode 249 = 21.250708\n","Loss of episode 250 = 21.234379\n","Loss of episode 251 = 21.245548\n","Loss of episode 252 = 21.249249\n","Loss of episode 253 = 21.259075\n","Loss of episode 254 = 21.210983\n","Loss of episode 255 = 21.177326\n","Loss of episode 256 = 21.179985\n","Loss of episode 257 = 21.192015\n","Loss of episode 258 = 21.203485\n","Loss of episode 259 = 21.245129\n","Loss of episode 260 = 21.227781\n","Loss of episode 261 = 21.204523\n","Loss of episode 262 = 21.168344\n","Loss of episode 263 = 21.169395\n","Loss of episode 264 = 21.151115\n","Loss of episode 265 = 21.139803\n","Loss of episode 266 = 21.129364\n","Loss of episode 267 = 21.121075\n","Loss of episode 268 = 21.116194\n","Loss of episode 269 = 21.124765\n","Loss of episode 270 = 21.130775\n","Loss of episode 271 = 21.141167\n","Loss of episode 272 = 21.15654\n","Loss of episode 273 = 21.138985\n","Loss of episode 274 = 21.118616\n","Loss of episode 275 = 21.12413\n","Loss of episode 276 = 21.107056\n","Loss of episode 277 = 21.096838\n","Loss of episode 278 = 21.129139\n","Loss of episode 279 = 21.129559\n","Loss of episode 280 = 21.105974\n","Loss of episode 281 = 21.074919\n","Loss of episode 282 = 21.0439\n","Loss of episode 283 = 21.047668\n","Loss of episode 284 = 21.036444\n","Loss of episode 285 = 21.02317\n","Loss of episode 286 = 21.021889\n","Loss of episode 287 = 21.023659\n","Loss of episode 288 = 21.01007\n","Loss of episode 289 = 21.008762\n","Loss of episode 290 = 21.00132\n","Loss of episode 291 = 20.989386\n","Loss of episode 292 = 20.996761\n","Loss of episode 293 = 20.996937\n","Loss of episode 294 = 21.003729\n","Loss of episode 295 = 21.00785\n","Loss of episode 296 = 21.018091\n","Loss of episode 297 = 21.00838\n","Loss of episode 298 = 21.010506\n","Loss of episode 299 = 21.003567\n","Loss of episode 300 = 20.992977\n","Loss of episode 301 = 20.993687\n","Loss of episode 302 = 21.015375\n","Loss of episode 303 = 20.982811\n","Loss of episode 304 = 21.001629\n","Loss of episode 305 = 21.003931\n","Loss of episode 306 = 21.013622\n","Loss of episode 307 = 21.054384\n","Loss of episode 308 = 21.068804\n","Loss of episode 309 = 21.023945\n","Loss of episode 310 = 21.00422\n","Loss of episode 311 = 20.975277\n","Loss of episode 312 = 20.958282\n","Loss of episode 313 = 20.969799\n","Loss of episode 314 = 20.964577\n","Loss of episode 315 = 20.943943\n","Loss of episode 316 = 20.936092\n","Loss of episode 317 = 20.940958\n","Loss of episode 318 = 20.956165\n","Loss of episode 319 = 20.95496\n","Loss of episode 320 = 20.97637\n","Loss of episode 321 = 20.981144\n","Loss of episode 322 = 20.975122\n","Loss of episode 323 = 20.971584\n","Loss of episode 324 = 20.967335\n","Loss of episode 325 = 20.94965\n","Loss of episode 326 = 20.945156\n","Loss of episode 327 = 20.953495\n","Loss of episode 328 = 20.968443\n","Loss of episode 329 = 20.96464\n","Loss of episode 330 = 20.919918\n","Loss of episode 331 = 20.926779\n","Loss of episode 332 = 20.925106\n","Loss of episode 333 = 20.92154\n","Loss of episode 334 = 20.954153\n","Loss of episode 335 = 20.919262\n","Loss of episode 336 = 20.902431\n","Loss of episode 337 = 20.888935\n","Loss of episode 338 = 20.89701\n","Loss of episode 339 = 20.92502\n","Loss of episode 340 = 20.93942\n","Loss of episode 341 = 20.91178\n","Loss of episode 342 = 20.892607\n","Loss of episode 343 = 20.863941\n","Loss of episode 344 = 20.895992\n","Loss of episode 345 = 20.89183\n","Loss of episode 346 = 20.877247\n","Loss of episode 347 = 20.883186\n","Loss of episode 348 = 20.87873\n","Loss of episode 349 = 20.892925\n","Loss of episode 350 = 20.884216\n","Loss of episode 351 = 20.875036\n","Loss of episode 352 = 20.838703\n","Loss of episode 353 = 20.841398\n","Loss of episode 354 = 20.846874\n","Loss of episode 355 = 20.85089\n","Loss of episode 356 = 20.84346\n","Loss of episode 357 = 20.823694\n","Loss of episode 358 = 20.810562\n","Loss of episode 359 = 20.807789\n","Loss of episode 360 = 20.819397\n","Loss of episode 361 = 20.829449\n","Loss of episode 362 = 20.816715\n","Loss of episode 363 = 20.83402\n","Loss of episode 364 = 20.851881\n","Loss of episode 365 = 20.851255\n","Loss of episode 366 = 20.830206\n","Loss of episode 367 = 20.810204\n","Loss of episode 368 = 20.828812\n","Loss of episode 369 = 20.81277\n","Loss of episode 370 = 20.81858\n","Loss of episode 371 = 20.839333\n","Loss of episode 372 = 20.836712\n","Loss of episode 373 = 20.873463\n","Loss of episode 374 = 20.846533\n","Loss of episode 375 = 20.873951\n","Loss of episode 376 = 20.855677\n","Loss of episode 377 = 20.823776\n","Loss of episode 378 = 20.826107\n","Loss of episode 379 = 20.823431\n","Loss of episode 380 = 20.796535\n","Loss of episode 381 = 20.79527\n","Loss of episode 382 = 20.786003\n","Loss of episode 383 = 20.77345\n","Loss of episode 384 = 20.7664\n","Loss of episode 385 = 20.767628\n","Loss of episode 386 = 20.775597\n","Loss of episode 387 = 20.76271\n","Loss of episode 388 = 20.776003\n","Loss of episode 389 = 20.771412\n","Loss of episode 390 = 20.773184\n","Loss of episode 391 = 20.763653\n","Loss of episode 392 = 20.75473\n","Loss of episode 393 = 20.76129\n","Loss of episode 394 = 20.744007\n","Loss of episode 395 = 20.771875\n","Loss of episode 396 = 20.794563\n","Loss of episode 397 = 20.783926\n","Loss of episode 398 = 20.794085\n","Loss of episode 399 = 20.82325\n","Loss of episode 400 = 20.813267\n","Loss of episode 401 = 20.793013\n","Loss of episode 402 = 20.781895\n","Loss of episode 403 = 20.76828\n","Loss of episode 404 = 20.764717\n","Loss of episode 405 = 20.774172\n","Loss of episode 406 = 20.748198\n","Loss of episode 407 = 20.75463\n","Loss of episode 408 = 20.752768\n","Loss of episode 409 = 20.74853\n","Loss of episode 410 = 20.743168\n","Loss of episode 411 = 20.76026\n","Loss of episode 412 = 20.736778\n","Loss of episode 413 = 20.736326\n","Loss of episode 414 = 20.739706\n","Loss of episode 415 = 20.7446\n","Loss of episode 416 = 20.740288\n","Loss of episode 417 = 20.75723\n","Loss of episode 418 = 20.746265\n","Loss of episode 419 = 20.724394\n","Loss of episode 420 = 20.71788\n","Loss of episode 421 = 20.71362\n","Loss of episode 422 = 20.71336\n","Loss of episode 423 = 20.72723\n","Loss of episode 424 = 20.704933\n","Loss of episode 425 = 20.70116\n","Loss of episode 426 = 20.702682\n","Loss of episode 427 = 20.706564\n","Loss of episode 428 = 20.69148\n","Loss of episode 429 = 20.699867\n","Loss of episode 430 = 20.689528\n","Loss of episode 431 = 20.693985\n","Loss of episode 432 = 20.695442\n","Loss of episode 433 = 20.697243\n","Loss of episode 434 = 20.716064\n","Loss of episode 435 = 20.699745\n","Loss of episode 436 = 20.702637\n","Loss of episode 437 = 20.702322\n","Loss of episode 438 = 20.726948\n","Loss of episode 439 = 20.739494\n","Loss of episode 440 = 20.753305\n","Loss of episode 441 = 20.732002\n","Loss of episode 442 = 20.694609\n","Loss of episode 443 = 20.69545\n","Loss of episode 444 = 20.700132\n","Loss of episode 445 = 20.68889\n","Loss of episode 446 = 20.69339\n","Loss of episode 447 = 20.693409\n","Loss of episode 448 = 20.675404\n","Loss of episode 449 = 20.677322\n","Loss of episode 450 = 20.679409\n","Loss of episode 451 = 20.658422\n","Loss of episode 452 = 20.66883\n","Loss of episode 453 = 20.656624\n","Loss of episode 454 = 20.660591\n","Loss of episode 455 = 20.654135\n","Loss of episode 456 = 20.662727\n","Loss of episode 457 = 20.668377\n","Loss of episode 458 = 20.66428\n","Loss of episode 459 = 20.655432\n","Loss of episode 460 = 20.655334\n","Loss of episode 461 = 20.644604\n","Loss of episode 462 = 20.646385\n","Loss of episode 463 = 20.641785\n","Loss of episode 464 = 20.646793\n","Loss of episode 465 = 20.637318\n","Loss of episode 466 = 20.630608\n","Loss of episode 467 = 20.64064\n","Loss of episode 468 = 20.647905\n","Loss of episode 469 = 20.651966\n","Loss of episode 470 = 20.651754\n","Loss of episode 471 = 20.645477\n","Loss of episode 472 = 20.6357\n","Loss of episode 473 = 20.65147\n","Loss of episode 474 = 20.643538\n","Loss of episode 475 = 20.62627\n","Loss of episode 476 = 20.636234\n","Loss of episode 477 = 20.634205\n","Loss of episode 478 = 20.634426\n","Loss of episode 479 = 20.643797\n","Loss of episode 480 = 20.621578\n","Loss of episode 481 = 20.62216\n","Loss of episode 482 = 20.62297\n","Loss of episode 483 = 20.635517\n","Loss of episode 484 = 20.641903\n","Loss of episode 485 = 20.635643\n","Loss of episode 486 = 20.644753\n","Loss of episode 487 = 20.660465\n","Loss of episode 488 = 20.66632\n","Loss of episode 489 = 20.692333\n","Loss of episode 490 = 20.67763\n","Loss of episode 491 = 20.670937\n","Loss of episode 492 = 20.673248\n","Loss of episode 493 = 20.692762\n","Loss of episode 494 = 20.68673\n","Loss of episode 495 = 20.660683\n","Loss of episode 496 = 20.63393\n","Loss of episode 497 = 20.632027\n","Loss of episode 498 = 20.622803\n","Loss of episode 499 = 20.614487\n","Loss of episode 500 = 20.614023\n","Loss of episode 501 = 20.604622\n","Loss of episode 502 = 20.596382\n","Loss of episode 503 = 20.59762\n","Loss of episode 504 = 20.592356\n","Loss of episode 505 = 20.602144\n","Loss of episode 506 = 20.585064\n","Loss of episode 507 = 20.60299\n","Loss of episode 508 = 20.585836\n","Loss of episode 509 = 20.588293\n","Loss of episode 510 = 20.60709\n","Loss of episode 511 = 20.601254\n","Loss of episode 512 = 20.615797\n","Loss of episode 513 = 20.613503\n","Loss of episode 514 = 20.601479\n","Loss of episode 515 = 20.601803\n","Loss of episode 516 = 20.602022\n","Loss of episode 517 = 20.608768\n","Loss of episode 518 = 20.630222\n","Loss of episode 519 = 20.63435\n","Loss of episode 520 = 20.619911\n","Loss of episode 521 = 20.624838\n","Loss of episode 522 = 20.619045\n","Loss of episode 523 = 20.619987\n","Loss of episode 524 = 20.618132\n","Loss of episode 525 = 20.640394\n","Loss of episode 526 = 20.64244\n","Loss of episode 527 = 20.615833\n","Loss of episode 528 = 20.592552\n","Loss of episode 529 = 20.59889\n","Loss of episode 530 = 20.584423\n","Loss of episode 531 = 20.582975\n","Loss of episode 532 = 20.584913\n","Loss of episode 533 = 20.577103\n","Loss of episode 534 = 20.589886\n","Loss of episode 535 = 20.586933\n","Loss of episode 536 = 20.588268\n","Loss of episode 537 = 20.597752\n","Loss of episode 538 = 20.574814\n","Loss of episode 539 = 20.563917\n","Loss of episode 540 = 20.570524\n","Loss of episode 541 = 20.560139\n","Loss of episode 542 = 20.561016\n","Loss of episode 543 = 20.559515\n","Loss of episode 544 = 20.550436\n","Loss of episode 545 = 20.56436\n","Loss of episode 546 = 20.573532\n","Loss of episode 547 = 20.571396\n","Loss of episode 548 = 20.57778\n","Loss of episode 549 = 20.564865\n","Loss of episode 550 = 20.545082\n","Loss of episode 551 = 20.552685\n","Loss of episode 552 = 20.548172\n","Loss of episode 553 = 20.548597\n","Loss of episode 554 = 20.558365\n","Loss of episode 555 = 20.546867\n","Loss of episode 556 = 20.538116\n","Loss of episode 557 = 20.545689\n","Loss of episode 558 = 20.556042\n","Loss of episode 559 = 20.549503\n","Loss of episode 560 = 20.548733\n","Loss of episode 561 = 20.53987\n","Loss of episode 562 = 20.53933\n","Loss of episode 563 = 20.545517\n","Loss of episode 564 = 20.53628\n","Loss of episode 565 = 20.542648\n","Loss of episode 566 = 20.543217\n","Loss of episode 567 = 20.541866\n","Loss of episode 568 = 20.549664\n","Loss of episode 569 = 20.550861\n","Loss of episode 570 = 20.55536\n","Loss of episode 571 = 20.555725\n","Loss of episode 572 = 20.553741\n","Loss of episode 573 = 20.556744\n","Loss of episode 574 = 20.546398\n","Loss of episode 575 = 20.54861\n","Loss of episode 576 = 20.56248\n","Loss of episode 577 = 20.551945\n","Loss of episode 578 = 20.562124\n","Loss of episode 579 = 20.557323\n","Loss of episode 580 = 20.551577\n","Loss of episode 581 = 20.536886\n","Loss of episode 582 = 20.53033\n","Loss of episode 583 = 20.53593\n","Loss of episode 584 = 20.537237\n","Loss of episode 585 = 20.545609\n","Loss of episode 586 = 20.540451\n","Loss of episode 587 = 20.544477\n","Loss of episode 588 = 20.539139\n","Loss of episode 589 = 20.56872\n","Loss of episode 590 = 20.559607\n","Loss of episode 591 = 20.544569\n","Loss of episode 592 = 20.5523\n","Loss of episode 593 = 20.561617\n","Loss of episode 594 = 20.550829\n","Loss of episode 595 = 20.533682\n","Loss of episode 596 = 20.546295\n","Loss of episode 597 = 20.544865\n","Loss of episode 598 = 20.528473\n","Loss of episode 599 = 20.539679\n","Loss of episode 600 = 20.541897\n","Loss of episode 601 = 20.52928\n","Loss of episode 602 = 20.532696\n","Loss of episode 603 = 20.527079\n","Loss of episode 604 = 20.533958\n","Loss of episode 605 = 20.51986\n","Loss of episode 606 = 20.523794\n","Loss of episode 607 = 20.517277\n","Loss of episode 608 = 20.515326\n","Loss of episode 609 = 20.508354\n","Loss of episode 610 = 20.507292\n","Loss of episode 611 = 20.512808\n","Loss of episode 612 = 20.510923\n","Loss of episode 613 = 20.524334\n","Loss of episode 614 = 20.537624\n","Loss of episode 615 = 20.53704\n","Loss of episode 616 = 20.525242\n","Loss of episode 617 = 20.524227\n","Loss of episode 618 = 20.523869\n","Loss of episode 619 = 20.516884\n","Loss of episode 620 = 20.526588\n","Loss of episode 621 = 20.509892\n","Loss of episode 622 = 20.526318\n","Loss of episode 623 = 20.522577\n","Loss of episode 624 = 20.52594\n","Loss of episode 625 = 20.541885\n","Loss of episode 626 = 20.53326\n","Loss of episode 627 = 20.52054\n","Loss of episode 628 = 20.521513\n","Loss of episode 629 = 20.533718\n","Loss of episode 630 = 20.534828\n","Loss of episode 631 = 20.524849\n","Loss of episode 632 = 20.518166\n","Loss of episode 633 = 20.513947\n","Loss of episode 634 = 20.528572\n","Loss of episode 635 = 20.524801\n","Loss of episode 636 = 20.51332\n","Loss of episode 637 = 20.503246\n","Loss of episode 638 = 20.504868\n","Loss of episode 639 = 20.510319\n","Loss of episode 640 = 20.504436\n","Loss of episode 641 = 20.500858\n","Loss of episode 642 = 20.504969\n","Loss of episode 643 = 20.500027\n","Loss of episode 644 = 20.500296\n","Loss of episode 645 = 20.496807\n","Loss of episode 646 = 20.491781\n","Loss of episode 647 = 20.499317\n","Loss of episode 648 = 20.491915\n","Loss of episode 649 = 20.492481\n","Loss of episode 650 = 20.500906\n","Loss of episode 651 = 20.499733\n","Loss of episode 652 = 20.488277\n","Loss of episode 653 = 20.495628\n","Loss of episode 654 = 20.48973\n","Loss of episode 655 = 20.48706\n","Loss of episode 656 = 20.489492\n","Loss of episode 657 = 20.498236\n","Loss of episode 658 = 20.49952\n","Loss of episode 659 = 20.50079\n","Loss of episode 660 = 20.515041\n","Loss of episode 661 = 20.520924\n","Loss of episode 662 = 20.499508\n","Loss of episode 663 = 20.494627\n","Loss of episode 664 = 20.493628\n","Loss of episode 665 = 20.490025\n","Loss of episode 666 = 20.501242\n","Loss of episode 667 = 20.487154\n","Loss of episode 668 = 20.493137\n","Loss of episode 669 = 20.499561\n","Loss of episode 670 = 20.479115\n","Loss of episode 671 = 20.47734\n","Loss of episode 672 = 20.473524\n","Loss of episode 673 = 20.479591\n","Loss of episode 674 = 20.479942\n","Loss of episode 675 = 20.47536\n","Loss of episode 676 = 20.48393\n","Loss of episode 677 = 20.46931\n","Loss of episode 678 = 20.464333\n","Loss of episode 679 = 20.467342\n","Loss of episode 680 = 20.472855\n","Loss of episode 681 = 20.47825\n","Loss of episode 682 = 20.482779\n","Loss of episode 683 = 20.461914\n","Loss of episode 684 = 20.48001\n","Loss of episode 685 = 20.468231\n","Loss of episode 686 = 20.469408\n","Loss of episode 687 = 20.475794\n","Loss of episode 688 = 20.464958\n","Loss of episode 689 = 20.472343\n","Loss of episode 690 = 20.471836\n","Loss of episode 691 = 20.460655\n","Loss of episode 692 = 20.463566\n","Loss of episode 693 = 20.450457\n","Loss of episode 694 = 20.464458\n","Loss of episode 695 = 20.473587\n","Loss of episode 696 = 20.46259\n","Loss of episode 697 = 20.46323\n","Loss of episode 698 = 20.467257\n","Loss of episode 699 = 20.45441\n","Loss of episode 700 = 20.46918\n","Loss of episode 701 = 20.460173\n","Loss of episode 702 = 20.478031\n","Loss of episode 703 = 20.48578\n","Loss of episode 704 = 20.473135\n","Loss of episode 705 = 20.471378\n","Loss of episode 706 = 20.46996\n","Loss of episode 707 = 20.483662\n","Loss of episode 708 = 20.465607\n","Loss of episode 709 = 20.473267\n","Loss of episode 710 = 20.47279\n","Loss of episode 711 = 20.480736\n","Loss of episode 712 = 20.472874\n","Loss of episode 713 = 20.4761\n","Loss of episode 714 = 20.460358\n","Loss of episode 715 = 20.450684\n","Loss of episode 716 = 20.444042\n","Loss of episode 717 = 20.449158\n","Loss of episode 718 = 20.442492\n","Loss of episode 719 = 20.446148\n","Loss of episode 720 = 20.4458\n","Loss of episode 721 = 20.448143\n","Loss of episode 722 = 20.445625\n","Loss of episode 723 = 20.44468\n","Loss of episode 724 = 20.458496\n","Loss of episode 725 = 20.464054\n","Loss of episode 726 = 20.478098\n","Loss of episode 727 = 20.466629\n","Loss of episode 728 = 20.462614\n","Loss of episode 729 = 20.47477\n","Loss of episode 730 = 20.459526\n","Loss of episode 731 = 20.457832\n","Loss of episode 732 = 20.460161\n","Loss of episode 733 = 20.460964\n","Loss of episode 734 = 20.457428\n","Loss of episode 735 = 20.446918\n","Loss of episode 736 = 20.447006\n","Loss of episode 737 = 20.44635\n","Loss of episode 738 = 20.430973\n","Loss of episode 739 = 20.441692\n","Loss of episode 740 = 20.44833\n","Loss of episode 741 = 20.464981\n","Loss of episode 742 = 20.466343\n","Loss of episode 743 = 20.459782\n","Loss of episode 744 = 20.481588\n","Loss of episode 745 = 20.47304\n","Loss of episode 746 = 20.45886\n","Loss of episode 747 = 20.449877\n","Loss of episode 748 = 20.455898\n","Loss of episode 749 = 20.445843\n","Loss of episode 750 = 20.464584\n","Loss of episode 751 = 20.473217\n","Loss of episode 752 = 20.469631\n","Loss of episode 753 = 20.462097\n","Loss of episode 754 = 20.448828\n","Loss of episode 755 = 20.448053\n","Loss of episode 756 = 20.428934\n","Loss of episode 757 = 20.436638\n","Loss of episode 758 = 20.432703\n","Loss of episode 759 = 20.440342\n","Loss of episode 760 = 20.431774\n","Loss of episode 761 = 20.444435\n","Loss of episode 762 = 20.43209\n","Loss of episode 763 = 20.429546\n","Loss of episode 764 = 20.425621\n","Loss of episode 765 = 20.427656\n","Loss of episode 766 = 20.438332\n","Loss of episode 767 = 20.441557\n","Loss of episode 768 = 20.433441\n","Loss of episode 769 = 20.426039\n","Loss of episode 770 = 20.42884\n","Loss of episode 771 = 20.42419\n","Loss of episode 772 = 20.42838\n","Loss of episode 773 = 20.436907\n","Loss of episode 774 = 20.44968\n","Loss of episode 775 = 20.435522\n","Loss of episode 776 = 20.432165\n","Loss of episode 777 = 20.433317\n","Loss of episode 778 = 20.424818\n","Loss of episode 779 = 20.441149\n","Loss of episode 780 = 20.426664\n","Loss of episode 781 = 20.415989\n","Loss of episode 782 = 20.416798\n","Loss of episode 783 = 20.426123\n","Loss of episode 784 = 20.426384\n","Loss of episode 785 = 20.434725\n","Loss of episode 786 = 20.443935\n","Loss of episode 787 = 20.429462\n","Loss of episode 788 = 20.437412\n","Loss of episode 789 = 20.433865\n","Loss of episode 790 = 20.441303\n","Loss of episode 791 = 20.430809\n","Loss of episode 792 = 20.441162\n","Loss of episode 793 = 20.426237\n","Loss of episode 794 = 20.435438\n","Loss of episode 795 = 20.434454\n","Loss of episode 796 = 20.426489\n","Loss of episode 797 = 20.412258\n","Loss of episode 798 = 20.423344\n","Loss of episode 799 = 20.429075\n","\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# feature_size, hidden_size, num_layers, output_size, (number_heads)\n","\n","model = MTORL(state_feature,512,2,3).to(device)\n","num_epoch = 800     # 800\n","Lambda = 1.5        # 1.5\n","Mu = 0.08           # 0.08\n","\n","# optimizer = torch.optim.SGD(net.parameters(),lr=0.5)\n","# optimizer = torch.optim.Adam(gru.parameters(),lr=0.001)\n","optimizer = torch.optim.AdamW(model.parameters(),lr=0.001)\n","\n","# loss_func = torch.nn.MSELoss()            #MSE for regression\n","loss_func = torch.nn.CrossEntropyLoss()     #CE for classification\n","Loss=[]\n","\n","for t in range(num_epoch):\n","    aver_loss = 0\n","    model.train()\n","    for batch, data in enumerate(data_train_loader):\n","        x, y ,z = data\n","        action_prediction, click_prediction = model(x)\n","        # print(action_prediction.shape)\n","        action_prediction = torch.transpose(action_prediction, dim0=0, dim1=1)\n","        click_prediction = torch.transpose(click_prediction, dim0=0, dim1=1)\n","        # Entropy = torch.mean(shannon_entropy(action_prediction))\n","        # loss = loss_func(action_prediction,y) + Mu * loss_func(click_prediction,z) \\\n","        # - Lambda * Entropy \n","\n","        # calculate DPO_loss\n","        click_list = torch.sum(z,dim=1)\n","        max_index = torch.argmax(click_list)\n","        min_index = torch.argmin(click_list)\n","        x_w = x[max_index].unsqueeze(0)\n","        x_l = x[min_index].unsqueeze(0)\n","        pi_w, _ = model(x_w)\n","        pi_l, _ = model(x_l)\n","        pi_w = pi_w.squeeze(1)\n","        pi_l = pi_l.squeeze(1)\n","\n","        # print((pi_w.squeeze(1)).shape)\n","        # print('-'*100)\n","        # print(pi_w[0])\n","        # print(pi_l[0])\n","\n","        DPO_loss = DPO_loss_func(pi_l,pi_w)\n","        # print(DPO_loss)\n","\n","        loss = loss_func(action_prediction,y) + Mu * loss_func(click_prediction,z) + Lambda * DPO_loss\n","        \n","        optimizer.zero_grad() \n","        loss.backward()    \n","        optimizer.step() \n","        aver_loss += loss\n","    aver_loss /= batch\n","    aver_loss=aver_loss.cpu().detach().numpy()\n","    print('Loss of episode %s ='%t,aver_loss)\n","    Loss.append(aver_loss)\n","plt.plot(Loss,color='r')\n","print()"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690978204313,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"lxxPbtYyOBS6","outputId":"d2b4bdae-3248-45d5-9a2c-25bb1d8fc299"},"outputs":[{"name":"stdout","output_type":"stream","text":["6680084\n"]}],"source":["device = torch.device(\"cuda\")\n","model_test = MTORL(state_feature,512,2,3).to(device)\n","total = sum([param.nelement() for param in model_test.parameters()])\n","print(total)\n","# gru_test"]},{"cell_type":"markdown","metadata":{"id":"Qiqn4DAJOBS7"},"source":["## Training accuracy"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690978204313,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"BUoPP1KjOBS7","outputId":"4ec4f842-dd61-42d5-e7f4-01bea8ab6944"},"outputs":[{"name":"stdout","output_type":"stream","text":["acc = 0.951015625\n"]}],"source":["Truth = []\n","Pred = []\n","aver_loss = 0\n","total_acc = 0\n","Entropy = 0\n","model.train()\n","for batch, data in enumerate(data_train_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    x, y, z = data\n","    # print(x)\n","    # prediction =net(x)\n","    test, _ = model(x)\n","    test = torch.transpose(test, dim0=0, dim1=1)\n","    # print(test)\n","    # print(y)\n","    # print(shannon_entropy(test).shape)\n","    # aver_entropy = sum(torch.mean(shannon_entropy(test),dim=1))\n","    # Entropy += aver_entropy\n","\n","    a = torch.argmax(test,dim = 2).cpu().data.numpy()\n","    b = torch.argmax(y,dim = 2).cpu().data.numpy()\n","    # print(a)\n","    # print(b)\n","    # print(sum(a==b))\n","    aver_acc = sum(sum(a==b))/length\n","    # print(aver_acc)\n","    total_acc += aver_acc\n","    # print(total_acc)\n","# print(batch)\n","print('acc =', total_acc/(0.8*max_number_traj))\n","# print('Entropy = ', Entropy/(0.8*max_number_traj))"]},{"cell_type":"markdown","metadata":{},"source":["## Validation accuracy (Hyperparameter tuning)"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Action acc = 0.6882500000000001\n","CTR acc = 0.7771250000000002\n"]}],"source":["from sklearn.metrics import roc_curve, auc\n","\n","\n","Loss=[]\n","Truth = []\n","Pred = []\n","aver_loss = 0\n","total_acc = 0\n","total_ctr = 0\n","total_auc = 0\n","Entropy = 0\n","num = 0\n","\n","# Click_pred = np.zeros((int(0.2*max_number_traj),length))\n","# Click_true = np.zeros((int(0.2*max_number_traj),length))\n","\n","for batch, data in enumerate(data_val_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    model.eval()\n","    x, y, z = data\n","    test, click = model(x)\n","    click_pred = torch.squeeze(click).cpu().data.numpy()\n","    click_true = torch.squeeze(z).cpu().data.numpy()\n","    click_pred[click_pred<1/2] = 0\n","    click_pred[click_pred>=1/2] = 1\n","    # Click_pred[batch] = click_pred\n","    # Click_true[batch] = click_true\n","\n","    p = test[:,0].cpu()\n","    # aver_entropy = torch.mean(shannon_entropy(p))\n","    # # print('aver_entropy =', aver_entropy)\n","    # Entropy += aver_entropy\n","    # print(test)\n","    a = torch.argmax(test,dim = 2).cpu().data.numpy().T[0]\n","    b = torch.argmax(y,dim = 2).cpu().data.numpy()[0]\n","    # print('-----------')\n","    # print('pred =',a.T[0])\n","    # print('true =',b[0])\n","    aver_acc = sum(a==b)/length\n","    total_acc += aver_acc\n","\n","    # print('-----------')\n","    # print('pred =', click_pred)\n","    # print('true =', click_true) \n","    aver_ctr = sum(click_pred==click_true)/length\n","    total_ctr += aver_ctr\n","\n","\n","# fpr, tpr, thresholds = roc_curve(Click_true.reshape(-1), Click_pred.reshape(-1),pos_label=1)\n","# total_auc = auc(fpr, tpr)\n","# print('CTR AUC =', total_auc)\n","print('Action acc =', total_acc/(0.1*max_number_traj))\n","print('CTR acc =', total_ctr/(0.1*max_number_traj))\n","# print('Entropy = ', Entropy/(0.1*max_number_traj))\n"]},{"cell_type":"markdown","metadata":{"id":"6Hi558JBOBS8"},"source":["## Inference accuracy"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1541,"status":"ok","timestamp":1690978205851,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"dH6BC5O2OBS8","outputId":"f0746245-4215-490b-c3b9-ee67948cc407"},"outputs":[{"name":"stdout","output_type":"stream","text":["Action acc = 0.68025\n","CTR acc = 0.7841250000000001\n"]}],"source":["from sklearn.metrics import roc_curve, auc\n","\n","\n","Loss=[]\n","Truth = []\n","Pred = []\n","aver_loss = 0\n","total_acc = 0\n","total_ctr = 0\n","total_auc = 0\n","Entropy = 0\n","num = 0\n","\n","# Click_pred = np.zeros((int(0.2*max_number_traj),length))\n","# Click_true = np.zeros((int(0.2*max_number_traj),length))\n","\n","for batch, data in enumerate(data_test_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    model.eval()\n","    x, y, z = data\n","    test, click = model(x)\n","    click_pred = torch.squeeze(click).cpu().data.numpy()\n","    click_true = torch.squeeze(z).cpu().data.numpy()\n","    click_pred[click_pred<1/2] = 0\n","    click_pred[click_pred>=1/2] = 1\n","    # Click_pred[batch] = click_pred\n","    # Click_true[batch] = click_true\n","\n","    p = test[:,0].cpu()\n","    # aver_entropy = torch.mean(shannon_entropy(p))\n","    # print('aver_entropy =', aver_entropy)\n","    # Entropy += aver_entropy\n","    # print(test)\n","    a = torch.argmax(test,dim = 2).cpu().data.numpy().T[0]\n","    b = torch.argmax(y,dim = 2).cpu().data.numpy()[0]\n","    # print('-----------')\n","    # print('pred =',a.T[0])\n","    # print('true =',b[0])\n","    aver_acc = sum(a==b)/length\n","    total_acc += aver_acc\n","\n","    # print('-----------')\n","    # print('pred =', click_pred)\n","    # print('true =', click_true) \n","    aver_ctr = sum(click_pred==click_true)/length\n","    total_ctr += aver_ctr\n","\n","\n","# fpr, tpr, thresholds = roc_curve(Click_true.reshape(-1), Click_pred.reshape(-1),pos_label=1)\n","# total_auc = auc(fpr, tpr)\n","# print('CTR AUC =', total_auc)\n","print('Action acc =', total_acc/(0.1*max_number_traj))\n","print('CTR acc =', total_ctr/(0.1*max_number_traj))\n","# print('Entropy = ', Entropy/(0.1*max_number_traj))\n"]},{"cell_type":"markdown","metadata":{"id":"n6ecXL0POBS8"},"source":["## AUC"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":736,"status":"ok","timestamp":1690978206585,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"45g3DhluOBS8","outputId":"9e567364-e555-4ef1-be01-fdcdc6de971b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Average AUC = [0.82005238 0.8330463  0.82142067]\n"]}],"source":["from sklearn.metrics import precision_score, roc_curve, auc\n","n_classes = 3\n","# length = 8\n","aver_auc = np.zeros(n_classes)\n","total_precision = 0\n","Y_score = np.zeros(((length, int(0.1*max_number_traj), n_classes)))\n","# print(Y_score.shape)\n","Y_label = np.zeros(((length, int(0.1*max_number_traj), n_classes)))\n","for batch, data in enumerate(data_test_loader):\n","    model.eval()\n","    x, y, z = data\n","    # print(x)\n","    test, _ = model(x)\n","    y_score = test[:,0,:].cpu().data.numpy()\n","    y_label = y[0].cpu().data.numpy()\n","    # print(batch)\n","    for i in range(length):\n","        Y_score[i][batch] = y_score[i]\n","        Y_label[i][batch] = y_label[i]\n","        # Y_score[i][batch] = [1/3,1/3,1/3]\n","        # Y_label[i][batch] = y_label[i]\n","        # print(y_score[i])\n","        # print(y_label[i])\n","fpr = dict()\n","tpr = dict()\n","roc_auc = dict()\n","aver_auc = np.zeros(n_classes)\n","for t in range(length):\n","    for i in range(n_classes):\n","        fpr[i], tpr[i], _ = roc_curve(Y_label[t][:, i], Y_score[t][:, i])\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","        # aver_auc[i] += auc(fpr[i], tpr[i])\n","        aver_auc[i] += roc_auc[i]/length\n","    # print('step %s AUC ='%(t+1),roc_auc)\n","print('Average AUC =', aver_auc)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1690978206585,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"Q-5eCoFCOBS8","outputId":"1fe76151-c1a2-4ced-8acf-4a37d2df9e6e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Aver_F1 = 0.673 Aver_Precision = 0.6775 Aver_Recall = 0.6742\n"]}],"source":["from sklearn.metrics import f1_score, precision_score, recall_score\n","\n","aver_f1 = 0\n","aver_p = 0\n","aver_r = 0\n","\n","# print(y_pred)\n","# print(y_true)\n","for i in range(length):\n","  y_true = np.argmax(Y_label[i],axis = 1)\n","  y_pred = np.argmax(Y_score[i],axis = 1)\n","  f1 = round(f1_score(y_true, y_pred, average='macro' ),4)\n","  p = round(precision_score(y_true, y_pred, average='macro'),4)\n","  r = round(recall_score(y_true, y_pred, average='macro'),4)\n","  aver_f1 += f1/length\n","  aver_p += p/length\n","  aver_r += r/length\n","print('Aver_F1 =', round(aver_f1,4), 'Aver_Precision =', round(aver_p,4), 'Aver_Recall =', round(aver_r,4))"]},{"cell_type":"markdown","metadata":{},"source":["## Budget Allocation"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["test_number = int(0.1*max_number_traj)\n","True_Click_matrix = np.zeros((test_number,length))\n","True_Action_matrix = np.zeros((test_number,length))\n","Click_pred_matrix = np.zeros((test_number,length))\n","Action_pred_matrix = np.zeros((test_number,length))\n","\n","for batch, data in enumerate(data_test_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    model.eval()\n","    x, y, z = data\n","    action_true = torch.squeeze(torch.argmax(y,dim = 2)).cpu().data.numpy()\n","    click_true = torch.squeeze(z).cpu().data.numpy()\n","    True_Click_matrix[batch] = click_true\n","    # True_Action_count += np.sum(action_true, axis = 0)\n","    \n","    action, click = model(x)\n","    action_pred = torch.squeeze(torch.argmax(action,dim = 2)).cpu().data.numpy()\n","    # print(action_true == action_pred)\n","    Action_pred_matrix[batch] = (action_true == action_pred)\n","    click_pred = torch.squeeze(click).cpu().data.numpy()\n","    Click_pred_matrix[batch] = click_pred\n","\n","# print(True_Click_matrix)"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True click number =  1468.0\n","Ture_CTR = 0.1835\n","Exposure number =  3000.0\n","Click number =  769.0\n","Policy_CTR = 0.25633333333333336\n"]}],"source":["choose_number = 150\n","True_cost = 0.1 * max_number_traj * length\n","True_Click_number = sum(sum(True_Click_matrix))\n","User_choose = np.zeros((test_number,length))\n","\n","print('True click number = ', True_Click_number)\n","True_CTR = True_Click_number/(0.1 * max_number_traj * length) \n","print('Ture_CTR =',True_CTR)\n","for i in range(length):\n","    tmp = Click_pred_matrix[:,i]\n","    idx = np.argsort(tmp)\n","    # print(idx[400:])\n","    # print(tmp[idx[400:]])\n","    User_choose[:,i][idx[test_number-choose_number:]] = 1\n","\n","Exposure = sum(sum(User_choose))\n","Click = sum(sum(User_choose*Action_pred_matrix*True_Click_matrix))\n","Policy_ctr = Click/Exposure\n","print('Exposure number = ', Exposure)\n","print('Click number = ', Click) \n","print('Policy_CTR =', Policy_ctr)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
