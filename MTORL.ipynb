{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1481,"status":"ok","timestamp":1690970114664,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"SHePlV70OBS2"},"outputs":[],"source":["# if torch.cuda.is_available():\n","#     print('CUDNN VERSION:',torch.backends.cudnn.version())\n","#     print('Number CUDA Devices:',torch.cuda.device_count())\n","#     print('CUDA Device Name:',torch.cuda.get_device_name(0))\n","#     print('CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6052,"status":"ok","timestamp":1690970120708,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"kUqW9r2qOBS2","outputId":"606cfa3e-2965-43bb-9f91-818746accbc6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on the GPU\n"]}],"source":["import torch\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")  # you can continue going on here, like cuda:1 cuda:2....etc.\n","    print(\"Running on the GPU\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"Running on the CPU\")\n","# torch.cuda.device_count()"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1690973614130,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"iZBy7-2rOBS3"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","Data = pd.read_csv('kuairand_sequence.csv',index_col=0)\n","# Data = pd.read_csv('kuairand_sequence_user=8000.csv',index_col=0)\n","# data['user_id'].value_counts()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1690973619591,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"3HzbfAYVOBS3","outputId":"2e5a6756-eb74-4e81-f782-0caddeae1081"},"outputs":[],"source":["import torch\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import random\n","from torch.utils.data import Dataset, DataLoader,TensorDataset\n","\n","max_length = 100\n","num_user = len(Data['user_id'].unique())\n","state_feature = len(Data.columns) - 1\n","action_feature = 3\n","\n","uid = Data['user_id'].unique()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2991,"status":"ok","timestamp":1690973742240,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"HdF3rl_LOBS4","outputId":"9a25cd7c-7681-4521-a5f4-db7dc0f9d462"},"outputs":[],"source":["State = np.zeros(((num_user,max_length,state_feature)))\n","Action = np.zeros((num_user,max_length,action_feature))\n","Click = np.zeros((num_user,max_length))\n","\n","j = 0\n","for i in uid:\n","    state_sequence = np.array(Data[Data['user_id']==i])[:-1]\n","    # state_sequence = np.array(data[data['user_id']==i].drop(['short','mid','long'],axis=1))\n","    action_sequence = np.array(Data[Data['user_id']==i][['short','mid','long']])[1:]\n","    click_sequence = np.array(Data[Data['user_id']==i]['is_click'])[1:]\n","    # action_sequence = np.array(data[data['user_id']==i][['short','mid','long']])\n","    # print(action_sequence)\n","    state = np.pad(state_sequence,((0,max_length-len(state_sequence)),(0,0)))[:,1:]\n","    action = np.pad(action_sequence,((0,max_length-len(action_sequence)),(0,0)))\n","    click = np.pad(click_sequence,((0,max_length-len(click_sequence))))\n","    # print(click)\n","    State[j] = state\n","    Action[j] = action\n","    Click[j] = click\n","    j += 1\n","\n","# print(len(State))\n","# print(len(Action))\n","# print(len(Click))\n","# print(Click.shape)"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":822,"status":"ok","timestamp":1690976535806,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"tPsLnRdtOBS4","outputId":"798634a6-86b6-4797-8422-298b81c046ca"},"outputs":[],"source":["length = 20\n","max_number_traj = 4000\n","\n","number_traj = 0\n","def Select_trajectory(X,Y,Z,number_traj):\n","    Select_states = []\n","    Select_actions = []\n","    Select_clicks = []\n","    # for s in range(number_traj):\n","    while True:\n","        i = random.randint(0,len(X)-1)\n","        select_state = X[i]\n","        select_action = Y[i]\n","        select_click = Z[i]\n","        # print(select_state)\n","        # print(select_action)\n","        for k in range(0,max_length):\n","            # print(select_episode[k])\n","            if select_state[k].any() == 0:\n","                max_episode_length = k\n","                break\n","        # print(k)\n","        # print(max_episode_length)\n","        if max_episode_length >= length + 1:\n","            j = random.randint(0,max_episode_length-length-1)\n","            select_state = select_state[j:j+length]\n","            select_action = select_action[j:j+length]\n","            select_click = select_click[j:j+length]\n","            Select_states.append(select_state)\n","            Select_actions.append(select_action)\n","            Select_clicks.append(select_click)\n","            number_traj += 1\n","        # print(select_trajectory)\n","        if number_traj == max_number_traj:\n","            break\n","    return np.array(Select_states),np.array(Select_actions),np.array(Select_clicks)\n","\n","X , Y , Z = Select_trajectory(State,Action,Click,number_traj)"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1690976536378,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"P1xlyTewOBS5"},"outputs":[],"source":["np.random.seed(2023)\n","per = np.random.permutation(X.shape[0])\n","# print(per)\n","X = X[per]\n","Y = Y[per]\n","Z = Z[per]\n","X,Y,Z = torch.from_numpy(X).to(torch.float32).to(device),torch.from_numpy(Y).to(torch.float32).to(device), torch.from_numpy(Z).to(torch.float32).to(device)\n","# print(sum(Y))"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690976536378,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"YB3Wa9SaOBS5","outputId":"c094f597-85a0-49b2-adee-00ee3d42b5d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["3200\n"]}],"source":["def split_data(State, Action, Reward, timestep, input_size, output_size):\n","\n","    # Training set\n","    train_size = int(np.round(0.8 * X.shape[0]))\n","    val_size = int(np.round(0.1 * X.shape[0]))\n","    print(train_size)\n","\n","    # Split training and test set 8:1:1\n","    x_train = X[: train_size, :].reshape(-1, timestep,input_size)\n","    y_train = Y[: train_size].reshape(-1,timestep, output_size)\n","    z_train = Z[: train_size].reshape(-1,timestep, 1)\n","\n","    # x_test = X[train_size:, :].reshape(-1, timestep,input_size)\n","    # y_test = Y[train_size:].reshape(-1,timestep, output_size)\n","    # z_test = Z[train_size:].reshape(-1,timestep, 1)\n","\n","    x_val = X[train_size:train_size+val_size, :].reshape(-1, timestep,input_size)\n","    y_val = Y[train_size:train_size+val_size].reshape(-1,timestep, output_size)\n","    z_val = Z[train_size:train_size+val_size].reshape(-1,timestep, 1) \n","\n","    x_test = X[train_size+val_size:, :].reshape(-1, timestep,input_size)\n","    y_test = Y[train_size+val_size:].reshape(-1,timestep, output_size)\n","    z_test = Z[train_size+val_size:].reshape(-1,timestep, 1)\n","\n","    return [x_train, y_train, z_train, x_val, y_val, z_val, x_test, y_test, z_test]\n","\n","# State, Action, Reward(is_click)\n","X1,Y1,Z1,X2,Y2,Z2,X3,Y3,Z3 = split_data(X,Y,Z,length,state_feature,action_feature)\n","# print(X1.shape)\n","# print(Y1.shape)\n","# print(Z1.shape)\n","# X1,Y1=torch.from_numpy(X1).to(torch.float32).to(device),torch.from_numpy(Y1).to(torch.float32).to(device)\n","# X2,Y2=torch.from_numpy(X2).to(torch.float32).to(device),torch.from_numpy(Y2).to(torch.float32).to(device)\n","# Z1,Z2=torch.from_numpy(Z1).to(torch.float32).to(device),torch.from_numpy(Z2).to(torch.float32).to(device)\n","train_ids = TensorDataset(X1,Y1,Z1)\n","val_ids = TensorDataset(X2,Y2,Z2)\n","test_ids = TensorDataset(X3,Y3,Z3)\n","# print(test_ids[0])"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690976536379,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"gxuxFJ5sOBS6"},"outputs":[],"source":["class GRU(nn.Module):\n","    def __init__(self, feature_size, hidden_size, num_layers, output_size):\n","        super(GRU, self).__init__()\n","        self.hidden_size = hidden_size  # hiddensize\n","        self.num_layers = num_layers  # gru layer\n","        # self.embedding = nn.Linear(feature_size,hidden_size)\n","        # self.gru = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)\n","        self.gru = nn.GRU(feature_size, hidden_size, num_layers, batch_first=True)\n","        # self.lstm = nn.LSTM(feature_size, hidden_size, num_layers, batch_first=True)\n","        self.hidden = nn.Linear(hidden_size, hidden_size)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","        self.fc_click = nn.Linear(hidden_size, 1)\n","        # self.fc1 = nn.Linear(hidden_size, fc_hidden_size)\n","        # self.fc2 = nn.Linear(fc_hidden_size, output_size)\n","        self.sigmoid = nn.Sigmoid()\n","        self.ReLU = nn.ReLU()\n","        self.LeakyReLU = nn.LeakyReLU()\n","        self.BN = nn.BatchNorm1d(8)\n","        self.LayerNorm = nn.LayerNorm(hidden_size)\n","        self.query = nn.Linear(hidden_size, hidden_size)\n","        self.key = nn.Linear(hidden_size, hidden_size)\n","        self.value = nn.Linear(hidden_size, hidden_size)\n","        self.sqrt_size = np.sqrt(hidden_size)\n","        self.softmax = nn.Softmax(dim=-1)\n","        # self.multihead = nn.MultiheadAttention(hidden_size,num_heads,dropout=0.1)\n","    def forward(self, x, hidden=None):\n","        batch_size = x.shape[0] #  batchsize\n","        # initial hidden state\n","        if hidden is None:\n","            h_0 = x.data.new(self.num_layers, batch_size, self.hidden_size).fill_(0).float()\n","        else:\n","            h_0 = hidden\n","\n","        # GRU operation\n","        GRU_output, h_0 = self.gru(x, h_0)\n","\n","        # # Batch Normalization\n","        # output = self.BN(output)\n","\n","        # click_prob = self.LeakyReLU(GRU_output)\n","        # click_prob = click_prob.transpose(0,1)\n","        click_prob = GRU_output.transpose(0,1)\n","\n","        click_prob = self.hidden(click_prob)\n","        click_prob = self.LeakyReLU(click_prob)\n","\n","        click_prob = self.fc_click(click_prob)\n","        click_prob = self.sigmoid(click_prob)\n","\n","        # Attention Layer\n","        query_layer = self.query(GRU_output)\n","        key_layer = self.key(GRU_output).permute(0, 2, 1)\n","        value_layer = self.value(GRU_output)\n","        attention_scores = torch.matmul(query_layer, key_layer)\n","        attention_scores = attention_scores / self.sqrt_size\n","\n","        # Mask\n","        mask = (torch.triu(torch.ones(length, length)) == 0).transpose(0,1).to(device)\n","        attention_scores = attention_scores.masked_fill(mask, value=torch.tensor(-1e9))\n","        # print(attention_scores)\n","\n","        attention_scores = F.dropout(attention_scores, p=0.2)\n","        attention_probs = self.softmax(attention_scores)\n","        output = torch.matmul(attention_probs, value_layer)\n","\n","        # ReLU/LeakyReLU activation\n","        # output = self.ReLU(output)\n","        output = self.LeakyReLU(output)\n","        # output = self.LeakyReLU(GRU_output)\n","\n","\n","        # # Multihead attention\n","        # query_layer = self.query(GRU_output)\n","        # key_layer = self.key(GRU_output)\n","        # value_layer = self.value(GRU_output)\n","        # # print(query_layer.shape)\n","        # output, _ = self.multihead(query_layer,key_layer,value_layer)\n","\n","        # transpose dim 1,2 to get shape (timestep, batch_size, hidden_dim)\n","        output = output.transpose(0,1)\n","\n","        # # Add&Norm (1)\n","        output = F.dropout(self.LeakyReLU(self.hidden(output)), p=0.2) + output\n","        output = self.LayerNorm(output)\n","        output = F.dropout(self.LeakyReLU(self.hidden(output)), p=0.2) + output\n","        output = self.LayerNorm(output)\n","\n","        # # Add&Norm (2)\n","        # output = self.LeakyReLU(self.hidden(output)) + output\n","        # output = self.LayerNorm(output)\n","        # output = F.dropout(output, p=0.2)\n","        # output = self.LeakyReLU(self.hidden(output)) + output\n","        # output = self.LayerNorm(output)\n","    \n","        # # Add&Norm (3)\n","        # output = F.dropout(output, p=0.2) + GRU_output.transpose(0,1)\n","        # output = self.LayerNorm(output)\n","        # output = F.dropout(self.hidden(self.LeakyReLU(self.hidden(output))), p=0.2) + output\n","        # output = self.LayerNorm(output)\n","\n","        output = self.fc(output)  # (batch_size, timestep, output_size)\n","\n","        # Softmax convert to [0,1]\n","        action_prob = F.softmax(output,dim=2)\n","\n","        # all timestep output\n","        return action_prob, click_prob\n"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690976536379,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"PwFFlRoiOBS6"},"outputs":[],"source":["Batchsize = 512\n","# data_train_loader = DataLoader(dataset=train_ids, batch_size=Batchsize, shuffle=True)\n","data_train_loader = DataLoader(dataset=train_ids, batch_size=Batchsize, shuffle=False)\n","data_val_loader = DataLoader(dataset=val_ids, batch_size=1, shuffle=False,drop_last=False)\n","data_test_loader = DataLoader(dataset=test_ids, batch_size=1, shuffle=False,drop_last=False)\n","# data_full_loader = DataLoader(dataset=data_ids, batch_size=1, shuffle=True)\n"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690976536379,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"5ulgrq9e6DgS"},"outputs":[],"source":["def shannon_entropy(x):\n","  p = x\n","  logp = torch.log2(p)\n","  # print(p)\n","  # print(logp)\n","  entropy = - torch.sum(p*logp,dim=-1)\n","  return entropy"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":122170,"status":"ok","timestamp":1690978204312,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"3qztBjLjOBS6","outputId":"f7e713e5-0d66-4c29-a488-d4a2910d4512"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss of episode 0 = 23.127472\n","Loss of episode 1 = 22.558958\n","Loss of episode 2 = 22.361013\n","Loss of episode 3 = 21.630924\n","Loss of episode 4 = 21.546825\n","Loss of episode 5 = 21.522354\n","Loss of episode 6 = 21.512093\n","Loss of episode 7 = 21.506374\n","Loss of episode 8 = 21.502148\n","Loss of episode 9 = 21.501999\n","Loss of episode 10 = 21.499336\n","Loss of episode 11 = 21.497551\n","Loss of episode 12 = 21.496777\n","Loss of episode 13 = 21.494596\n","Loss of episode 14 = 21.493835\n","Loss of episode 15 = 21.491543\n","Loss of episode 16 = 21.488043\n","Loss of episode 17 = 21.487637\n","Loss of episode 18 = 21.484701\n","Loss of episode 19 = 21.483746\n","Loss of episode 20 = 21.483109\n","Loss of episode 21 = 21.481579\n","Loss of episode 22 = 21.483044\n","Loss of episode 23 = 21.481949\n","Loss of episode 24 = 21.48072\n","Loss of episode 25 = 21.479116\n","Loss of episode 26 = 21.479599\n","Loss of episode 27 = 21.480354\n","Loss of episode 28 = 21.478628\n","Loss of episode 29 = 21.477554\n","Loss of episode 30 = 21.47544\n","Loss of episode 31 = 21.476147\n","Loss of episode 32 = 21.475119\n","Loss of episode 33 = 21.474026\n","Loss of episode 34 = 21.470898\n","Loss of episode 35 = 21.463722\n","Loss of episode 36 = 21.454557\n","Loss of episode 37 = 21.444118\n","Loss of episode 38 = 21.437002\n","Loss of episode 39 = 21.434149\n","Loss of episode 40 = 21.433126\n","Loss of episode 41 = 21.431374\n","Loss of episode 42 = 21.427982\n","Loss of episode 43 = 21.428555\n","Loss of episode 44 = 21.427383\n","Loss of episode 45 = 21.425377\n","Loss of episode 46 = 21.424017\n","Loss of episode 47 = 21.423147\n","Loss of episode 48 = 21.422668\n","Loss of episode 49 = 21.422092\n","Loss of episode 50 = 21.421562\n","Loss of episode 51 = 21.419754\n","Loss of episode 52 = 21.418066\n","Loss of episode 53 = 21.417496\n","Loss of episode 54 = 21.41832\n","Loss of episode 55 = 21.419899\n","Loss of episode 56 = 21.421446\n","Loss of episode 57 = 21.416374\n","Loss of episode 58 = 21.416687\n","Loss of episode 59 = 21.419071\n","Loss of episode 60 = 21.41638\n","Loss of episode 61 = 21.415272\n","Loss of episode 62 = 21.412739\n","Loss of episode 63 = 21.4123\n","Loss of episode 64 = 21.41198\n","Loss of episode 65 = 21.412758\n","Loss of episode 66 = 21.412367\n","Loss of episode 67 = 21.408773\n","Loss of episode 68 = 21.409424\n","Loss of episode 69 = 21.407484\n","Loss of episode 70 = 21.407293\n","Loss of episode 71 = 21.40515\n","Loss of episode 72 = 21.40556\n","Loss of episode 73 = 21.406044\n","Loss of episode 74 = 21.405224\n","Loss of episode 75 = 21.404034\n","Loss of episode 76 = 21.398794\n","Loss of episode 77 = 21.40109\n","Loss of episode 78 = 21.400778\n","Loss of episode 79 = 21.399078\n","Loss of episode 80 = 21.398006\n","Loss of episode 81 = 21.39685\n","Loss of episode 82 = 21.392406\n","Loss of episode 83 = 21.389572\n","Loss of episode 84 = 21.390423\n","Loss of episode 85 = 21.386883\n","Loss of episode 86 = 21.386292\n","Loss of episode 87 = 21.388672\n","Loss of episode 88 = 21.385387\n","Loss of episode 89 = 21.381592\n","Loss of episode 90 = 21.377705\n","Loss of episode 91 = 21.373886\n","Loss of episode 92 = 21.370642\n","Loss of episode 93 = 21.364597\n","Loss of episode 94 = 21.361662\n","Loss of episode 95 = 21.363571\n","Loss of episode 96 = 21.365866\n","Loss of episode 97 = 21.371582\n","Loss of episode 98 = 21.358046\n","Loss of episode 99 = 21.347769\n","Loss of episode 100 = 21.335384\n","Loss of episode 101 = 21.325409\n","Loss of episode 102 = 21.325909\n","Loss of episode 103 = 21.325657\n","Loss of episode 104 = 21.32525\n","Loss of episode 105 = 21.310104\n","Loss of episode 106 = 21.310741\n","Loss of episode 107 = 21.307335\n","Loss of episode 108 = 21.294842\n","Loss of episode 109 = 21.289042\n","Loss of episode 110 = 21.286224\n","Loss of episode 111 = 21.267918\n","Loss of episode 112 = 21.264374\n","Loss of episode 113 = 21.247444\n","Loss of episode 114 = 21.237759\n","Loss of episode 115 = 21.229118\n","Loss of episode 116 = 21.249187\n","Loss of episode 117 = 21.242859\n","Loss of episode 118 = 21.224861\n","Loss of episode 119 = 21.203253\n","Loss of episode 120 = 21.20882\n","Loss of episode 121 = 21.189556\n","Loss of episode 122 = 21.169052\n","Loss of episode 123 = 21.158377\n","Loss of episode 124 = 21.139984\n","Loss of episode 125 = 21.109976\n","Loss of episode 126 = 21.089392\n","Loss of episode 127 = 21.06929\n","Loss of episode 128 = 21.057184\n","Loss of episode 129 = 21.039185\n","Loss of episode 130 = 21.041042\n","Loss of episode 131 = 21.01947\n","Loss of episode 132 = 20.993765\n","Loss of episode 133 = 20.968174\n","Loss of episode 134 = 20.956282\n","Loss of episode 135 = 20.920025\n","Loss of episode 136 = 20.906242\n","Loss of episode 137 = 20.908768\n","Loss of episode 138 = 20.892551\n","Loss of episode 139 = 20.859325\n","Loss of episode 140 = 20.837393\n","Loss of episode 141 = 20.783844\n","Loss of episode 142 = 20.755243\n","Loss of episode 143 = 20.738615\n","Loss of episode 144 = 20.701937\n","Loss of episode 145 = 20.682545\n","Loss of episode 146 = 20.668852\n","Loss of episode 147 = 20.648706\n","Loss of episode 148 = 20.65547\n","Loss of episode 149 = 20.615294\n","Loss of episode 150 = 20.592487\n","Loss of episode 151 = 20.56316\n","Loss of episode 152 = 20.55002\n","Loss of episode 153 = 20.524775\n","Loss of episode 154 = 20.509052\n","Loss of episode 155 = 20.487148\n","Loss of episode 156 = 20.495487\n","Loss of episode 157 = 20.47432\n","Loss of episode 158 = 20.467808\n","Loss of episode 159 = 20.462225\n","Loss of episode 160 = 20.466038\n","Loss of episode 161 = 20.44675\n","Loss of episode 162 = 20.43004\n","Loss of episode 163 = 20.395372\n","Loss of episode 164 = 20.350231\n","Loss of episode 165 = 20.326094\n","Loss of episode 166 = 20.301334\n","Loss of episode 167 = 20.270306\n","Loss of episode 168 = 20.250633\n","Loss of episode 169 = 20.221855\n","Loss of episode 170 = 20.207998\n","Loss of episode 171 = 20.238518\n","Loss of episode 172 = 20.239223\n","Loss of episode 173 = 20.292124\n","Loss of episode 174 = 20.31926\n","Loss of episode 175 = 20.374794\n","Loss of episode 176 = 20.372204\n","Loss of episode 177 = 20.301434\n","Loss of episode 178 = 20.227118\n","Loss of episode 179 = 20.187996\n","Loss of episode 180 = 20.153288\n","Loss of episode 181 = 20.111763\n","Loss of episode 182 = 20.096138\n","Loss of episode 183 = 20.098473\n","Loss of episode 184 = 20.07538\n","Loss of episode 185 = 20.065662\n","Loss of episode 186 = 20.062645\n","Loss of episode 187 = 20.089296\n","Loss of episode 188 = 20.136097\n","Loss of episode 189 = 20.11589\n","Loss of episode 190 = 20.109665\n","Loss of episode 191 = 20.082767\n","Loss of episode 192 = 20.027617\n","Loss of episode 193 = 19.988194\n","Loss of episode 194 = 19.963608\n","Loss of episode 195 = 19.934906\n","Loss of episode 196 = 19.9189\n","Loss of episode 197 = 19.911194\n","Loss of episode 198 = 19.893156\n","Loss of episode 199 = 19.893177\n","Loss of episode 200 = 19.880707\n","Loss of episode 201 = 19.87855\n","Loss of episode 202 = 19.86728\n","Loss of episode 203 = 19.86267\n","Loss of episode 204 = 19.862309\n","Loss of episode 205 = 19.847118\n","Loss of episode 206 = 19.840685\n","Loss of episode 207 = 19.842503\n","Loss of episode 208 = 19.842108\n","Loss of episode 209 = 19.847715\n","Loss of episode 210 = 19.839481\n","Loss of episode 211 = 19.84209\n","Loss of episode 212 = 19.849268\n","Loss of episode 213 = 19.83147\n","Loss of episode 214 = 19.8366\n","Loss of episode 215 = 19.834549\n","Loss of episode 216 = 19.828634\n","Loss of episode 217 = 19.828716\n","Loss of episode 218 = 19.832493\n","Loss of episode 219 = 19.824654\n","Loss of episode 220 = 19.806545\n","Loss of episode 221 = 19.796522\n","Loss of episode 222 = 19.79884\n","Loss of episode 223 = 19.795765\n","Loss of episode 224 = 19.791214\n","Loss of episode 225 = 19.801712\n","Loss of episode 226 = 19.78761\n","Loss of episode 227 = 19.784243\n","Loss of episode 228 = 19.778578\n","Loss of episode 229 = 19.784111\n","Loss of episode 230 = 19.766043\n","Loss of episode 231 = 19.773605\n","Loss of episode 232 = 19.766277\n","Loss of episode 233 = 19.770115\n","Loss of episode 234 = 19.769629\n","Loss of episode 235 = 19.771948\n","Loss of episode 236 = 19.773151\n","Loss of episode 237 = 19.764282\n","Loss of episode 238 = 19.762882\n","Loss of episode 239 = 19.758774\n","Loss of episode 240 = 19.758244\n","Loss of episode 241 = 19.744926\n","Loss of episode 242 = 19.751934\n","Loss of episode 243 = 19.752464\n","Loss of episode 244 = 19.742027\n","Loss of episode 245 = 19.73531\n","Loss of episode 246 = 19.723652\n","Loss of episode 247 = 19.711952\n","Loss of episode 248 = 19.710762\n","Loss of episode 249 = 19.711147\n","Loss of episode 250 = 19.711554\n","Loss of episode 251 = 19.704025\n","Loss of episode 252 = 19.696619\n","Loss of episode 253 = 19.699867\n","Loss of episode 254 = 19.694721\n","Loss of episode 255 = 19.699627\n","Loss of episode 256 = 19.6995\n","Loss of episode 257 = 19.700768\n","Loss of episode 258 = 19.705105\n","Loss of episode 259 = 19.697338\n","Loss of episode 260 = 19.6892\n","Loss of episode 261 = 19.690947\n","Loss of episode 262 = 19.686377\n","Loss of episode 263 = 19.681587\n","Loss of episode 264 = 19.681461\n","Loss of episode 265 = 19.677303\n","Loss of episode 266 = 19.681799\n","Loss of episode 267 = 19.681158\n","Loss of episode 268 = 19.684668\n","Loss of episode 269 = 19.689325\n","Loss of episode 270 = 19.698748\n","Loss of episode 271 = 19.690525\n","Loss of episode 272 = 19.683048\n","Loss of episode 273 = 19.691223\n","Loss of episode 274 = 19.677547\n","Loss of episode 275 = 19.677528\n","Loss of episode 276 = 19.676235\n","Loss of episode 277 = 19.67326\n","Loss of episode 278 = 19.672834\n","Loss of episode 279 = 19.666351\n","Loss of episode 280 = 19.659546\n","Loss of episode 281 = 19.656189\n","Loss of episode 282 = 19.658295\n","Loss of episode 283 = 19.6554\n","Loss of episode 284 = 19.659653\n","Loss of episode 285 = 19.656006\n","Loss of episode 286 = 19.661526\n","Loss of episode 287 = 19.66124\n","Loss of episode 288 = 19.663557\n","Loss of episode 289 = 19.662281\n","Loss of episode 290 = 19.665874\n","Loss of episode 291 = 19.665722\n","Loss of episode 292 = 19.669127\n","Loss of episode 293 = 19.66244\n","Loss of episode 294 = 19.654514\n","Loss of episode 295 = 19.659273\n","Loss of episode 296 = 19.650932\n","Loss of episode 297 = 19.644596\n","Loss of episode 298 = 19.65331\n","Loss of episode 299 = 19.64671\n","Loss of episode 300 = 19.649193\n","Loss of episode 301 = 19.649326\n","Loss of episode 302 = 19.646423\n","Loss of episode 303 = 19.644316\n","Loss of episode 304 = 19.637331\n","Loss of episode 305 = 19.639362\n","Loss of episode 306 = 19.64036\n","Loss of episode 307 = 19.631287\n","Loss of episode 308 = 19.631567\n","Loss of episode 309 = 19.631983\n","Loss of episode 310 = 19.623884\n","Loss of episode 311 = 19.626856\n","Loss of episode 312 = 19.625862\n","Loss of episode 313 = 19.623886\n","Loss of episode 314 = 19.622023\n","Loss of episode 315 = 19.619595\n","Loss of episode 316 = 19.630453\n","Loss of episode 317 = 19.62065\n","Loss of episode 318 = 19.62814\n","Loss of episode 319 = 19.622074\n","Loss of episode 320 = 19.623991\n","Loss of episode 321 = 19.618595\n","Loss of episode 322 = 19.620121\n","Loss of episode 323 = 19.622618\n","Loss of episode 324 = 19.616545\n","Loss of episode 325 = 19.614456\n","Loss of episode 326 = 19.624668\n","Loss of episode 327 = 19.626965\n","Loss of episode 328 = 19.61876\n","Loss of episode 329 = 19.629042\n","Loss of episode 330 = 19.61969\n","Loss of episode 331 = 19.616617\n","Loss of episode 332 = 19.617949\n","Loss of episode 333 = 19.61685\n","Loss of episode 334 = 19.621733\n","Loss of episode 335 = 19.61967\n","Loss of episode 336 = 19.614946\n","Loss of episode 337 = 19.619556\n","Loss of episode 338 = 19.626205\n","Loss of episode 339 = 19.621628\n","Loss of episode 340 = 19.612492\n","Loss of episode 341 = 19.613083\n","Loss of episode 342 = 19.619139\n","Loss of episode 343 = 19.618622\n","Loss of episode 344 = 19.61753\n","Loss of episode 345 = 19.616802\n","Loss of episode 346 = 19.616302\n","Loss of episode 347 = 19.625723\n","Loss of episode 348 = 19.628134\n","Loss of episode 349 = 19.618893\n","Loss of episode 350 = 19.621368\n","Loss of episode 351 = 19.619629\n","Loss of episode 352 = 19.615246\n","Loss of episode 353 = 19.614006\n","Loss of episode 354 = 19.616802\n","Loss of episode 355 = 19.610416\n","Loss of episode 356 = 19.618656\n","Loss of episode 357 = 19.645638\n","Loss of episode 358 = 19.637154\n","Loss of episode 359 = 19.631779\n","Loss of episode 360 = 19.624163\n","Loss of episode 361 = 19.623055\n","Loss of episode 362 = 19.618229\n","Loss of episode 363 = 19.627447\n","Loss of episode 364 = 19.624538\n","Loss of episode 365 = 19.625072\n","Loss of episode 366 = 19.616175\n","Loss of episode 367 = 19.617197\n","Loss of episode 368 = 19.624508\n","Loss of episode 369 = 19.621328\n","Loss of episode 370 = 19.612156\n","Loss of episode 371 = 19.607811\n","Loss of episode 372 = 19.608404\n","Loss of episode 373 = 19.609356\n","Loss of episode 374 = 19.618843\n","Loss of episode 375 = 19.613821\n","Loss of episode 376 = 19.60643\n","Loss of episode 377 = 19.599987\n","Loss of episode 378 = 19.59478\n","Loss of episode 379 = 19.594948\n","Loss of episode 380 = 19.601982\n","Loss of episode 381 = 19.602852\n","Loss of episode 382 = 19.600582\n","Loss of episode 383 = 19.600164\n","Loss of episode 384 = 19.600506\n","Loss of episode 385 = 19.606491\n","Loss of episode 386 = 19.603722\n","Loss of episode 387 = 19.59195\n","Loss of episode 388 = 19.59266\n","Loss of episode 389 = 19.59404\n","Loss of episode 390 = 19.593716\n","Loss of episode 391 = 19.59116\n","Loss of episode 392 = 19.594368\n","Loss of episode 393 = 19.594196\n","Loss of episode 394 = 19.591887\n","Loss of episode 395 = 19.597507\n","Loss of episode 396 = 19.591625\n","Loss of episode 397 = 19.58517\n","Loss of episode 398 = 19.591179\n","Loss of episode 399 = 19.587883\n","Loss of episode 400 = 19.583752\n","Loss of episode 401 = 19.585846\n","Loss of episode 402 = 19.586943\n","Loss of episode 403 = 19.57936\n","Loss of episode 404 = 19.586308\n","Loss of episode 405 = 19.588768\n","Loss of episode 406 = 19.588299\n","Loss of episode 407 = 19.592772\n","Loss of episode 408 = 19.596722\n","Loss of episode 409 = 19.598295\n","Loss of episode 410 = 19.59641\n","Loss of episode 411 = 19.60168\n","Loss of episode 412 = 19.595741\n","Loss of episode 413 = 19.59182\n","Loss of episode 414 = 19.597752\n","Loss of episode 415 = 19.610113\n","Loss of episode 416 = 19.621555\n","Loss of episode 417 = 19.615248\n","Loss of episode 418 = 19.605608\n","Loss of episode 419 = 19.598072\n","Loss of episode 420 = 19.59998\n","Loss of episode 421 = 19.591702\n","Loss of episode 422 = 19.60059\n","Loss of episode 423 = 19.604553\n","Loss of episode 424 = 19.606304\n","Loss of episode 425 = 19.60614\n","Loss of episode 426 = 19.602224\n","Loss of episode 427 = 19.599842\n","Loss of episode 428 = 19.591017\n","Loss of episode 429 = 19.593695\n","Loss of episode 430 = 19.591621\n","Loss of episode 431 = 19.589775\n","Loss of episode 432 = 19.59235\n","Loss of episode 433 = 19.578762\n","Loss of episode 434 = 19.585579\n","Loss of episode 435 = 19.583063\n","Loss of episode 436 = 19.582539\n","Loss of episode 437 = 19.57835\n","Loss of episode 438 = 19.582088\n","Loss of episode 439 = 19.578226\n","Loss of episode 440 = 19.58329\n","Loss of episode 441 = 19.58125\n","Loss of episode 442 = 19.583359\n","Loss of episode 443 = 19.579544\n","Loss of episode 444 = 19.582993\n","Loss of episode 445 = 19.577589\n","Loss of episode 446 = 19.583084\n","Loss of episode 447 = 19.58496\n","Loss of episode 448 = 19.573788\n","Loss of episode 449 = 19.58247\n","Loss of episode 450 = 19.575813\n","Loss of episode 451 = 19.572315\n","Loss of episode 452 = 19.573784\n","Loss of episode 453 = 19.57343\n","Loss of episode 454 = 19.57224\n","Loss of episode 455 = 19.569674\n","Loss of episode 456 = 19.574385\n","Loss of episode 457 = 19.571466\n","Loss of episode 458 = 19.571487\n","Loss of episode 459 = 19.569036\n","Loss of episode 460 = 19.56726\n","Loss of episode 461 = 19.569756\n","Loss of episode 462 = 19.564848\n","Loss of episode 463 = 19.569244\n","Loss of episode 464 = 19.570763\n","Loss of episode 465 = 19.562841\n","Loss of episode 466 = 19.56432\n","Loss of episode 467 = 19.562819\n","Loss of episode 468 = 19.567568\n","Loss of episode 469 = 19.563543\n","Loss of episode 470 = 19.567314\n","Loss of episode 471 = 19.570454\n","Loss of episode 472 = 19.567942\n","Loss of episode 473 = 19.569778\n","Loss of episode 474 = 19.56408\n","Loss of episode 475 = 19.564716\n","Loss of episode 476 = 19.56667\n","Loss of episode 477 = 19.565216\n","Loss of episode 478 = 19.57175\n","Loss of episode 479 = 19.572397\n","Loss of episode 480 = 19.567425\n","Loss of episode 481 = 19.574562\n","Loss of episode 482 = 19.571081\n","Loss of episode 483 = 19.568758\n","Loss of episode 484 = 19.567635\n","Loss of episode 485 = 19.565916\n","Loss of episode 486 = 19.557966\n","Loss of episode 487 = 19.561354\n","Loss of episode 488 = 19.557793\n","Loss of episode 489 = 19.562561\n","Loss of episode 490 = 19.565762\n","Loss of episode 491 = 19.56002\n","Loss of episode 492 = 19.561218\n","Loss of episode 493 = 19.568146\n","Loss of episode 494 = 19.569698\n","Loss of episode 495 = 19.568073\n","Loss of episode 496 = 19.571247\n","Loss of episode 497 = 19.570084\n","Loss of episode 498 = 19.57249\n","Loss of episode 499 = 19.571375\n","Loss of episode 500 = 19.568241\n","Loss of episode 501 = 19.56631\n","Loss of episode 502 = 19.56082\n","Loss of episode 503 = 19.562023\n","Loss of episode 504 = 19.564236\n","Loss of episode 505 = 19.560085\n","Loss of episode 506 = 19.558508\n","Loss of episode 507 = 19.555912\n","Loss of episode 508 = 19.557968\n","Loss of episode 509 = 19.565594\n","Loss of episode 510 = 19.567062\n","Loss of episode 511 = 19.566828\n","Loss of episode 512 = 19.563147\n","Loss of episode 513 = 19.559418\n","Loss of episode 514 = 19.565428\n","Loss of episode 515 = 19.562952\n","Loss of episode 516 = 19.5662\n","Loss of episode 517 = 19.563778\n","Loss of episode 518 = 19.558052\n","Loss of episode 519 = 19.561544\n","Loss of episode 520 = 19.558332\n","Loss of episode 521 = 19.557377\n","Loss of episode 522 = 19.554146\n","Loss of episode 523 = 19.553467\n","Loss of episode 524 = 19.555584\n","Loss of episode 525 = 19.556639\n","Loss of episode 526 = 19.55878\n","Loss of episode 527 = 19.553684\n","Loss of episode 528 = 19.553993\n","Loss of episode 529 = 19.553555\n","Loss of episode 530 = 19.556025\n","Loss of episode 531 = 19.559278\n","Loss of episode 532 = 19.555244\n","Loss of episode 533 = 19.563625\n","Loss of episode 534 = 19.557076\n","Loss of episode 535 = 19.557312\n","Loss of episode 536 = 19.561771\n","Loss of episode 537 = 19.56236\n","Loss of episode 538 = 19.56192\n","Loss of episode 539 = 19.563438\n","Loss of episode 540 = 19.566679\n","Loss of episode 541 = 19.554993\n","Loss of episode 542 = 19.556187\n","Loss of episode 543 = 19.555723\n","Loss of episode 544 = 19.547855\n","Loss of episode 545 = 19.55436\n","Loss of episode 546 = 19.551712\n","Loss of episode 547 = 19.554237\n","Loss of episode 548 = 19.5558\n","Loss of episode 549 = 19.557953\n","Loss of episode 550 = 19.55716\n","Loss of episode 551 = 19.555475\n","Loss of episode 552 = 19.553558\n","Loss of episode 553 = 19.553623\n","Loss of episode 554 = 19.551632\n","Loss of episode 555 = 19.55549\n","Loss of episode 556 = 19.548046\n","Loss of episode 557 = 19.554226\n","Loss of episode 558 = 19.547863\n","Loss of episode 559 = 19.552544\n","Loss of episode 560 = 19.5528\n","Loss of episode 561 = 19.56542\n","Loss of episode 562 = 19.574286\n","Loss of episode 563 = 19.568846\n","Loss of episode 564 = 19.570473\n","Loss of episode 565 = 19.571545\n","Loss of episode 566 = 19.566484\n","Loss of episode 567 = 19.565763\n","Loss of episode 568 = 19.559917\n","Loss of episode 569 = 19.56117\n","Loss of episode 570 = 19.558334\n","Loss of episode 571 = 19.563921\n","Loss of episode 572 = 19.560799\n","Loss of episode 573 = 19.55494\n","Loss of episode 574 = 19.556595\n","Loss of episode 575 = 19.557259\n","Loss of episode 576 = 19.55366\n","Loss of episode 577 = 19.551567\n","Loss of episode 578 = 19.547653\n","Loss of episode 579 = 19.552753\n","Loss of episode 580 = 19.552044\n","Loss of episode 581 = 19.548248\n","Loss of episode 582 = 19.5499\n","Loss of episode 583 = 19.547123\n","Loss of episode 584 = 19.547699\n","Loss of episode 585 = 19.568707\n","Loss of episode 586 = 19.572514\n","Loss of episode 587 = 19.565475\n","Loss of episode 588 = 19.558735\n","Loss of episode 589 = 19.55983\n","Loss of episode 590 = 19.559158\n","Loss of episode 591 = 19.561913\n","Loss of episode 592 = 19.560236\n","Loss of episode 593 = 19.563549\n","Loss of episode 594 = 19.571728\n","Loss of episode 595 = 19.571867\n","Loss of episode 596 = 19.580742\n","Loss of episode 597 = 19.570726\n","Loss of episode 598 = 19.569145\n","Loss of episode 599 = 19.561325\n","Loss of episode 600 = 19.562382\n","Loss of episode 601 = 19.555075\n","Loss of episode 602 = 19.554539\n","Loss of episode 603 = 19.556267\n","Loss of episode 604 = 19.550741\n","Loss of episode 605 = 19.555845\n","Loss of episode 606 = 19.553371\n","Loss of episode 607 = 19.552542\n","Loss of episode 608 = 19.553848\n","Loss of episode 609 = 19.558207\n","Loss of episode 610 = 19.55254\n","Loss of episode 611 = 19.553852\n","Loss of episode 612 = 19.55518\n","Loss of episode 613 = 19.555756\n","Loss of episode 614 = 19.550892\n","Loss of episode 615 = 19.55321\n","Loss of episode 616 = 19.549532\n","Loss of episode 617 = 19.54834\n","Loss of episode 618 = 19.555103\n","Loss of episode 619 = 19.552156\n","Loss of episode 620 = 19.555145\n","Loss of episode 621 = 19.549595\n","Loss of episode 622 = 19.551252\n","Loss of episode 623 = 19.553408\n","Loss of episode 624 = 19.55587\n","Loss of episode 625 = 19.558424\n","Loss of episode 626 = 19.560415\n","Loss of episode 627 = 19.553429\n","Loss of episode 628 = 19.551746\n","Loss of episode 629 = 19.54959\n","Loss of episode 630 = 19.545364\n","Loss of episode 631 = 19.545403\n","Loss of episode 632 = 19.554083\n","Loss of episode 633 = 19.549582\n","Loss of episode 634 = 19.549898\n","Loss of episode 635 = 19.54647\n","Loss of episode 636 = 19.543661\n","Loss of episode 637 = 19.542557\n","Loss of episode 638 = 19.548828\n","Loss of episode 639 = 19.554543\n","Loss of episode 640 = 19.553848\n","Loss of episode 641 = 19.55094\n","Loss of episode 642 = 19.550535\n","Loss of episode 643 = 19.54309\n","Loss of episode 644 = 19.54579\n","Loss of episode 645 = 19.543715\n","Loss of episode 646 = 19.539186\n","Loss of episode 647 = 19.539604\n","Loss of episode 648 = 19.539753\n","Loss of episode 649 = 19.546806\n","Loss of episode 650 = 19.546648\n","Loss of episode 651 = 19.54461\n","Loss of episode 652 = 19.551702\n","Loss of episode 653 = 19.549816\n","Loss of episode 654 = 19.549953\n","Loss of episode 655 = 19.550411\n","Loss of episode 656 = 19.547121\n","Loss of episode 657 = 19.550322\n","Loss of episode 658 = 19.547825\n","Loss of episode 659 = 19.553265\n","Loss of episode 660 = 19.546873\n","Loss of episode 661 = 19.542\n","Loss of episode 662 = 19.543524\n","Loss of episode 663 = 19.545193\n","Loss of episode 664 = 19.544264\n","Loss of episode 665 = 19.544577\n","Loss of episode 666 = 19.546005\n","Loss of episode 667 = 19.540726\n","Loss of episode 668 = 19.54693\n","Loss of episode 669 = 19.546375\n","Loss of episode 670 = 19.540945\n","Loss of episode 671 = 19.540571\n","Loss of episode 672 = 19.544884\n","Loss of episode 673 = 19.54308\n","Loss of episode 674 = 19.545904\n","Loss of episode 675 = 19.542465\n","Loss of episode 676 = 19.540253\n","Loss of episode 677 = 19.54401\n","Loss of episode 678 = 19.546429\n","Loss of episode 679 = 19.54635\n","Loss of episode 680 = 19.54617\n","Loss of episode 681 = 19.542135\n","Loss of episode 682 = 19.54666\n","Loss of episode 683 = 19.543423\n","Loss of episode 684 = 19.55029\n","Loss of episode 685 = 19.555525\n","Loss of episode 686 = 19.551601\n","Loss of episode 687 = 19.552076\n","Loss of episode 688 = 19.550282\n","Loss of episode 689 = 19.54573\n","Loss of episode 690 = 19.538952\n","Loss of episode 691 = 19.543285\n","Loss of episode 692 = 19.543541\n","Loss of episode 693 = 19.538952\n","Loss of episode 694 = 19.536036\n","Loss of episode 695 = 19.532269\n","Loss of episode 696 = 19.530592\n","Loss of episode 697 = 19.536945\n","Loss of episode 698 = 19.545876\n","Loss of episode 699 = 19.546385\n","Loss of episode 700 = 19.536602\n","Loss of episode 701 = 19.53966\n","Loss of episode 702 = 19.540262\n","Loss of episode 703 = 19.544853\n","Loss of episode 704 = 19.538116\n","Loss of episode 705 = 19.542988\n","Loss of episode 706 = 19.539267\n","Loss of episode 707 = 19.538767\n","Loss of episode 708 = 19.53513\n","Loss of episode 709 = 19.538641\n","Loss of episode 710 = 19.54058\n","Loss of episode 711 = 19.542744\n","Loss of episode 712 = 19.53826\n","Loss of episode 713 = 19.53257\n","Loss of episode 714 = 19.53607\n","Loss of episode 715 = 19.53875\n","Loss of episode 716 = 19.536491\n","Loss of episode 717 = 19.539242\n","Loss of episode 718 = 19.53664\n","Loss of episode 719 = 19.534367\n","Loss of episode 720 = 19.530502\n","Loss of episode 721 = 19.532543\n","Loss of episode 722 = 19.549911\n","Loss of episode 723 = 19.550106\n","Loss of episode 724 = 19.550621\n","Loss of episode 725 = 19.54992\n","Loss of episode 726 = 19.544025\n","Loss of episode 727 = 19.540028\n","Loss of episode 728 = 19.54158\n","Loss of episode 729 = 19.543512\n","Loss of episode 730 = 19.539814\n","Loss of episode 731 = 19.540138\n","Loss of episode 732 = 19.54181\n","Loss of episode 733 = 19.54984\n","Loss of episode 734 = 19.557755\n","Loss of episode 735 = 19.574593\n","Loss of episode 736 = 19.565561\n","Loss of episode 737 = 19.561798\n","Loss of episode 738 = 19.551615\n","Loss of episode 739 = 19.54881\n","Loss of episode 740 = 19.54356\n","Loss of episode 741 = 19.538542\n","Loss of episode 742 = 19.538597\n","Loss of episode 743 = 19.538397\n","Loss of episode 744 = 19.540419\n","Loss of episode 745 = 19.543348\n","Loss of episode 746 = 19.541084\n","Loss of episode 747 = 19.544966\n","Loss of episode 748 = 19.541815\n","Loss of episode 749 = 19.549679\n","Loss of episode 750 = 19.54825\n","Loss of episode 751 = 19.540169\n","Loss of episode 752 = 19.54053\n","Loss of episode 753 = 19.5405\n","Loss of episode 754 = 19.541166\n","Loss of episode 755 = 19.5394\n","Loss of episode 756 = 19.539898\n","Loss of episode 757 = 19.539871\n","Loss of episode 758 = 19.541\n","Loss of episode 759 = 19.544147\n","Loss of episode 760 = 19.543167\n","Loss of episode 761 = 19.545776\n","Loss of episode 762 = 19.538864\n","Loss of episode 763 = 19.541422\n","Loss of episode 764 = 19.548405\n","Loss of episode 765 = 19.54638\n","Loss of episode 766 = 19.546347\n","Loss of episode 767 = 19.540218\n","Loss of episode 768 = 19.542929\n","Loss of episode 769 = 19.539967\n","Loss of episode 770 = 19.538145\n","Loss of episode 771 = 19.539314\n","Loss of episode 772 = 19.547434\n","Loss of episode 773 = 19.54919\n","Loss of episode 774 = 19.541859\n","Loss of episode 775 = 19.54143\n","Loss of episode 776 = 19.538017\n","Loss of episode 777 = 19.545368\n","Loss of episode 778 = 19.541542\n","Loss of episode 779 = 19.541088\n","Loss of episode 780 = 19.534273\n","Loss of episode 781 = 19.536856\n","Loss of episode 782 = 19.530418\n","Loss of episode 783 = 19.531008\n","Loss of episode 784 = 19.536535\n","Loss of episode 785 = 19.530743\n","Loss of episode 786 = 19.529877\n","Loss of episode 787 = 19.52786\n","Loss of episode 788 = 19.525055\n","Loss of episode 789 = 19.526327\n","Loss of episode 790 = 19.531416\n","Loss of episode 791 = 19.5328\n","Loss of episode 792 = 19.535053\n","Loss of episode 793 = 19.524048\n","Loss of episode 794 = 19.527725\n","Loss of episode 795 = 19.53236\n","Loss of episode 796 = 19.531647\n","Loss of episode 797 = 19.5329\n","Loss of episode 798 = 19.534355\n","Loss of episode 799 = 19.536903\n","\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# feature_size, hidden_size, num_layers, output_size, (number_heads)\n","gru = GRU(state_feature,512,2,3).to(device)\n","num_epoch = 800     # 800\n","Lambda = 1.5        # 1.5\n","Mu = 0.08           # 0.08\n","\n","# optimizer = torch.optim.SGD(net.parameters(),lr=0.5)\n","# optimizer = torch.optim.Adam(gru.parameters(),lr=0.001)\n","optimizer = torch.optim.AdamW(gru.parameters(),lr=0.001)\n","\n","# loss_func = torch.nn.MSELoss()            #MSE for regression\n","loss_func = torch.nn.CrossEntropyLoss()     #CE for classification\n","Loss=[]\n","\n","for t in range(num_epoch):\n","    aver_loss = 0\n","    gru.train()\n","    for batch, data in enumerate(data_train_loader):\n","        x, y ,z = data\n","        action_prediction, click_prediction = gru(x)\n","        # print(action_prediction.shape)\n","        action_prediction = torch.transpose(action_prediction, dim0=0, dim1=1)\n","        click_prediction = torch.transpose(click_prediction, dim0=0, dim1=1)\n","        Entropy = torch.mean(shannon_entropy(action_prediction))\n","        # loss = loss_func(action_prediction,y) - Lambda * Entropy #//2-loss\n","        loss = loss_func(action_prediction,y) + Mu * loss_func(click_prediction,z) \\\n","        - Lambda * Entropy \n","        optimizer.zero_grad() \n","        loss.backward()    \n","        optimizer.step() \n","        aver_loss += loss\n","    aver_loss /= batch\n","    aver_loss=aver_loss.cpu().detach().numpy()\n","    print('Loss of episode %s ='%t,aver_loss)\n","    Loss.append(aver_loss)\n","plt.plot(Loss,color='r')\n","print()"]},{"cell_type":"code","execution_count":103,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690978204313,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"lxxPbtYyOBS6","outputId":"d2b4bdae-3248-45d5-9a2c-25bb1d8fc299"},"outputs":[{"name":"stdout","output_type":"stream","text":["3460628\n"]}],"source":["device = torch.device(\"cuda\")\n","gru_test = GRU(state_feature,512,2,3).to(device)\n","total = sum([param.nelement() for param in gru_test.parameters()])\n","print(total)\n","# gru_test"]},{"cell_type":"markdown","metadata":{"id":"Qiqn4DAJOBS7"},"source":["## Training accuracy"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690978204313,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"BUoPP1KjOBS7","outputId":"4ec4f842-dd61-42d5-e7f4-01bea8ab6944"},"outputs":[{"name":"stdout","output_type":"stream","text":["acc = 0.9851406250000001\n"]}],"source":["Truth = []\n","Pred = []\n","aver_loss = 0\n","total_acc = 0\n","Entropy = 0\n","for batch, data in enumerate(data_train_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    x, y, z = data\n","    # print(x)\n","    # prediction =net(x)\n","    test, _ = gru(x)\n","    test = torch.transpose(test, dim0=0, dim1=1)\n","    # print(test)\n","    # print(y)\n","    # print(shannon_entropy(test).shape)\n","    aver_entropy = sum(torch.mean(shannon_entropy(test),dim=1))\n","    # print(sum(aver_entropy))\n","    Entropy += aver_entropy\n","\n","    a = torch.argmax(test,dim = 2).cpu().data.numpy()\n","    b = torch.argmax(y,dim = 2).cpu().data.numpy()\n","    # print(a)\n","    # print(b)\n","    # print(sum(a==b))\n","    aver_acc = sum(sum(a==b))/length\n","    # print(aver_acc)\n","    total_acc += aver_acc\n","    # print(total_acc)\n","# print(batch)\n","print('acc =', total_acc/(0.8*max_number_traj))\n","# print('Entropy = ', Entropy/(0.8*max_number_traj))"]},{"cell_type":"markdown","metadata":{},"source":["## Validation accuracy (Hyperparameter tuning)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Action acc = 0.6974999999999996\n","CTR acc = 0.796875\n","Entropy =  tensor(0.9753, grad_fn=<DivBackward0>)\n"]}],"source":["from sklearn.metrics import roc_curve, auc\n","\n","\n","Loss=[]\n","Truth = []\n","Pred = []\n","aver_loss = 0\n","total_acc = 0\n","total_ctr = 0\n","total_auc = 0\n","Entropy = 0\n","num = 0\n","\n","# Click_pred = np.zeros((int(0.2*max_number_traj),length))\n","# Click_true = np.zeros((int(0.2*max_number_traj),length))\n","\n","for batch, data in enumerate(data_val_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    gru.eval()\n","    x, y, z = data\n","    test, click = gru(x)\n","    click_pred = torch.squeeze(click).cpu().data.numpy()\n","    click_true = torch.squeeze(z).cpu().data.numpy()\n","    click_pred[click_pred<1/2] = 0\n","    click_pred[click_pred>=1/2] = 1\n","    # Click_pred[batch] = click_pred\n","    # Click_true[batch] = click_true\n","\n","    p = test[:,0].cpu()\n","    aver_entropy = torch.mean(shannon_entropy(p))\n","    # print('aver_entropy =', aver_entropy)\n","    Entropy += aver_entropy\n","    # print(test)\n","    a = torch.argmax(test,dim = 2).cpu().data.numpy().T[0]\n","    b = torch.argmax(y,dim = 2).cpu().data.numpy()[0]\n","    # print('-----------')\n","    # print('pred =',a.T[0])\n","    # print('true =',b[0])\n","    aver_acc = sum(a==b)/length\n","    total_acc += aver_acc\n","\n","    # print('-----------')\n","    # print('pred =', click_pred)\n","    # print('true =', click_true) \n","    aver_ctr = sum(click_pred==click_true)/length\n","    total_ctr += aver_ctr\n","\n","\n","# fpr, tpr, thresholds = roc_curve(Click_true.reshape(-1), Click_pred.reshape(-1),pos_label=1)\n","# total_auc = auc(fpr, tpr)\n","# print('CTR AUC =', total_auc)\n","print('Action acc =', total_acc/(0.1*max_number_traj))\n","print('CTR acc =', total_ctr/(0.1*max_number_traj))\n","print('Entropy = ', Entropy/(0.1*max_number_traj))\n"]},{"cell_type":"markdown","metadata":{"id":"6Hi558JBOBS8"},"source":["## Inference accuracy"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1541,"status":"ok","timestamp":1690978205851,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"dH6BC5O2OBS8","outputId":"f0746245-4215-490b-c3b9-ee67948cc407"},"outputs":[{"name":"stdout","output_type":"stream","text":["Action acc = 0.6532499999999996\n","CTR acc = 0.7693749999999988\n","Entropy =  tensor(0.9812, grad_fn=<DivBackward0>)\n"]}],"source":["from sklearn.metrics import roc_curve, auc\n","\n","\n","Loss=[]\n","Truth = []\n","Pred = []\n","aver_loss = 0\n","total_acc = 0\n","total_ctr = 0\n","total_auc = 0\n","Entropy = 0\n","num = 0\n","\n","# Click_pred = np.zeros((int(0.2*max_number_traj),length))\n","# Click_true = np.zeros((int(0.2*max_number_traj),length))\n","\n","for batch, data in enumerate(data_test_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    gru.eval()\n","    x, y, z = data\n","    test, click = gru(x)\n","    click_pred = torch.squeeze(click).cpu().data.numpy()\n","    click_true = torch.squeeze(z).cpu().data.numpy()\n","    click_pred[click_pred<1/2] = 0\n","    click_pred[click_pred>=1/2] = 1\n","    # Click_pred[batch] = click_pred\n","    # Click_true[batch] = click_true\n","\n","    p = test[:,0].cpu()\n","    aver_entropy = torch.mean(shannon_entropy(p))\n","    # print('aver_entropy =', aver_entropy)\n","    Entropy += aver_entropy\n","    # print(test)\n","    a = torch.argmax(test,dim = 2).cpu().data.numpy().T[0]\n","    b = torch.argmax(y,dim = 2).cpu().data.numpy()[0]\n","    # print('-----------')\n","    # print('pred =',a.T[0])\n","    # print('true =',b[0])\n","    aver_acc = sum(a==b)/length\n","    total_acc += aver_acc\n","\n","    # print('-----------')\n","    # print('pred =', click_pred)\n","    # print('true =', click_true) \n","    aver_ctr = sum(click_pred==click_true)/length\n","    total_ctr += aver_ctr\n","\n","\n","# fpr, tpr, thresholds = roc_curve(Click_true.reshape(-1), Click_pred.reshape(-1),pos_label=1)\n","# total_auc = auc(fpr, tpr)\n","# print('CTR AUC =', total_auc)\n","print('Action acc =', total_acc/(0.1*max_number_traj))\n","print('CTR acc =', total_ctr/(0.1*max_number_traj))\n","print('Entropy = ', Entropy/(0.1*max_number_traj))\n"]},{"cell_type":"markdown","metadata":{"id":"n6ecXL0POBS8"},"source":["## AUC"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":736,"status":"ok","timestamp":1690978206585,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"45g3DhluOBS8","outputId":"9e567364-e555-4ef1-be01-fdcdc6de971b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Average AUC = [0.76362596 0.77135919 0.76290932]\n"]}],"source":["from sklearn.metrics import precision_score, roc_curve, auc\n","n_classes = 3\n","# length = 8\n","aver_auc = np.zeros(n_classes)\n","total_precision = 0\n","Y_score = np.zeros(((length, int(0.1*max_number_traj), n_classes)))\n","# print(Y_score.shape)\n","Y_label = np.zeros(((length, int(0.1*max_number_traj), n_classes)))\n","for batch, data in enumerate(data_test_loader):\n","    gru.eval()\n","    x, y, z = data\n","    # print(x)\n","    test, _ = gru(x)\n","    y_score = test[:,0,:].cpu().data.numpy()\n","    y_label = y[0].cpu().data.numpy()\n","    # print(batch)\n","    for i in range(length):\n","        Y_score[i][batch] = y_score[i]\n","        Y_label[i][batch] = y_label[i]\n","        # Y_score[i][batch] = [1/3,1/3,1/3]\n","        # Y_label[i][batch] = y_label[i]\n","        # print(y_score[i])\n","        # print(y_label[i])\n","fpr = dict()\n","tpr = dict()\n","roc_auc = dict()\n","aver_auc = np.zeros(n_classes)\n","for t in range(length):\n","    for i in range(n_classes):\n","        fpr[i], tpr[i], _ = roc_curve(Y_label[t][:, i], Y_score[t][:, i])\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","        # aver_auc[i] += auc(fpr[i], tpr[i])\n","        aver_auc[i] += roc_auc[i]/length\n","    # print('step %s AUC ='%(t+1),roc_auc)\n","print('Average AUC =', aver_auc)"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1690978206585,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"Q-5eCoFCOBS8","outputId":"1fe76151-c1a2-4ced-8acf-4a37d2df9e6e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Aver_F1 = 0.656 Aver_Precision = 0.6579 Aver_Recall = 0.6574\n"]}],"source":["from sklearn.metrics import f1_score, precision_score, recall_score\n","\n","aver_f1 = 0\n","aver_p = 0\n","aver_r = 0\n","\n","# print(y_pred)\n","# print(y_true)\n","for i in range(length):\n","  y_true = np.argmax(Y_label[i],axis = 1)\n","  y_pred = np.argmax(Y_score[i],axis = 1)\n","  f1 = round(f1_score(y_true, y_pred, average='macro' ),4)\n","  p = round(precision_score(y_true, y_pred, average='macro'),4)\n","  r = round(recall_score(y_true, y_pred, average='macro'),4)\n","  aver_f1 += f1/length\n","  aver_p += p/length\n","  aver_r += r/length\n","print('Aver_F1 =', round(aver_f1,4), 'Aver_Precision =', round(aver_p,4), 'Aver_Recall =', round(aver_r,4))"]},{"cell_type":"markdown","metadata":{},"source":["## Budget Allocation"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["test_number = int(0.1*max_number_traj)\n","True_Click_matrix = np.zeros((test_number,length))\n","True_Action_matrix = np.zeros((test_number,length))\n","Click_pred_matrix = np.zeros((test_number,length))\n","Action_pred_matrix = np.zeros((test_number,length))\n","\n","for batch, data in enumerate(data_test_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    gru.eval()\n","    x, y, z = data\n","    action_true = torch.squeeze(torch.argmax(y,dim = 2)).cpu().data.numpy()\n","    click_true = torch.squeeze(z).cpu().data.numpy()\n","    True_Click_matrix[batch] = click_true\n","    # True_Action_count += np.sum(action_true, axis = 0)\n","    \n","    action, click = gru(x)\n","    action_pred = torch.squeeze(torch.argmax(action,dim = 2)).cpu().data.numpy()\n","    # print(action_true == action_pred)\n","    Action_pred_matrix[batch] = (action_true == action_pred)\n","    click_pred = torch.squeeze(click).cpu().data.numpy()\n","    Click_pred_matrix[batch] = click_pred\n","\n","# print(True_Click_matrix)"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True_cost =  8000.0\n","True click number =  1481.0\n","Ture_CPC = 5.401755570560432\n","Policy_cost =  2000.0\n","Policy click number =  552.0\n","Policy_CPC = 3.6231884057971016\n"]}],"source":["choose_number = 100     # 50\n","True_cost = 0.1 * max_number_traj * length\n","True_Click_number = sum(sum(True_Click_matrix))\n","User_choose = np.zeros((test_number,length))\n","\n","print('True_cost = ', True_cost)\n","print('True click number = ', True_Click_number)\n","True_CPC = (0.1 * max_number_traj) * length / True_Click_number\n","print('Ture_CPC =',True_CPC)\n","for i in range(length):\n","    tmp = Click_pred_matrix[:,i]\n","    idx = np.argsort(tmp)\n","    # print(idx[400:])\n","    # print(tmp[idx[400:]])\n","    User_choose[:,i][idx[test_number-choose_number:]] = 1\n","\n","Cost = sum(sum(User_choose))\n","Click = sum(sum(User_choose*Action_pred_matrix*True_Click_matrix))\n","cpc = Cost/Click\n","print('Policy_cost = ', Cost)\n","print('Policy click number = ', Click) \n","print('Policy_CPC =', cpc)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
