{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1481,"status":"ok","timestamp":1690970114664,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"SHePlV70OBS2"},"outputs":[],"source":["# if torch.cuda.is_available():\n","#     print('CUDNN VERSION:',torch.backends.cudnn.version())\n","#     print('Number CUDA Devices:',torch.cuda.device_count())\n","#     print('CUDA Device Name:',torch.cuda.get_device_name(0))\n","#     print('CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6052,"status":"ok","timestamp":1690970120708,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"kUqW9r2qOBS2","outputId":"606cfa3e-2965-43bb-9f91-818746accbc6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on the GPU\n"]}],"source":["import torch\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")  # you can continue going on here, like cuda:1 cuda:2....etc.\n","    print(\"Running on the GPU\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"Running on the CPU\")\n","# torch.cuda.device_count()"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1690973614130,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"iZBy7-2rOBS3"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","Data = pd.read_csv('kuairand_sequence.csv',index_col=0)\n","# Data = pd.read_csv('kuairand_sequence_user=8000.csv',index_col=0)\n","# data['user_id'].value_counts()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1690973619591,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"3HzbfAYVOBS3","outputId":"2e5a6756-eb74-4e81-f782-0caddeae1081"},"outputs":[],"source":["import torch\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import random\n","from torch.utils.data import Dataset, DataLoader,TensorDataset\n","\n","max_length = 100\n","num_user = len(Data['user_id'].unique())\n","state_feature = len(Data.columns) - 1\n","action_feature = 3\n","\n","uid = Data['user_id'].unique()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2991,"status":"ok","timestamp":1690973742240,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"HdF3rl_LOBS4","outputId":"9a25cd7c-7681-4521-a5f4-db7dc0f9d462"},"outputs":[],"source":["State = np.zeros(((num_user,max_length,state_feature)))\n","Action = np.zeros((num_user,max_length,action_feature))\n","Click = np.zeros((num_user,max_length))\n","\n","j = 0\n","for i in uid:\n","    state_sequence = np.array(Data[Data['user_id']==i])[:-1]\n","    # state_sequence = np.array(data[data['user_id']==i].drop(['short','mid','long'],axis=1))\n","    action_sequence = np.array(Data[Data['user_id']==i][['short','mid','long']])[1:]\n","    click_sequence = np.array(Data[Data['user_id']==i]['is_click'])[1:]\n","    # action_sequence = np.array(data[data['user_id']==i][['short','mid','long']])\n","    # print(action_sequence)\n","    state = np.pad(state_sequence,((0,max_length-len(state_sequence)),(0,0)))[:,1:]\n","    action = np.pad(action_sequence,((0,max_length-len(action_sequence)),(0,0)))\n","    click = np.pad(click_sequence,((0,max_length-len(click_sequence))))\n","    # print(click)\n","    State[j] = state\n","    Action[j] = action\n","    Click[j] = click\n","    j += 1\n","\n","# print(len(State))\n","# print(len(Action))\n","# print(len(Click))\n","# print(Click.shape)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":822,"status":"ok","timestamp":1690976535806,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"tPsLnRdtOBS4","outputId":"798634a6-86b6-4797-8422-298b81c046ca"},"outputs":[],"source":["length = 20\n","max_number_traj = 4000\n","\n","number_traj = 0\n","def Select_trajectory(X,Y,Z,number_traj):\n","    Select_states = []\n","    Select_actions = []\n","    Select_clicks = []\n","    # for s in range(number_traj):\n","    while True:\n","        i = random.randint(0,len(X)-1)\n","        select_state = X[i]\n","        select_action = Y[i]\n","        select_click = Z[i]\n","        # print(select_state)\n","        # print(select_action)\n","        for k in range(0,max_length):\n","            # print(select_episode[k])\n","            if select_state[k].any() == 0:\n","                max_episode_length = k\n","                break\n","        # print(k)\n","        # print(max_episode_length)\n","        if max_episode_length >= length + 1:\n","            j = random.randint(0,max_episode_length-length-1)\n","            select_state = select_state[j:j+length]\n","            select_action = select_action[j:j+length]\n","            select_click = select_click[j:j+length]\n","            Select_states.append(select_state)\n","            Select_actions.append(select_action)\n","            Select_clicks.append(select_click)\n","            number_traj += 1\n","        # print(select_trajectory)\n","        if number_traj == max_number_traj:\n","            break\n","    return np.array(Select_states),np.array(Select_actions),np.array(Select_clicks)\n","\n","X , Y , Z = Select_trajectory(State,Action,Click,number_traj)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1690976536378,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"P1xlyTewOBS5"},"outputs":[],"source":["np.random.seed(2023)\n","per = np.random.permutation(X.shape[0])\n","# print(per)\n","X = X[per]\n","Y = Y[per]\n","Z = Z[per]\n","X,Y,Z = torch.from_numpy(X).to(torch.float32).to(device),torch.from_numpy(Y).to(torch.float32).to(device), torch.from_numpy(Z).to(torch.float32).to(device)\n","# print(sum(Y))"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690976536378,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"YB3Wa9SaOBS5","outputId":"c094f597-85a0-49b2-adee-00ee3d42b5d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["3200\n"]}],"source":["def split_data(State, Action, Reward, timestep, input_size, output_size):\n","\n","    # Training set\n","    train_size = int(np.round(0.8 * X.shape[0]))\n","    val_size = int(np.round(0.1 * X.shape[0]))\n","    print(train_size)\n","\n","    # Split training and test set 8:1:1\n","    x_train = X[: train_size, :].reshape(-1, timestep,input_size)\n","    y_train = Y[: train_size].reshape(-1,timestep, output_size)\n","    z_train = Z[: train_size].reshape(-1,timestep, 1)\n","\n","    # x_test = X[train_size:, :].reshape(-1, timestep,input_size)\n","    # y_test = Y[train_size:].reshape(-1,timestep, output_size)\n","    # z_test = Z[train_size:].reshape(-1,timestep, 1)\n","\n","    x_val = X[train_size:train_size+val_size, :].reshape(-1, timestep,input_size)\n","    y_val = Y[train_size:train_size+val_size].reshape(-1,timestep, output_size)\n","    z_val = Z[train_size:train_size+val_size].reshape(-1,timestep, 1) \n","\n","    x_test = X[train_size+val_size:, :].reshape(-1, timestep,input_size)\n","    y_test = Y[train_size+val_size:].reshape(-1,timestep, output_size)\n","    z_test = Z[train_size+val_size:].reshape(-1,timestep, 1)\n","\n","    return [x_train, y_train, z_train, x_val, y_val, z_val, x_test, y_test, z_test]\n","\n","# State, Action, Reward(is_click)\n","X1,Y1,Z1,X2,Y2,Z2,X3,Y3,Z3 = split_data(X,Y,Z,length,state_feature,action_feature)\n","# print(X1.shape)\n","# print(Y1.shape)\n","# print(Z1.shape)\n","# X1,Y1=torch.from_numpy(X1).to(torch.float32).to(device),torch.from_numpy(Y1).to(torch.float32).to(device)\n","# X2,Y2=torch.from_numpy(X2).to(torch.float32).to(device),torch.from_numpy(Y2).to(torch.float32).to(device)\n","# Z1,Z2=torch.from_numpy(Z1).to(torch.float32).to(device),torch.from_numpy(Z2).to(torch.float32).to(device)\n","train_ids = TensorDataset(X1,Y1,Z1)\n","val_ids = TensorDataset(X2,Y2,Z2)\n","test_ids = TensorDataset(X3,Y3,Z3)\n","# print(test_ids[0])"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([512, 512, 20])\n"]}],"source":["\n","from pytorch_tcn.tcn import TCN\n","x = torch.ones(512,state_feature,length)\n","model = TCN(state_feature,[512,512],causal=True)\n","print(model(x).shape)\n","# model = TCN(\n","#     num_inputs: int,\n","#     num_channels: ArrayLike,\n","#     kernel_size: int = 4,\n","#     dilations: Optional[ ArrayLike ] = None,\n","#     dilaton_reset: Optional[ int ] = None,\n","#     dropout: float = 0.1,\n","#     causal: bool = True,\n","#     use_norm: str = 'weight_norm',\n","#     activation: str = 'relu',\n","#     kernel_initializer: str = 'xavier_uniform',\n","#     use_skip_connections: bool = False,\n","#     input_shape: str = 'NCL',\n","# )"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690976536379,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"gxuxFJ5sOBS6"},"outputs":[],"source":["class GRU(nn.Module):\n","    def __init__(self, feature_size, hidden_size, num_layers, output_size):\n","        super(GRU, self).__init__()\n","        self.hidden_size = hidden_size  # hiddensize\n","        self.num_layers = num_layers  # gru layer\n","        # self.embedding = nn.Linear(feature_size,hidden_size)\n","        # self.gru = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)\n","        self.gru = nn.GRU(feature_size, hidden_size, num_layers, batch_first=True)\n","        # self.lstm = nn.LSTM(feature_size, hidden_size, num_layers, batch_first=True)\n","        self.hidden = nn.Linear(hidden_size, hidden_size)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","        self.fc_click = nn.Linear(hidden_size, 1)\n","        # self.fc1 = nn.Linear(hidden_size, fc_hidden_size)\n","        # self.fc2 = nn.Linear(fc_hidden_size, output_size)\n","        self.sigmoid = nn.Sigmoid()\n","        self.ReLU = nn.ReLU()\n","        self.LeakyReLU = nn.LeakyReLU()\n","        self.BN = nn.BatchNorm1d(8)\n","        self.LayerNorm = nn.LayerNorm(hidden_size)\n","        self.query = nn.Linear(hidden_size, hidden_size)\n","        self.key = nn.Linear(hidden_size, hidden_size)\n","        self.value = nn.Linear(hidden_size, hidden_size)\n","        self.sqrt_size = np.sqrt(hidden_size)\n","        self.softmax = nn.Softmax(dim=-1)\n","        self.tcn = TCN(state_feature,[hidden_size,hidden_size])\n","        # self.multihead = nn.MultiheadAttention(hidden_size,num_heads,dropout=0.1)\n","    def forward(self, x, hidden=None):\n","        batch_size = x.shape[0] #  batchsize\n","        # initial hidden state\n","        if hidden is None:\n","            h_0 = x.data.new(self.num_layers, batch_size, self.hidden_size).fill_(0).float()\n","        else:\n","            h_0 = hidden\n","        # GRU operation\n","        GRU_output, h_0 = self.gru(x, h_0)\n","\n","        # TCN\n","        x = x.permute(0, 2, 1)\n","        output = self.tcn(x)\n","        output = output.permute(0, 2, 1)\n","\n","        # # Batch Normalization\n","        # output = self.BN(output)\n","\n","        # click_prob = self.LeakyReLU(GRU_output)\n","        # click_prob = click_prob.transpose(0,1)\n","        click_prob = GRU_output.transpose(0,1)\n","\n","        click_prob = self.hidden(click_prob)\n","        click_prob = self.LeakyReLU(click_prob)\n","\n","        click_prob = self.fc_click(click_prob)\n","        click_prob = self.sigmoid(click_prob)\n","\n","        # Attention Layer\n","        query_layer = self.query(GRU_output)\n","        key_layer = self.key(GRU_output).permute(0, 2, 1)\n","        value_layer = self.value(GRU_output)\n","        attention_scores = torch.matmul(query_layer, key_layer)\n","        attention_scores = attention_scores / self.sqrt_size\n","\n","        # Mask\n","        mask = (torch.triu(torch.ones(length, length)) == 0).transpose(0,1).to(device)\n","        attention_scores = attention_scores.masked_fill(mask, value=torch.tensor(-1e9))\n","        # print(attention_scores)\n","\n","        attention_scores = F.dropout(attention_scores, p=0.2)\n","        attention_probs = self.softmax(attention_scores)\n","        output = torch.matmul(attention_probs, value_layer)\n","\n","        # ReLU/LeakyReLU activation\n","        # output = self.ReLU(output)\n","        output = self.LeakyReLU(output)\n","        # output = self.LeakyReLU(GRU_output)\n","\n","\n","        # # Multihead attention\n","        # query_layer = self.query(GRU_output)\n","        # key_layer = self.key(GRU_output)\n","        # value_layer = self.value(GRU_output)\n","        # # print(query_layer.shape)\n","        # output, _ = self.multihead(query_layer,key_layer,value_layer)\n","\n","        # transpose dim 1,2 to get shape (timestep, batch_size, hidden_dim)\n","        output = output.transpose(0,1)\n","\n","        # # Add&Norm (1)\n","        output = F.dropout(self.LeakyReLU(self.hidden(output)), p=0.2) + output\n","        output = self.LayerNorm(output)\n","        output = F.dropout(self.LeakyReLU(self.hidden(output)), p=0.2) + output\n","        output = self.LayerNorm(output)\n","\n","        # # Add&Norm (2)\n","        # output = self.LeakyReLU(self.hidden(output)) + output\n","        # output = self.LayerNorm(output)\n","        # output = F.dropout(output, p=0.2)\n","        # output = self.LeakyReLU(self.hidden(output)) + output\n","        # output = self.LayerNorm(output)\n","    \n","        # # Add&Norm (3)\n","        # output = F.dropout(output, p=0.2) + GRU_output.transpose(0,1)\n","        # output = self.LayerNorm(output)\n","        # output = F.dropout(self.hidden(self.LeakyReLU(self.hidden(output))), p=0.2) + output\n","        # output = self.LayerNorm(output)\n","\n","        output = self.fc(output)  # (batch_size, timestep, output_size)\n","\n","        # Softmax convert to [0,1]\n","        action_prob = F.softmax(output,dim=2)\n","\n","        # all timestep output\n","        return action_prob, click_prob\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690976536379,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"PwFFlRoiOBS6"},"outputs":[],"source":["Batchsize = 512\n","# data_train_loader = DataLoader(dataset=train_ids, batch_size=Batchsize, shuffle=True)\n","data_train_loader = DataLoader(dataset=train_ids, batch_size=Batchsize, shuffle=False)\n","data_val_loader = DataLoader(dataset=val_ids, batch_size=1, shuffle=False,drop_last=False)\n","data_test_loader = DataLoader(dataset=test_ids, batch_size=1, shuffle=False,drop_last=False)\n","# data_full_loader = DataLoader(dataset=data_ids, batch_size=1, shuffle=True)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690976536379,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"5ulgrq9e6DgS"},"outputs":[],"source":["def shannon_entropy(x):\n","  p = x\n","  logp = torch.log2(p)\n","  # print(p)\n","  # print(logp)\n","  entropy = - torch.sum(p*logp,dim=-1)\n","  return entropy"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":122170,"status":"ok","timestamp":1690978204312,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"3qztBjLjOBS6","outputId":"f7e713e5-0d66-4c29-a488-d4a2910d4512"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss of episode 0 = 21.867657\n","Loss of episode 1 = 20.69394\n","Loss of episode 2 = 20.566406\n","Loss of episode 3 = 20.551643\n","Loss of episode 4 = 20.543423\n","Loss of episode 5 = 20.539602\n","Loss of episode 6 = 20.535072\n","Loss of episode 7 = 20.532682\n","Loss of episode 8 = 20.533024\n","Loss of episode 9 = 20.531567\n","Loss of episode 10 = 20.530354\n","Loss of episode 11 = 20.530216\n","Loss of episode 12 = 20.528847\n","Loss of episode 13 = 20.52674\n","Loss of episode 14 = 20.524914\n","Loss of episode 15 = 20.518501\n","Loss of episode 16 = 20.508686\n","Loss of episode 17 = 20.49913\n","Loss of episode 18 = 20.493515\n","Loss of episode 19 = 20.491627\n","Loss of episode 20 = 20.48813\n","Loss of episode 21 = 20.484806\n","Loss of episode 22 = 20.481579\n","Loss of episode 23 = 20.480751\n","Loss of episode 24 = 20.480001\n","Loss of episode 25 = 20.479895\n","Loss of episode 26 = 20.477995\n","Loss of episode 27 = 20.47534\n","Loss of episode 28 = 20.476038\n","Loss of episode 29 = 20.474335\n","Loss of episode 30 = 20.474445\n","Loss of episode 31 = 20.4737\n","Loss of episode 32 = 20.47456\n","Loss of episode 33 = 20.47456\n","Loss of episode 34 = 20.47498\n","Loss of episode 35 = 20.474802\n","Loss of episode 36 = 20.472435\n","Loss of episode 37 = 20.471313\n","Loss of episode 38 = 20.471174\n","Loss of episode 39 = 20.470335\n","Loss of episode 40 = 20.468811\n","Loss of episode 41 = 20.469816\n","Loss of episode 42 = 20.467186\n","Loss of episode 43 = 20.466755\n","Loss of episode 44 = 20.465681\n","Loss of episode 45 = 20.466759\n","Loss of episode 46 = 20.464848\n","Loss of episode 47 = 20.464128\n","Loss of episode 48 = 20.46564\n","Loss of episode 49 = 20.46519\n","Loss of episode 50 = 20.46455\n","Loss of episode 51 = 20.462795\n","Loss of episode 52 = 20.461365\n","Loss of episode 53 = 20.459312\n","Loss of episode 54 = 20.458641\n","Loss of episode 55 = 20.459177\n","Loss of episode 56 = 20.457733\n","Loss of episode 57 = 20.45787\n","Loss of episode 58 = 20.454645\n","Loss of episode 59 = 20.457657\n","Loss of episode 60 = 20.457897\n","Loss of episode 61 = 20.456081\n","Loss of episode 62 = 20.45215\n","Loss of episode 63 = 20.451767\n","Loss of episode 64 = 20.449291\n","Loss of episode 65 = 20.452135\n","Loss of episode 66 = 20.453016\n","Loss of episode 67 = 20.450249\n","Loss of episode 68 = 20.44537\n","Loss of episode 69 = 20.439651\n","Loss of episode 70 = 20.438517\n","Loss of episode 71 = 20.435862\n","Loss of episode 72 = 20.432735\n","Loss of episode 73 = 20.433796\n","Loss of episode 74 = 20.433567\n","Loss of episode 75 = 20.432716\n","Loss of episode 76 = 20.426956\n","Loss of episode 77 = 20.420757\n","Loss of episode 78 = 20.41473\n","Loss of episode 79 = 20.409319\n","Loss of episode 80 = 20.413818\n","Loss of episode 81 = 20.414112\n","Loss of episode 82 = 20.40593\n","Loss of episode 83 = 20.395462\n","Loss of episode 84 = 20.387432\n","Loss of episode 85 = 20.382797\n","Loss of episode 86 = 20.377457\n","Loss of episode 87 = 20.37768\n","Loss of episode 88 = 20.37806\n","Loss of episode 89 = 20.371115\n","Loss of episode 90 = 20.356888\n","Loss of episode 91 = 20.347397\n","Loss of episode 92 = 20.33262\n","Loss of episode 93 = 20.321829\n","Loss of episode 94 = 20.326796\n","Loss of episode 95 = 20.326786\n","Loss of episode 96 = 20.318947\n","Loss of episode 97 = 20.295355\n","Loss of episode 98 = 20.272419\n","Loss of episode 99 = 20.250538\n","Loss of episode 100 = 20.229086\n","Loss of episode 101 = 20.219845\n","Loss of episode 102 = 20.205212\n","Loss of episode 103 = 20.193075\n","Loss of episode 104 = 20.175777\n","Loss of episode 105 = 20.174215\n","Loss of episode 106 = 20.17886\n","Loss of episode 107 = 20.176006\n","Loss of episode 108 = 20.166306\n","Loss of episode 109 = 20.144129\n","Loss of episode 110 = 20.09263\n","Loss of episode 111 = 20.069685\n","Loss of episode 112 = 20.035217\n","Loss of episode 113 = 20.001127\n","Loss of episode 114 = 19.994244\n","Loss of episode 115 = 19.998678\n","Loss of episode 116 = 19.96456\n","Loss of episode 117 = 19.974499\n","Loss of episode 118 = 19.981167\n","Loss of episode 119 = 19.942623\n","Loss of episode 120 = 19.952427\n","Loss of episode 121 = 19.910698\n","Loss of episode 122 = 19.880102\n","Loss of episode 123 = 19.841312\n","Loss of episode 124 = 19.806732\n","Loss of episode 125 = 19.80766\n","Loss of episode 126 = 19.786602\n","Loss of episode 127 = 19.768932\n","Loss of episode 128 = 19.77121\n","Loss of episode 129 = 19.756798\n","Loss of episode 130 = 19.745432\n","Loss of episode 131 = 19.733528\n","Loss of episode 132 = 19.706907\n","Loss of episode 133 = 19.689047\n","Loss of episode 134 = 19.65979\n","Loss of episode 135 = 19.622541\n","Loss of episode 136 = 19.5998\n","Loss of episode 137 = 19.572903\n","Loss of episode 138 = 19.563663\n","Loss of episode 139 = 19.542381\n","Loss of episode 140 = 19.529684\n","Loss of episode 141 = 19.52807\n","Loss of episode 142 = 19.522747\n","Loss of episode 143 = 19.516956\n","Loss of episode 144 = 19.506784\n","Loss of episode 145 = 19.492887\n","Loss of episode 146 = 19.498917\n","Loss of episode 147 = 19.495167\n","Loss of episode 148 = 19.502966\n","Loss of episode 149 = 19.46765\n","Loss of episode 150 = 19.45029\n","Loss of episode 151 = 19.422773\n","Loss of episode 152 = 19.401428\n","Loss of episode 153 = 19.406057\n","Loss of episode 154 = 19.385551\n","Loss of episode 155 = 19.36958\n","Loss of episode 156 = 19.352343\n","Loss of episode 157 = 19.362944\n","Loss of episode 158 = 19.386318\n","Loss of episode 159 = 19.430069\n","Loss of episode 160 = 19.515718\n","Loss of episode 161 = 19.588806\n","Loss of episode 162 = 19.574568\n","Loss of episode 163 = 19.508022\n","Loss of episode 164 = 19.464943\n","Loss of episode 165 = 19.432894\n","Loss of episode 166 = 19.380493\n","Loss of episode 167 = 19.337513\n","Loss of episode 168 = 19.336529\n","Loss of episode 169 = 19.33913\n","Loss of episode 170 = 19.322989\n","Loss of episode 171 = 19.30495\n","Loss of episode 172 = 19.350037\n","Loss of episode 173 = 19.334885\n","Loss of episode 174 = 19.296415\n","Loss of episode 175 = 19.258917\n","Loss of episode 176 = 19.242374\n","Loss of episode 177 = 19.227806\n","Loss of episode 178 = 19.211796\n","Loss of episode 179 = 19.18753\n","Loss of episode 180 = 19.175774\n","Loss of episode 181 = 19.173086\n","Loss of episode 182 = 19.160929\n","Loss of episode 183 = 19.150908\n","Loss of episode 184 = 19.14421\n","Loss of episode 185 = 19.143301\n","Loss of episode 186 = 19.130259\n","Loss of episode 187 = 19.1357\n","Loss of episode 188 = 19.140432\n","Loss of episode 189 = 19.12343\n","Loss of episode 190 = 19.125727\n","Loss of episode 191 = 19.13928\n","Loss of episode 192 = 19.109821\n","Loss of episode 193 = 19.117977\n","Loss of episode 194 = 19.125553\n","Loss of episode 195 = 19.095797\n","Loss of episode 196 = 19.095228\n","Loss of episode 197 = 19.08179\n","Loss of episode 198 = 19.071718\n","Loss of episode 199 = 19.074677\n","Loss of episode 200 = 19.072199\n","Loss of episode 201 = 19.058464\n","Loss of episode 202 = 19.052425\n","Loss of episode 203 = 19.050978\n","Loss of episode 204 = 19.045788\n","Loss of episode 205 = 19.042274\n","Loss of episode 206 = 19.03996\n","Loss of episode 207 = 19.038383\n","Loss of episode 208 = 19.042341\n","Loss of episode 209 = 19.040604\n","Loss of episode 210 = 19.030497\n","Loss of episode 211 = 19.035078\n","Loss of episode 212 = 19.03363\n","Loss of episode 213 = 19.032509\n","Loss of episode 214 = 19.024452\n","Loss of episode 215 = 19.023983\n","Loss of episode 216 = 19.025429\n","Loss of episode 217 = 19.028547\n","Loss of episode 218 = 19.014761\n","Loss of episode 219 = 19.027403\n","Loss of episode 220 = 19.011274\n","Loss of episode 221 = 19.009209\n","Loss of episode 222 = 19.018005\n","Loss of episode 223 = 18.996914\n","Loss of episode 224 = 18.987967\n","Loss of episode 225 = 18.98014\n","Loss of episode 226 = 18.972183\n","Loss of episode 227 = 18.971912\n","Loss of episode 228 = 18.9701\n","Loss of episode 229 = 18.971806\n","Loss of episode 230 = 18.963316\n","Loss of episode 231 = 18.95965\n","Loss of episode 232 = 18.959871\n","Loss of episode 233 = 18.950161\n","Loss of episode 234 = 18.954443\n","Loss of episode 235 = 18.949099\n","Loss of episode 236 = 18.947407\n","Loss of episode 237 = 18.93948\n","Loss of episode 238 = 18.93825\n","Loss of episode 239 = 18.935017\n","Loss of episode 240 = 18.93327\n","Loss of episode 241 = 18.92807\n","Loss of episode 242 = 18.933462\n","Loss of episode 243 = 18.934118\n","Loss of episode 244 = 18.930485\n","Loss of episode 245 = 18.93401\n","Loss of episode 246 = 18.932764\n","Loss of episode 247 = 18.921558\n","Loss of episode 248 = 18.923548\n","Loss of episode 249 = 18.920942\n","Loss of episode 250 = 18.914837\n","Loss of episode 251 = 18.920414\n","Loss of episode 252 = 18.918922\n","Loss of episode 253 = 18.908207\n","Loss of episode 254 = 18.919096\n","Loss of episode 255 = 18.918518\n","Loss of episode 256 = 18.915863\n","Loss of episode 257 = 18.91264\n","Loss of episode 258 = 18.919939\n","Loss of episode 259 = 18.91387\n","Loss of episode 260 = 18.912891\n","Loss of episode 261 = 18.920752\n","Loss of episode 262 = 18.913918\n","Loss of episode 263 = 18.921492\n","Loss of episode 264 = 18.902805\n","Loss of episode 265 = 18.9054\n","Loss of episode 266 = 18.90225\n","Loss of episode 267 = 18.909784\n","Loss of episode 268 = 18.903603\n","Loss of episode 269 = 18.910067\n","Loss of episode 270 = 18.916027\n","Loss of episode 271 = 18.914751\n","Loss of episode 272 = 18.91536\n","Loss of episode 273 = 18.917048\n","Loss of episode 274 = 18.913927\n","Loss of episode 275 = 18.899677\n","Loss of episode 276 = 18.90348\n","Loss of episode 277 = 18.892666\n","Loss of episode 278 = 18.893631\n","Loss of episode 279 = 18.889755\n","Loss of episode 280 = 18.884735\n","Loss of episode 281 = 18.882364\n","Loss of episode 282 = 18.881207\n","Loss of episode 283 = 18.885931\n","Loss of episode 284 = 18.880512\n","Loss of episode 285 = 18.875578\n","Loss of episode 286 = 18.869736\n","Loss of episode 287 = 18.874405\n","Loss of episode 288 = 18.874073\n","Loss of episode 289 = 18.866314\n","Loss of episode 290 = 18.86607\n","Loss of episode 291 = 18.86681\n","Loss of episode 292 = 18.865004\n","Loss of episode 293 = 18.864212\n","Loss of episode 294 = 18.858652\n","Loss of episode 295 = 18.85593\n","Loss of episode 296 = 18.854618\n","Loss of episode 297 = 18.860153\n","Loss of episode 298 = 18.858435\n","Loss of episode 299 = 18.85196\n","Loss of episode 300 = 18.858582\n","Loss of episode 301 = 18.858282\n","Loss of episode 302 = 18.855976\n","Loss of episode 303 = 18.84648\n","Loss of episode 304 = 18.855015\n","Loss of episode 305 = 18.85748\n","Loss of episode 306 = 18.85584\n","Loss of episode 307 = 18.85244\n","Loss of episode 308 = 18.849707\n","Loss of episode 309 = 18.84702\n","Loss of episode 310 = 18.845558\n","Loss of episode 311 = 18.844133\n","Loss of episode 312 = 18.837347\n","Loss of episode 313 = 18.84068\n","Loss of episode 314 = 18.842192\n","Loss of episode 315 = 18.844696\n","Loss of episode 316 = 18.837246\n","Loss of episode 317 = 18.842155\n","Loss of episode 318 = 18.843863\n","Loss of episode 319 = 18.84294\n","Loss of episode 320 = 18.839249\n","Loss of episode 321 = 18.838419\n","Loss of episode 322 = 18.83096\n","Loss of episode 323 = 18.832756\n","Loss of episode 324 = 18.832561\n","Loss of episode 325 = 18.828995\n","Loss of episode 326 = 18.83826\n","Loss of episode 327 = 18.837555\n","Loss of episode 328 = 18.83364\n","Loss of episode 329 = 18.832664\n","Loss of episode 330 = 18.834806\n","Loss of episode 331 = 18.832926\n","Loss of episode 332 = 18.829268\n","Loss of episode 333 = 18.831207\n","Loss of episode 334 = 18.82899\n","Loss of episode 335 = 18.833448\n","Loss of episode 336 = 18.83418\n","Loss of episode 337 = 18.834295\n","Loss of episode 338 = 18.828348\n","Loss of episode 339 = 18.828491\n","Loss of episode 340 = 18.830826\n","Loss of episode 341 = 18.833576\n","Loss of episode 342 = 18.82734\n","Loss of episode 343 = 18.823917\n","Loss of episode 344 = 18.825583\n","Loss of episode 345 = 18.828672\n","Loss of episode 346 = 18.826557\n","Loss of episode 347 = 18.826729\n","Loss of episode 348 = 18.828516\n","Loss of episode 349 = 18.825165\n","Loss of episode 350 = 18.820139\n","Loss of episode 351 = 18.823418\n","Loss of episode 352 = 18.818512\n","Loss of episode 353 = 18.812057\n","Loss of episode 354 = 18.813116\n","Loss of episode 355 = 18.814024\n","Loss of episode 356 = 18.812218\n","Loss of episode 357 = 18.815138\n","Loss of episode 358 = 18.805721\n","Loss of episode 359 = 18.810257\n","Loss of episode 360 = 18.804882\n","Loss of episode 361 = 18.805489\n","Loss of episode 362 = 18.812767\n","Loss of episode 363 = 18.81048\n","Loss of episode 364 = 18.808979\n","Loss of episode 365 = 18.809086\n","Loss of episode 366 = 18.807663\n","Loss of episode 367 = 18.81332\n","Loss of episode 368 = 18.806368\n","Loss of episode 369 = 18.808315\n","Loss of episode 370 = 18.810163\n","Loss of episode 371 = 18.807571\n","Loss of episode 372 = 18.80751\n","Loss of episode 373 = 18.805601\n","Loss of episode 374 = 18.810085\n","Loss of episode 375 = 18.806988\n","Loss of episode 376 = 18.802374\n","Loss of episode 377 = 18.801361\n","Loss of episode 378 = 18.807705\n","Loss of episode 379 = 18.806362\n","Loss of episode 380 = 18.799488\n","Loss of episode 381 = 18.79721\n","Loss of episode 382 = 18.797808\n","Loss of episode 383 = 18.797134\n","Loss of episode 384 = 18.79852\n","Loss of episode 385 = 18.799248\n","Loss of episode 386 = 18.792553\n","Loss of episode 387 = 18.796917\n","Loss of episode 388 = 18.797947\n","Loss of episode 389 = 18.804811\n","Loss of episode 390 = 18.802631\n","Loss of episode 391 = 18.795452\n","Loss of episode 392 = 18.801329\n","Loss of episode 393 = 18.793991\n","Loss of episode 394 = 18.794655\n","Loss of episode 395 = 18.796928\n","Loss of episode 396 = 18.794388\n","Loss of episode 397 = 18.800564\n","Loss of episode 398 = 18.788649\n","Loss of episode 399 = 18.785538\n","Loss of episode 400 = 18.788269\n","Loss of episode 401 = 18.793488\n","Loss of episode 402 = 18.789932\n","Loss of episode 403 = 18.78782\n","Loss of episode 404 = 18.78755\n","Loss of episode 405 = 18.786533\n","Loss of episode 406 = 18.785908\n","Loss of episode 407 = 18.791187\n","Loss of episode 408 = 18.78953\n","Loss of episode 409 = 18.790892\n","Loss of episode 410 = 18.78697\n","Loss of episode 411 = 18.785152\n","Loss of episode 412 = 18.785908\n","Loss of episode 413 = 18.793652\n","Loss of episode 414 = 18.790337\n","Loss of episode 415 = 18.79118\n","Loss of episode 416 = 18.787226\n","Loss of episode 417 = 18.789167\n","Loss of episode 418 = 18.787228\n","Loss of episode 419 = 18.784086\n","Loss of episode 420 = 18.786095\n","Loss of episode 421 = 18.782642\n","Loss of episode 422 = 18.775253\n","Loss of episode 423 = 18.78007\n","Loss of episode 424 = 18.778875\n","Loss of episode 425 = 18.776821\n","Loss of episode 426 = 18.778969\n","Loss of episode 427 = 18.78275\n","Loss of episode 428 = 18.776989\n","Loss of episode 429 = 18.78095\n","Loss of episode 430 = 18.783127\n","Loss of episode 431 = 18.779963\n","Loss of episode 432 = 18.780731\n","Loss of episode 433 = 18.771328\n","Loss of episode 434 = 18.773956\n","Loss of episode 435 = 18.777786\n","Loss of episode 436 = 18.778023\n","Loss of episode 437 = 18.777433\n","Loss of episode 438 = 18.774921\n","Loss of episode 439 = 18.769901\n","Loss of episode 440 = 18.76934\n","Loss of episode 441 = 18.77594\n","Loss of episode 442 = 18.769966\n","Loss of episode 443 = 18.771833\n","Loss of episode 444 = 18.771202\n","Loss of episode 445 = 18.770914\n","Loss of episode 446 = 18.775097\n","Loss of episode 447 = 18.771164\n","Loss of episode 448 = 18.774195\n","Loss of episode 449 = 18.772991\n","Loss of episode 450 = 18.776955\n","Loss of episode 451 = 18.774317\n","Loss of episode 452 = 18.775082\n","Loss of episode 453 = 18.774515\n","Loss of episode 454 = 18.772648\n","Loss of episode 455 = 18.76947\n","Loss of episode 456 = 18.76695\n","Loss of episode 457 = 18.773642\n","Loss of episode 458 = 18.773067\n","Loss of episode 459 = 18.780632\n","Loss of episode 460 = 18.778662\n","Loss of episode 461 = 18.775518\n","Loss of episode 462 = 18.774551\n","Loss of episode 463 = 18.773703\n","Loss of episode 464 = 18.773226\n","Loss of episode 465 = 18.775764\n","Loss of episode 466 = 18.774242\n","Loss of episode 467 = 18.776402\n","Loss of episode 468 = 18.776604\n","Loss of episode 469 = 18.765404\n","Loss of episode 470 = 18.77597\n","Loss of episode 471 = 18.772406\n","Loss of episode 472 = 18.77076\n","Loss of episode 473 = 18.773193\n","Loss of episode 474 = 18.772125\n","Loss of episode 475 = 18.776283\n","Loss of episode 476 = 18.77782\n","Loss of episode 477 = 18.773182\n","Loss of episode 478 = 18.771683\n","Loss of episode 479 = 18.772577\n","Loss of episode 480 = 18.769207\n","Loss of episode 481 = 18.768646\n","Loss of episode 482 = 18.763567\n","Loss of episode 483 = 18.766205\n","Loss of episode 484 = 18.76795\n","Loss of episode 485 = 18.765553\n","Loss of episode 486 = 18.76189\n","Loss of episode 487 = 18.765238\n","Loss of episode 488 = 18.766832\n","Loss of episode 489 = 18.765104\n","Loss of episode 490 = 18.761284\n","Loss of episode 491 = 18.760445\n","Loss of episode 492 = 18.764137\n","Loss of episode 493 = 18.756325\n","Loss of episode 494 = 18.754734\n","Loss of episode 495 = 18.754616\n","Loss of episode 496 = 18.755764\n","Loss of episode 497 = 18.75653\n","Loss of episode 498 = 18.753439\n","Loss of episode 499 = 18.751076\n","Loss of episode 500 = 18.749893\n","Loss of episode 501 = 18.748049\n","Loss of episode 502 = 18.75209\n","Loss of episode 503 = 18.753426\n","Loss of episode 504 = 18.751816\n","Loss of episode 505 = 18.752094\n","Loss of episode 506 = 18.752075\n","Loss of episode 507 = 18.753984\n","Loss of episode 508 = 18.752361\n","Loss of episode 509 = 18.75555\n","Loss of episode 510 = 18.749954\n","Loss of episode 511 = 18.748745\n","Loss of episode 512 = 18.74975\n","Loss of episode 513 = 18.74828\n","Loss of episode 514 = 18.7548\n","Loss of episode 515 = 18.750412\n","Loss of episode 516 = 18.755499\n","Loss of episode 517 = 18.75045\n","Loss of episode 518 = 18.747726\n","Loss of episode 519 = 18.746109\n","Loss of episode 520 = 18.748272\n","Loss of episode 521 = 18.750223\n","Loss of episode 522 = 18.749231\n","Loss of episode 523 = 18.748905\n","Loss of episode 524 = 18.754366\n","Loss of episode 525 = 18.756395\n","Loss of episode 526 = 18.756998\n","Loss of episode 527 = 18.754662\n","Loss of episode 528 = 18.749289\n","Loss of episode 529 = 18.752327\n","Loss of episode 530 = 18.754734\n","Loss of episode 531 = 18.754032\n","Loss of episode 532 = 18.752262\n","Loss of episode 533 = 18.747578\n","Loss of episode 534 = 18.75064\n","Loss of episode 535 = 18.750767\n","Loss of episode 536 = 18.753084\n","Loss of episode 537 = 18.746216\n","Loss of episode 538 = 18.748047\n","Loss of episode 539 = 18.747732\n","Loss of episode 540 = 18.743454\n","Loss of episode 541 = 18.749006\n","Loss of episode 542 = 18.7529\n","Loss of episode 543 = 18.749002\n","Loss of episode 544 = 18.744831\n","Loss of episode 545 = 18.750853\n","Loss of episode 546 = 18.747917\n","Loss of episode 547 = 18.74652\n","Loss of episode 548 = 18.744745\n","Loss of episode 549 = 18.748974\n","Loss of episode 550 = 18.749111\n","Loss of episode 551 = 18.745117\n","Loss of episode 552 = 18.748528\n","Loss of episode 553 = 18.74426\n","Loss of episode 554 = 18.742847\n","Loss of episode 555 = 18.744137\n","Loss of episode 556 = 18.740038\n","Loss of episode 557 = 18.745438\n","Loss of episode 558 = 18.744532\n","Loss of episode 559 = 18.746868\n","Loss of episode 560 = 18.743877\n","Loss of episode 561 = 18.742834\n","Loss of episode 562 = 18.74618\n","Loss of episode 563 = 18.745039\n","Loss of episode 564 = 18.747128\n","Loss of episode 565 = 18.745607\n","Loss of episode 566 = 18.747314\n","Loss of episode 567 = 18.744987\n","Loss of episode 568 = 18.742691\n","Loss of episode 569 = 18.743814\n","Loss of episode 570 = 18.735607\n","Loss of episode 571 = 18.742325\n","Loss of episode 572 = 18.742432\n","Loss of episode 573 = 18.748056\n","Loss of episode 574 = 18.738178\n","Loss of episode 575 = 18.73861\n","Loss of episode 576 = 18.739256\n","Loss of episode 577 = 18.741032\n","Loss of episode 578 = 18.738663\n","Loss of episode 579 = 18.739758\n","Loss of episode 580 = 18.738323\n","Loss of episode 581 = 18.739834\n","Loss of episode 582 = 18.74127\n","Loss of episode 583 = 18.739725\n","Loss of episode 584 = 18.743004\n","Loss of episode 585 = 18.745003\n","Loss of episode 586 = 18.744095\n","Loss of episode 587 = 18.736923\n","Loss of episode 588 = 18.73745\n","Loss of episode 589 = 18.739254\n","Loss of episode 590 = 18.741417\n","Loss of episode 591 = 18.744492\n","Loss of episode 592 = 18.737858\n","Loss of episode 593 = 18.74693\n","Loss of episode 594 = 18.740921\n","Loss of episode 595 = 18.73827\n","Loss of episode 596 = 18.73981\n","Loss of episode 597 = 18.741116\n","Loss of episode 598 = 18.737923\n","Loss of episode 599 = 18.73868\n","Loss of episode 600 = 18.73354\n","Loss of episode 601 = 18.735771\n","Loss of episode 602 = 18.739632\n","Loss of episode 603 = 18.733007\n","Loss of episode 604 = 18.737713\n","Loss of episode 605 = 18.73415\n","Loss of episode 606 = 18.735344\n","Loss of episode 607 = 18.73789\n","Loss of episode 608 = 18.731989\n","Loss of episode 609 = 18.740568\n","Loss of episode 610 = 18.738808\n","Loss of episode 611 = 18.745901\n","Loss of episode 612 = 18.741247\n","Loss of episode 613 = 18.738869\n","Loss of episode 614 = 18.741947\n","Loss of episode 615 = 18.7403\n","Loss of episode 616 = 18.750095\n","Loss of episode 617 = 18.755116\n","Loss of episode 618 = 18.746788\n","Loss of episode 619 = 18.750212\n","Loss of episode 620 = 18.749458\n","Loss of episode 621 = 18.745712\n","Loss of episode 622 = 18.744267\n","Loss of episode 623 = 18.742939\n","Loss of episode 624 = 18.738876\n","Loss of episode 625 = 18.738615\n","Loss of episode 626 = 18.737648\n","Loss of episode 627 = 18.737236\n","Loss of episode 628 = 18.73067\n","Loss of episode 629 = 18.73622\n","Loss of episode 630 = 18.730091\n","Loss of episode 631 = 18.735945\n","Loss of episode 632 = 18.737972\n","Loss of episode 633 = 18.73675\n","Loss of episode 634 = 18.736822\n","Loss of episode 635 = 18.73535\n","Loss of episode 636 = 18.736092\n","Loss of episode 637 = 18.737026\n","Loss of episode 638 = 18.73837\n","Loss of episode 639 = 18.737099\n","Loss of episode 640 = 18.734173\n","Loss of episode 641 = 18.733028\n","Loss of episode 642 = 18.734455\n","Loss of episode 643 = 18.732676\n","Loss of episode 644 = 18.726187\n","Loss of episode 645 = 18.729856\n","Loss of episode 646 = 18.72605\n","Loss of episode 647 = 18.730236\n","Loss of episode 648 = 18.727049\n","Loss of episode 649 = 18.729778\n","Loss of episode 650 = 18.729683\n","Loss of episode 651 = 18.726574\n","Loss of episode 652 = 18.733189\n","Loss of episode 653 = 18.73258\n","Loss of episode 654 = 18.726637\n","Loss of episode 655 = 18.730974\n","Loss of episode 656 = 18.724913\n","Loss of episode 657 = 18.726242\n","Loss of episode 658 = 18.730621\n","Loss of episode 659 = 18.727146\n","Loss of episode 660 = 18.727612\n","Loss of episode 661 = 18.731934\n","Loss of episode 662 = 18.729996\n","Loss of episode 663 = 18.736206\n","Loss of episode 664 = 18.728699\n","Loss of episode 665 = 18.734146\n","Loss of episode 666 = 18.73119\n","Loss of episode 667 = 18.730549\n","Loss of episode 668 = 18.738035\n","Loss of episode 669 = 18.740517\n","Loss of episode 670 = 18.73908\n","Loss of episode 671 = 18.733658\n","Loss of episode 672 = 18.7286\n","Loss of episode 673 = 18.735863\n","Loss of episode 674 = 18.729494\n","Loss of episode 675 = 18.732452\n","Loss of episode 676 = 18.732128\n","Loss of episode 677 = 18.735327\n","Loss of episode 678 = 18.729664\n","Loss of episode 679 = 18.731148\n","Loss of episode 680 = 18.725128\n","Loss of episode 681 = 18.732422\n","Loss of episode 682 = 18.738552\n","Loss of episode 683 = 18.735691\n","Loss of episode 684 = 18.734127\n","Loss of episode 685 = 18.730679\n","Loss of episode 686 = 18.734446\n","Loss of episode 687 = 18.732338\n","Loss of episode 688 = 18.732822\n","Loss of episode 689 = 18.736074\n","Loss of episode 690 = 18.733017\n","Loss of episode 691 = 18.726326\n","Loss of episode 692 = 18.727757\n","Loss of episode 693 = 18.726957\n","Loss of episode 694 = 18.726282\n","Loss of episode 695 = 18.722374\n","Loss of episode 696 = 18.724934\n","Loss of episode 697 = 18.72636\n","Loss of episode 698 = 18.729485\n","Loss of episode 699 = 18.725828\n","Loss of episode 700 = 18.728634\n","Loss of episode 701 = 18.725683\n","Loss of episode 702 = 18.734219\n","Loss of episode 703 = 18.727814\n","Loss of episode 704 = 18.727606\n","Loss of episode 705 = 18.736155\n","Loss of episode 706 = 18.734413\n","Loss of episode 707 = 18.732117\n","Loss of episode 708 = 18.7332\n","Loss of episode 709 = 18.731327\n","Loss of episode 710 = 18.728811\n","Loss of episode 711 = 18.7311\n","Loss of episode 712 = 18.729612\n","Loss of episode 713 = 18.72759\n","Loss of episode 714 = 18.72637\n","Loss of episode 715 = 18.724361\n","Loss of episode 716 = 18.726887\n","Loss of episode 717 = 18.727852\n","Loss of episode 718 = 18.724781\n","Loss of episode 719 = 18.722902\n","Loss of episode 720 = 18.725325\n","Loss of episode 721 = 18.72638\n","Loss of episode 722 = 18.728622\n","Loss of episode 723 = 18.730104\n","Loss of episode 724 = 18.723906\n","Loss of episode 725 = 18.72337\n","Loss of episode 726 = 18.726452\n","Loss of episode 727 = 18.723238\n","Loss of episode 728 = 18.722868\n","Loss of episode 729 = 18.722523\n","Loss of episode 730 = 18.726898\n","Loss of episode 731 = 18.718641\n","Loss of episode 732 = 18.72201\n","Loss of episode 733 = 18.722414\n","Loss of episode 734 = 18.720566\n","Loss of episode 735 = 18.724724\n","Loss of episode 736 = 18.725395\n","Loss of episode 737 = 18.727348\n","Loss of episode 738 = 18.727951\n","Loss of episode 739 = 18.72519\n","Loss of episode 740 = 18.72054\n","Loss of episode 741 = 18.720306\n","Loss of episode 742 = 18.720886\n","Loss of episode 743 = 18.722008\n","Loss of episode 744 = 18.719534\n","Loss of episode 745 = 18.721785\n","Loss of episode 746 = 18.71918\n","Loss of episode 747 = 18.721594\n","Loss of episode 748 = 18.7196\n","Loss of episode 749 = 18.72298\n","Loss of episode 750 = 18.720757\n","Loss of episode 751 = 18.719732\n","Loss of episode 752 = 18.718351\n","Loss of episode 753 = 18.719025\n","Loss of episode 754 = 18.721266\n","Loss of episode 755 = 18.722746\n","Loss of episode 756 = 18.725416\n","Loss of episode 757 = 18.73616\n","Loss of episode 758 = 18.738258\n","Loss of episode 759 = 18.735233\n","Loss of episode 760 = 18.728357\n","Loss of episode 761 = 18.7315\n","Loss of episode 762 = 18.725445\n","Loss of episode 763 = 18.720678\n","Loss of episode 764 = 18.718945\n","Loss of episode 765 = 18.723095\n","Loss of episode 766 = 18.73294\n","Loss of episode 767 = 18.72826\n","Loss of episode 768 = 18.725052\n","Loss of episode 769 = 18.7222\n","Loss of episode 770 = 18.722145\n","Loss of episode 771 = 18.725779\n","Loss of episode 772 = 18.725874\n","Loss of episode 773 = 18.723755\n","Loss of episode 774 = 18.723866\n","Loss of episode 775 = 18.7277\n","Loss of episode 776 = 18.715216\n","Loss of episode 777 = 18.71918\n","Loss of episode 778 = 18.724398\n","Loss of episode 779 = 18.72341\n","Loss of episode 780 = 18.722147\n","Loss of episode 781 = 18.727108\n","Loss of episode 782 = 18.720413\n","Loss of episode 783 = 18.719528\n","Loss of episode 784 = 18.72177\n","Loss of episode 785 = 18.716549\n","Loss of episode 786 = 18.718925\n","Loss of episode 787 = 18.71746\n","Loss of episode 788 = 18.718903\n","Loss of episode 789 = 18.721235\n","Loss of episode 790 = 18.7236\n","Loss of episode 791 = 18.728983\n","Loss of episode 792 = 18.720892\n","Loss of episode 793 = 18.718853\n","Loss of episode 794 = 18.717686\n","Loss of episode 795 = 18.718843\n","Loss of episode 796 = 18.715631\n","Loss of episode 797 = 18.717495\n","Loss of episode 798 = 18.718105\n","Loss of episode 799 = 18.711008\n","\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# feature_size, hidden_size, num_layers, output_size, (number_heads)\n","gru = GRU(state_feature,512,2,3).to(device)\n","num_epoch = 800     # 800\n","Lambda = 1.5        # 1.5\n","Mu = 0.08           # 0.08\n","\n","# optimizer = torch.optim.SGD(net.parameters(),lr=0.5)\n","# optimizer = torch.optim.Adam(gru.parameters(),lr=0.001)\n","optimizer = torch.optim.AdamW(gru.parameters(),lr=0.001)\n","\n","# loss_func = torch.nn.MSELoss()            #MSE for regression\n","loss_func = torch.nn.CrossEntropyLoss()     #CE for classification\n","Loss=[]\n","\n","for t in range(num_epoch):\n","    aver_loss = 0\n","    gru.train()\n","    for batch, data in enumerate(data_train_loader):\n","        x, y ,z = data\n","        action_prediction, click_prediction = gru(x)\n","        # print(action_prediction.shape)\n","        action_prediction = torch.transpose(action_prediction, dim0=0, dim1=1)\n","        click_prediction = torch.transpose(click_prediction, dim0=0, dim1=1)\n","        Entropy = torch.mean(shannon_entropy(action_prediction))\n","        # loss = loss_func(action_prediction,y) - Lambda * Entropy #//2-loss\n","        loss = loss_func(action_prediction,y) + Mu * loss_func(click_prediction,z) \\\n","        - Lambda * Entropy \n","        optimizer.zero_grad() \n","        loss.backward()    \n","        optimizer.step() \n","        aver_loss += loss\n","    aver_loss /= batch\n","    aver_loss=aver_loss.cpu().detach().numpy()\n","    print('Loss of episode %s ='%t,aver_loss)\n","    Loss.append(aver_loss)\n","plt.plot(Loss,color='r')\n","print()"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690978204313,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"lxxPbtYyOBS6","outputId":"d2b4bdae-3248-45d5-9a2c-25bb1d8fc299"},"outputs":[{"name":"stdout","output_type":"stream","text":["6680084\n"]}],"source":["device = torch.device(\"cuda\")\n","gru_test = GRU(state_feature,512,2,3).to(device)\n","total = sum([param.nelement() for param in gru_test.parameters()])\n","print(total)\n","# gru_test"]},{"cell_type":"markdown","metadata":{"id":"Qiqn4DAJOBS7"},"source":["## Training accuracy"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690978204313,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"BUoPP1KjOBS7","outputId":"4ec4f842-dd61-42d5-e7f4-01bea8ab6944"},"outputs":[{"name":"stdout","output_type":"stream","text":["acc = 0.9843906250000001\n"]}],"source":["Truth = []\n","Pred = []\n","aver_loss = 0\n","total_acc = 0\n","Entropy = 0\n","for batch, data in enumerate(data_train_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    x, y, z = data\n","    # print(x)\n","    # prediction =net(x)\n","    test, _ = gru(x)\n","    test = torch.transpose(test, dim0=0, dim1=1)\n","    # print(test)\n","    # print(y)\n","    # print(shannon_entropy(test).shape)\n","    aver_entropy = sum(torch.mean(shannon_entropy(test),dim=1))\n","    # print(sum(aver_entropy))\n","    Entropy += aver_entropy\n","\n","    a = torch.argmax(test,dim = 2).cpu().data.numpy()\n","    b = torch.argmax(y,dim = 2).cpu().data.numpy()\n","    # print(a)\n","    # print(b)\n","    # print(sum(a==b))\n","    aver_acc = sum(sum(a==b))/length\n","    # print(aver_acc)\n","    total_acc += aver_acc\n","    # print(total_acc)\n","# print(batch)\n","print('acc =', total_acc/(0.8*max_number_traj))\n","# print('Entropy = ', Entropy/(0.8*max_number_traj))"]},{"cell_type":"markdown","metadata":{},"source":["## Validation accuracy (Hyperparameter tuning)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Action acc = 0.6743749999999997\n","CTR acc = 0.7497499999999997\n","Entropy =  tensor(0.9938, grad_fn=<DivBackward0>)\n"]}],"source":["from sklearn.metrics import roc_curve, auc\n","\n","\n","Loss=[]\n","Truth = []\n","Pred = []\n","aver_loss = 0\n","total_acc = 0\n","total_ctr = 0\n","total_auc = 0\n","Entropy = 0\n","num = 0\n","\n","# Click_pred = np.zeros((int(0.2*max_number_traj),length))\n","# Click_true = np.zeros((int(0.2*max_number_traj),length))\n","\n","for batch, data in enumerate(data_val_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    gru.eval()\n","    x, y, z = data\n","    test, click = gru(x)\n","    click_pred = torch.squeeze(click).cpu().data.numpy()\n","    click_true = torch.squeeze(z).cpu().data.numpy()\n","    click_pred[click_pred<1/2] = 0\n","    click_pred[click_pred>=1/2] = 1\n","    # Click_pred[batch] = click_pred\n","    # Click_true[batch] = click_true\n","\n","    p = test[:,0].cpu()\n","    aver_entropy = torch.mean(shannon_entropy(p))\n","    # print('aver_entropy =', aver_entropy)\n","    Entropy += aver_entropy\n","    # print(test)\n","    a = torch.argmax(test,dim = 2).cpu().data.numpy().T[0]\n","    b = torch.argmax(y,dim = 2).cpu().data.numpy()[0]\n","    # print('-----------')\n","    # print('pred =',a.T[0])\n","    # print('true =',b[0])\n","    aver_acc = sum(a==b)/length\n","    total_acc += aver_acc\n","\n","    # print('-----------')\n","    # print('pred =', click_pred)\n","    # print('true =', click_true) \n","    aver_ctr = sum(click_pred==click_true)/length\n","    total_ctr += aver_ctr\n","\n","\n","# fpr, tpr, thresholds = roc_curve(Click_true.reshape(-1), Click_pred.reshape(-1),pos_label=1)\n","# total_auc = auc(fpr, tpr)\n","# print('CTR AUC =', total_auc)\n","print('Action acc =', total_acc/(0.1*max_number_traj))\n","print('CTR acc =', total_ctr/(0.1*max_number_traj))\n","print('Entropy = ', Entropy/(0.1*max_number_traj))\n"]},{"cell_type":"markdown","metadata":{"id":"6Hi558JBOBS8"},"source":["## Inference accuracy"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1541,"status":"ok","timestamp":1690978205851,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"dH6BC5O2OBS8","outputId":"f0746245-4215-490b-c3b9-ee67948cc407"},"outputs":[{"name":"stdout","output_type":"stream","text":["Action acc = 0.6577499999999997\n","CTR acc = 0.742625\n","Entropy =  tensor(0.9962, grad_fn=<DivBackward0>)\n"]}],"source":["from sklearn.metrics import roc_curve, auc\n","\n","\n","Loss=[]\n","Truth = []\n","Pred = []\n","aver_loss = 0\n","total_acc = 0\n","total_ctr = 0\n","total_auc = 0\n","Entropy = 0\n","num = 0\n","\n","# Click_pred = np.zeros((int(0.2*max_number_traj),length))\n","# Click_true = np.zeros((int(0.2*max_number_traj),length))\n","\n","for batch, data in enumerate(data_test_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    gru.eval()\n","    x, y, z = data\n","    test, click = gru(x)\n","    click_pred = torch.squeeze(click).cpu().data.numpy()\n","    click_true = torch.squeeze(z).cpu().data.numpy()\n","    click_pred[click_pred<1/2] = 0\n","    click_pred[click_pred>=1/2] = 1\n","    # Click_pred[batch] = click_pred\n","    # Click_true[batch] = click_true\n","\n","    p = test[:,0].cpu()\n","    aver_entropy = torch.mean(shannon_entropy(p))\n","    # print('aver_entropy =', aver_entropy)\n","    Entropy += aver_entropy\n","    # print(test)\n","    a = torch.argmax(test,dim = 2).cpu().data.numpy().T[0]\n","    b = torch.argmax(y,dim = 2).cpu().data.numpy()[0]\n","    # print('-----------')\n","    # print('pred =',a.T[0])\n","    # print('true =',b[0])\n","    aver_acc = sum(a==b)/length\n","    total_acc += aver_acc\n","\n","    # print('-----------')\n","    # print('pred =', click_pred)\n","    # print('true =', click_true) \n","    aver_ctr = sum(click_pred==click_true)/length\n","    total_ctr += aver_ctr\n","\n","\n","# fpr, tpr, thresholds = roc_curve(Click_true.reshape(-1), Click_pred.reshape(-1),pos_label=1)\n","# total_auc = auc(fpr, tpr)\n","# print('CTR AUC =', total_auc)\n","print('Action acc =', total_acc/(0.1*max_number_traj))\n","print('CTR acc =', total_ctr/(0.1*max_number_traj))\n","print('Entropy = ', Entropy/(0.1*max_number_traj))\n"]},{"cell_type":"markdown","metadata":{"id":"n6ecXL0POBS8"},"source":["## AUC"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":736,"status":"ok","timestamp":1690978206585,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"45g3DhluOBS8","outputId":"9e567364-e555-4ef1-be01-fdcdc6de971b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Average AUC = [0.77470135 0.76828068 0.77157773]\n"]}],"source":["from sklearn.metrics import precision_score, roc_curve, auc\n","n_classes = 3\n","# length = 8\n","aver_auc = np.zeros(n_classes)\n","total_precision = 0\n","Y_score = np.zeros(((length, int(0.1*max_number_traj), n_classes)))\n","# print(Y_score.shape)\n","Y_label = np.zeros(((length, int(0.1*max_number_traj), n_classes)))\n","for batch, data in enumerate(data_test_loader):\n","    gru.eval()\n","    x, y, z = data\n","    # print(x)\n","    test, _ = gru(x)\n","    y_score = test[:,0,:].cpu().data.numpy()\n","    y_label = y[0].cpu().data.numpy()\n","    # print(batch)\n","    for i in range(length):\n","        Y_score[i][batch] = y_score[i]\n","        Y_label[i][batch] = y_label[i]\n","        # Y_score[i][batch] = [1/3,1/3,1/3]\n","        # Y_label[i][batch] = y_label[i]\n","        # print(y_score[i])\n","        # print(y_label[i])\n","fpr = dict()\n","tpr = dict()\n","roc_auc = dict()\n","aver_auc = np.zeros(n_classes)\n","for t in range(length):\n","    for i in range(n_classes):\n","        fpr[i], tpr[i], _ = roc_curve(Y_label[t][:, i], Y_score[t][:, i])\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","        # aver_auc[i] += auc(fpr[i], tpr[i])\n","        aver_auc[i] += roc_auc[i]/length\n","    # print('step %s AUC ='%(t+1),roc_auc)\n","print('Average AUC =', aver_auc)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1690978206585,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"Q-5eCoFCOBS8","outputId":"1fe76151-c1a2-4ced-8acf-4a37d2df9e6e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Aver_F1 = 0.6543 Aver_Precision = 0.6554 Aver_Recall = 0.6551\n"]}],"source":["from sklearn.metrics import f1_score, precision_score, recall_score\n","\n","aver_f1 = 0\n","aver_p = 0\n","aver_r = 0\n","\n","# print(y_pred)\n","# print(y_true)\n","for i in range(length):\n","  y_true = np.argmax(Y_label[i],axis = 1)\n","  y_pred = np.argmax(Y_score[i],axis = 1)\n","  f1 = round(f1_score(y_true, y_pred, average='macro' ),4)\n","  p = round(precision_score(y_true, y_pred, average='macro'),4)\n","  r = round(recall_score(y_true, y_pred, average='macro'),4)\n","  aver_f1 += f1/length\n","  aver_p += p/length\n","  aver_r += r/length\n","print('Aver_F1 =', round(aver_f1,4), 'Aver_Precision =', round(aver_p,4), 'Aver_Recall =', round(aver_r,4))"]},{"cell_type":"markdown","metadata":{},"source":["## Budget Allocation"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["test_number = int(0.1*max_number_traj)\n","True_Click_matrix = np.zeros((test_number,length))\n","True_Action_matrix = np.zeros((test_number,length))\n","Click_pred_matrix = np.zeros((test_number,length))\n","Action_pred_matrix = np.zeros((test_number,length))\n","\n","for batch, data in enumerate(data_test_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    gru.eval()\n","    x, y, z = data\n","    action_true = torch.squeeze(torch.argmax(y,dim = 2)).cpu().data.numpy()\n","    click_true = torch.squeeze(z).cpu().data.numpy()\n","    True_Click_matrix[batch] = click_true\n","    # True_Action_count += np.sum(action_true, axis = 0)\n","    \n","    action, click = gru(x)\n","    action_pred = torch.squeeze(torch.argmax(action,dim = 2)).cpu().data.numpy()\n","    # print(action_true == action_pred)\n","    Action_pred_matrix[batch] = (action_true == action_pred)\n","    click_pred = torch.squeeze(click).cpu().data.numpy()\n","    Click_pred_matrix[batch] = click_pred\n","\n","# print(True_Click_matrix)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True_cost =  8000.0\n","True click number =  1352.0\n","Ture_CPC = 5.9171597633136095\n","Policy_cost =  2000.0\n","Policy click number =  234.0\n","Policy_CPC = 8.547008547008547\n"]}],"source":["choose_number = 100     # 50\n","True_cost = 0.1 * max_number_traj * length\n","True_Click_number = sum(sum(True_Click_matrix))\n","User_choose = np.zeros((test_number,length))\n","\n","print('True_cost = ', True_cost)\n","print('True click number = ', True_Click_number)\n","True_CPC = (0.1 * max_number_traj) * length / True_Click_number\n","print('Ture_CPC =',True_CPC)\n","for i in range(length):\n","    tmp = Click_pred_matrix[:,i]\n","    idx = np.argsort(tmp)\n","    # print(idx[400:])\n","    # print(tmp[idx[400:]])\n","    User_choose[:,i][idx[test_number-choose_number:]] = 1\n","\n","Cost = sum(sum(User_choose))\n","Click = sum(sum(User_choose*Action_pred_matrix*True_Click_matrix))\n","cpc = Cost/Click\n","print('Policy_cost = ', Cost)\n","print('Policy click number = ', Click) \n","print('Policy_CPC =', cpc)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
