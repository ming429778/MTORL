{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1481,"status":"ok","timestamp":1690970114664,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"SHePlV70OBS2"},"outputs":[],"source":["# if torch.cuda.is_available():\n","#     print('CUDNN VERSION:',torch.backends.cudnn.version())\n","#     print('Number CUDA Devices:',torch.cuda.device_count())\n","#     print('CUDA Device Name:',torch.cuda.get_device_name(0))\n","#     print('CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6052,"status":"ok","timestamp":1690970120708,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"kUqW9r2qOBS2","outputId":"606cfa3e-2965-43bb-9f91-818746accbc6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on the GPU\n"]}],"source":["import torch\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")  # you can continue going on here, like cuda:1 cuda:2....etc.\n","    print(\"Running on the GPU\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"Running on the CPU\")\n","# torch.cuda.device_count()"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1690973614130,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"iZBy7-2rOBS3"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","Data = pd.read_csv('kuairand_sequence.csv',index_col=0)\n","# Data = pd.read_csv('kuairand_sequence_user=8000.csv',index_col=0)\n","# data['user_id'].value_counts()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1690973619591,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"3HzbfAYVOBS3","outputId":"2e5a6756-eb74-4e81-f782-0caddeae1081"},"outputs":[],"source":["import torch\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import random\n","from torch.utils.data import Dataset, DataLoader,TensorDataset\n","\n","max_length = 100\n","num_user = len(Data['user_id'].unique())\n","state_feature = len(Data.columns) - 1\n","action_feature = 3\n","\n","uid = Data['user_id'].unique()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2991,"status":"ok","timestamp":1690973742240,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"HdF3rl_LOBS4","outputId":"9a25cd7c-7681-4521-a5f4-db7dc0f9d462"},"outputs":[],"source":["State = np.zeros(((num_user,max_length,state_feature)))\n","Action = np.zeros((num_user,max_length,action_feature))\n","Click = np.zeros((num_user,max_length))\n","\n","j = 0\n","for i in uid:\n","    state_sequence = np.array(Data[Data['user_id']==i])[:-1]\n","    # state_sequence = np.array(data[data['user_id']==i].drop(['short','mid','long'],axis=1))\n","    action_sequence = np.array(Data[Data['user_id']==i][['short','mid','long']])[1:]\n","    click_sequence = np.array(Data[Data['user_id']==i]['is_click'])[1:]\n","    # action_sequence = np.array(data[data['user_id']==i][['short','mid','long']])\n","    # print(action_sequence)\n","    state = np.pad(state_sequence,((0,max_length-len(state_sequence)),(0,0)))[:,1:]\n","    action = np.pad(action_sequence,((0,max_length-len(action_sequence)),(0,0)))\n","    click = np.pad(click_sequence,((0,max_length-len(click_sequence))))\n","    # print(click)\n","    State[j] = state\n","    Action[j] = action\n","    Click[j] = click\n","    j += 1\n","\n","# print(len(State))\n","# print(len(Action))\n","# print(len(Click))\n","# print(Click.shape)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":822,"status":"ok","timestamp":1690976535806,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"tPsLnRdtOBS4","outputId":"798634a6-86b6-4797-8422-298b81c046ca"},"outputs":[],"source":["length = 20\n","max_number_traj = 4000\n","\n","number_traj = 0\n","def Select_trajectory(X,Y,Z,number_traj):\n","    Select_states = []\n","    Select_actions = []\n","    Select_clicks = []\n","    # for s in range(number_traj):\n","    while True:\n","        i = random.randint(0,len(X)-1)\n","        select_state = X[i]\n","        select_action = Y[i]\n","        select_click = Z[i]\n","        # print(select_state)\n","        # print(select_action)\n","        for k in range(0,max_length):\n","            # print(select_episode[k])\n","            if select_state[k].any() == 0:\n","                max_episode_length = k\n","                break\n","        # print(k)\n","        # print(max_episode_length)\n","        if max_episode_length >= length + 1:\n","            j = random.randint(0,max_episode_length-length-1)\n","            select_state = select_state[j:j+length]\n","            select_action = select_action[j:j+length]\n","            select_click = select_click[j:j+length]\n","            Select_states.append(select_state)\n","            Select_actions.append(select_action)\n","            Select_clicks.append(select_click)\n","            number_traj += 1\n","        # print(select_trajectory)\n","        if number_traj == max_number_traj:\n","            break\n","    return np.array(Select_states),np.array(Select_actions),np.array(Select_clicks)\n","\n","X , Y , Z = Select_trajectory(State,Action,Click,number_traj)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1690976536378,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"P1xlyTewOBS5"},"outputs":[],"source":["np.random.seed(2023)\n","per = np.random.permutation(X.shape[0])\n","# print(per)\n","X = X[per]\n","Y = Y[per]\n","Z = Z[per]\n","# print(sum(Y))"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1690976536378,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"GVz4ycfnOBS5","outputId":"cb73aa8e-ee79-4218-bb29-653e5d037856"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["np.array([0.5,0,0]).any()"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690976536378,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"YB3Wa9SaOBS5","outputId":"c094f597-85a0-49b2-adee-00ee3d42b5d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["3200\n"]}],"source":["def split_data(State, Action, Reward, timestep, input_size, output_size):\n","\n","    # Training set\n","    train_size = int(np.round(0.8 * X.shape[0]))\n","    print(train_size)\n","    # Split training and test set\n","    x_train = X[: train_size, :].reshape(-1, timestep,input_size)\n","    y_train = Y[: train_size].reshape(-1,timestep, output_size)\n","    z_train = Z[: train_size].reshape(-1,timestep, 1)\n","\n","    x_test = X[train_size:, :].reshape(-1, timestep,input_size)\n","    y_test = Y[train_size:].reshape(-1,timestep, output_size)\n","    z_test = Z[train_size:].reshape(-1,timestep, 1)\n","\n","    return [x_train, y_train, z_train, x_test, y_test, z_test]\n","\n","# State, Action, Reward(is_click)\n","X1,Y1,Z1,X2,Y2,Z2=split_data(X,Y,Z,length,state_feature,action_feature)\n","# print(len(X2))\n","# print(len(X2))\n","X1,Y1=torch.from_numpy(X1).to(torch.float32).to(device),torch.from_numpy(Y1).to(torch.float32).to(device)\n","# print(Y2)\n","X2,Y2=torch.from_numpy(X2).to(torch.float32).to(device),torch.from_numpy(Y2).to(torch.float32).to(device)\n","Z1,Z2=torch.from_numpy(Z1).to(torch.float32).to(device),torch.from_numpy(Z2).to(torch.float32).to(device)\n","# X,Y=torch.from_numpy(X).to(torch.float32),torch.from_numpy(Y).to(torch.float32)\n","train_ids = TensorDataset(X1,Y1,Z1)\n","test_ids = TensorDataset(X2,Y2,Z2)\n","# data_ids = TensorDataset(X,Y)\n","# print(train_ids[12])\n","# print(test_ids[0])"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690976536379,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"gxuxFJ5sOBS6"},"outputs":[],"source":["\n","# 定义GRU网络\n","class GRU(nn.Module):\n","    def __init__(self, feature_size, hidden_size, num_layers, output_size):\n","        super(GRU, self).__init__()\n","        self.hidden_size = hidden_size  # hiddensize\n","        self.num_layers = num_layers  # gru layer\n","        # self.embedding = nn.Linear(feature_size,hidden_size)\n","        # self.gru = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)\n","        self.gru = nn.GRU(feature_size, hidden_size, num_layers, batch_first=True)\n","        # self.lstm = nn.LSTM(feature_size, hidden_size, num_layers, batch_first=True)\n","        self.hidden = nn.Linear(hidden_size, hidden_size)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","        self.fc_click = nn.Linear(hidden_size, 1)\n","        # self.fc1 = nn.Linear(hidden_size, fc_hidden_size)\n","        # self.fc2 = nn.Linear(fc_hidden_size, output_size)\n","        self.sigmoid = nn.Sigmoid()\n","        self.ReLU = nn.ReLU()\n","        self.LeakyReLU = nn.LeakyReLU()\n","        self.BN = nn.BatchNorm1d(8)\n","        self.LayerNorm = nn.LayerNorm(hidden_size)\n","        self.query = nn.Linear(hidden_size, hidden_size)\n","        self.key = nn.Linear(hidden_size, hidden_size)\n","        self.value = nn.Linear(hidden_size, hidden_size)\n","        self.sqrt_size = np.sqrt(hidden_size)\n","        self.softmax = nn.Softmax(dim=-1)\n","        # self.multihead = nn.MultiheadAttention(hidden_size,num_heads,dropout=0.1)\n","    def forward(self, x, hidden=None):\n","        batch_size = x.shape[0] #  batchsize\n","        # initial hidden state\n","        if hidden is None:\n","            h_0 = x.data.new(self.num_layers, batch_size, self.hidden_size).fill_(0).float()\n","        else:\n","            h_0 = hidden\n","\n","        # GRU operation\n","        GRU_output, h_0 = self.gru(x, h_0)\n","\n","        # # 获取GRU输出的维度信息\n","        # batch_size, timestep, hidden_size = GRU_output.shape\n","\n","        # # Batch Normalization\n","        # output = self.BN(output)\n","\n","        # click_prob = self.LeakyReLU(GRU_output)\n","        # click_prob = click_prob.transpose(0,1)\n","        click_prob = GRU_output.transpose(0,1)\n","\n","        click_prob = self.hidden(click_prob)\n","        click_prob = self.LeakyReLU(click_prob)\n","\n","        click_prob = self.fc_click(click_prob)\n","        click_prob = self.sigmoid(click_prob)\n","\n","        # Attention Layer\n","        query_layer = self.query(GRU_output)\n","        key_layer = self.key(GRU_output).permute(0, 2, 1)\n","        value_layer = self.value(GRU_output)\n","        attention_scores = torch.matmul(query_layer, key_layer)\n","        attention_scores = attention_scores / self.sqrt_size\n","\n","        # Mask\n","        mask = (torch.triu(torch.ones(length, length)) == 0).transpose(0,1).to(device)\n","        attention_scores = attention_scores.masked_fill(mask, value=torch.tensor(-1e9))\n","        # print(attention_scores)\n","\n","        attention_scores = F.dropout(attention_scores, p=0.2)\n","        attention_probs = self.softmax(attention_scores)\n","        output = torch.matmul(attention_probs, value_layer)\n","\n","        # ReLU/LeakyReLU activation\n","        # output = self.ReLU(output)\n","        output = self.LeakyReLU(output)\n","        # output = self.LeakyReLU(GRU_output)\n","\n","\n","        # # Multihead attention\n","        # query_layer = self.query(GRU_output)\n","        # key_layer = self.key(GRU_output)\n","        # value_layer = self.value(GRU_output)\n","        # # print(query_layer.shape)\n","        # output, _ = self.multihead(query_layer,key_layer,value_layer)\n","\n","        # 转置1，2维度得到timestep, batch_size, hidden_dim\n","        output = output.transpose(0,1)\n","\n","        # # Add&Norm (1)\n","        output = F.dropout(self.LeakyReLU(self.hidden(output)), p=0.2) + output\n","        output = self.LayerNorm(output)\n","        output = F.dropout(self.LeakyReLU(self.hidden(output)), p=0.2) + output\n","        output = self.LayerNorm(output)\n","\n","        # # Add&Norm (2)\n","        # output = self.LeakyReLU(self.hidden(output)) + output\n","        # output = self.LayerNorm(output)\n","        # output = F.dropout(output, p=0.2)\n","        # output = self.LeakyReLU(self.hidden(output)) + output\n","        # output = self.LayerNorm(output)\n","    \n","        # # Add&Norm (3)\n","        # output = F.dropout(output, p=0.2) + GRU_output.transpose(0,1)\n","        # output = self.LayerNorm(output)\n","        # output = F.dropout(self.hidden(self.LeakyReLU(self.hidden(output))), p=0.2) + output\n","        # output = self.LayerNorm(output)\n","\n","        output = self.fc(output)  # (batch_size, timestep, output_size)\n","\n","        # Softmax convert to [0,1]\n","        action_prob = F.softmax(output,dim=2)\n","\n","        # all timestep output\n","        return action_prob, click_prob\n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690976536379,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"PwFFlRoiOBS6"},"outputs":[],"source":["Batchsize = 512\n","# data_train_loader = DataLoader(dataset=train_ids, batch_size=Batchsize, shuffle=True)\n","data_train_loader = DataLoader(dataset=train_ids, batch_size=Batchsize, shuffle=False)\n","data_test_loader = DataLoader(dataset=test_ids, batch_size=1, shuffle=False,drop_last=False)\n","# data_full_loader = DataLoader(dataset=data_ids, batch_size=1, shuffle=True)\n"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690976536379,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"5ulgrq9e6DgS"},"outputs":[],"source":["def shannon_entropy(x):\n","  p = x\n","  logp = torch.log2(p)\n","  # print(p)\n","  # print(logp)\n","  entropy = - torch.sum(p*logp,dim=-1)\n","  return entropy"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":122170,"status":"ok","timestamp":1690978204312,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"3qztBjLjOBS6","outputId":"f7e713e5-0d66-4c29-a488-d4a2910d4512"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss of episode 0 = 23.119503\n","Loss of episode 1 = 22.589209\n","Loss of episode 2 = 22.38826\n","Loss of episode 3 = 21.65644\n","Loss of episode 4 = 21.559471\n","Loss of episode 5 = 21.535645\n","Loss of episode 6 = 21.51959\n","Loss of episode 7 = 21.514206\n","Loss of episode 8 = 21.511688\n","Loss of episode 9 = 21.508572\n","Loss of episode 10 = 21.509436\n","Loss of episode 11 = 21.50411\n","Loss of episode 12 = 21.507175\n","Loss of episode 13 = 21.50366\n","Loss of episode 14 = 21.502218\n","Loss of episode 15 = 21.500114\n","Loss of episode 16 = 21.499058\n","Loss of episode 17 = 21.49613\n","Loss of episode 18 = 21.49266\n","Loss of episode 19 = 21.490765\n","Loss of episode 20 = 21.489866\n","Loss of episode 21 = 21.488949\n","Loss of episode 22 = 21.489223\n","Loss of episode 23 = 21.488113\n","Loss of episode 24 = 21.488098\n","Loss of episode 25 = 21.486538\n","Loss of episode 26 = 21.48627\n","Loss of episode 27 = 21.486084\n","Loss of episode 28 = 21.484913\n","Loss of episode 29 = 21.485113\n","Loss of episode 30 = 21.484207\n","Loss of episode 31 = 21.483276\n","Loss of episode 32 = 21.483067\n","Loss of episode 33 = 21.480558\n","Loss of episode 34 = 21.4776\n","Loss of episode 35 = 21.470951\n","Loss of episode 36 = 21.464537\n","Loss of episode 37 = 21.455381\n","Loss of episode 38 = 21.447647\n","Loss of episode 39 = 21.44469\n","Loss of episode 40 = 21.44357\n","Loss of episode 41 = 21.44251\n","Loss of episode 42 = 21.44027\n","Loss of episode 43 = 21.43636\n","Loss of episode 44 = 21.437302\n","Loss of episode 45 = 21.437841\n","Loss of episode 46 = 21.437256\n","Loss of episode 47 = 21.432722\n","Loss of episode 48 = 21.432575\n","Loss of episode 49 = 21.431393\n","Loss of episode 50 = 21.428885\n","Loss of episode 51 = 21.429897\n","Loss of episode 52 = 21.427856\n","Loss of episode 53 = 21.427084\n","Loss of episode 54 = 21.426327\n","Loss of episode 55 = 21.426067\n","Loss of episode 56 = 21.42678\n","Loss of episode 57 = 21.425198\n","Loss of episode 58 = 21.423073\n","Loss of episode 59 = 21.42363\n","Loss of episode 60 = 21.42021\n","Loss of episode 61 = 21.42134\n","Loss of episode 62 = 21.422298\n","Loss of episode 63 = 21.421177\n","Loss of episode 64 = 21.421415\n","Loss of episode 65 = 21.417974\n","Loss of episode 66 = 21.418613\n","Loss of episode 67 = 21.419348\n","Loss of episode 68 = 21.416756\n","Loss of episode 69 = 21.415207\n","Loss of episode 70 = 21.412716\n","Loss of episode 71 = 21.411406\n","Loss of episode 72 = 21.408619\n","Loss of episode 73 = 21.410606\n","Loss of episode 74 = 21.413975\n","Loss of episode 75 = 21.410095\n","Loss of episode 76 = 21.406866\n","Loss of episode 77 = 21.404774\n","Loss of episode 78 = 21.402939\n","Loss of episode 79 = 21.403946\n","Loss of episode 80 = 21.402464\n","Loss of episode 81 = 21.414528\n","Loss of episode 82 = 21.40857\n","Loss of episode 83 = 21.4056\n","Loss of episode 84 = 21.396307\n","Loss of episode 85 = 21.392738\n","Loss of episode 86 = 21.388044\n","Loss of episode 87 = 21.390707\n","Loss of episode 88 = 21.392935\n","Loss of episode 89 = 21.388395\n","Loss of episode 90 = 21.384647\n","Loss of episode 91 = 21.37538\n","Loss of episode 92 = 21.376484\n","Loss of episode 93 = 21.374336\n","Loss of episode 94 = 21.372925\n","Loss of episode 95 = 21.363426\n","Loss of episode 96 = 21.36045\n","Loss of episode 97 = 21.36142\n","Loss of episode 98 = 21.360952\n","Loss of episode 99 = 21.353247\n","Loss of episode 100 = 21.360043\n","Loss of episode 101 = 21.357944\n","Loss of episode 102 = 21.345207\n","Loss of episode 103 = 21.334805\n","Loss of episode 104 = 21.3283\n","Loss of episode 105 = 21.325954\n","Loss of episode 106 = 21.317215\n","Loss of episode 107 = 21.312479\n","Loss of episode 108 = 21.311548\n","Loss of episode 109 = 21.31084\n","Loss of episode 110 = 21.305172\n","Loss of episode 111 = 21.29771\n","Loss of episode 112 = 21.285013\n","Loss of episode 113 = 21.275185\n","Loss of episode 114 = 21.264463\n","Loss of episode 115 = 21.257364\n","Loss of episode 116 = 21.240221\n","Loss of episode 117 = 21.237562\n","Loss of episode 118 = 21.221865\n","Loss of episode 119 = 21.20438\n","Loss of episode 120 = 21.225681\n","Loss of episode 121 = 21.206745\n","Loss of episode 122 = 21.184685\n","Loss of episode 123 = 21.157888\n","Loss of episode 124 = 21.14441\n","Loss of episode 125 = 21.12629\n","Loss of episode 126 = 21.10663\n","Loss of episode 127 = 21.095776\n","Loss of episode 128 = 21.094189\n","Loss of episode 129 = 21.088943\n","Loss of episode 130 = 21.061771\n","Loss of episode 131 = 21.042358\n","Loss of episode 132 = 21.030449\n","Loss of episode 133 = 21.010647\n","Loss of episode 134 = 20.987375\n","Loss of episode 135 = 20.996185\n","Loss of episode 136 = 20.980946\n","Loss of episode 137 = 20.952213\n","Loss of episode 138 = 20.927605\n","Loss of episode 139 = 20.92455\n","Loss of episode 140 = 20.946524\n","Loss of episode 141 = 20.902607\n","Loss of episode 142 = 20.86361\n","Loss of episode 143 = 20.845146\n","Loss of episode 144 = 20.819166\n","Loss of episode 145 = 20.787117\n","Loss of episode 146 = 20.754686\n","Loss of episode 147 = 20.730373\n","Loss of episode 148 = 20.715668\n","Loss of episode 149 = 20.716576\n","Loss of episode 150 = 20.722296\n","Loss of episode 151 = 20.7044\n","Loss of episode 152 = 20.659786\n","Loss of episode 153 = 20.630278\n","Loss of episode 154 = 20.614222\n","Loss of episode 155 = 20.582489\n","Loss of episode 156 = 20.564959\n","Loss of episode 157 = 20.545898\n","Loss of episode 158 = 20.53764\n","Loss of episode 159 = 20.512274\n","Loss of episode 160 = 20.488678\n","Loss of episode 161 = 20.481651\n","Loss of episode 162 = 20.463264\n","Loss of episode 163 = 20.445667\n","Loss of episode 164 = 20.417053\n","Loss of episode 165 = 20.42383\n","Loss of episode 166 = 20.410141\n","Loss of episode 167 = 20.403618\n","Loss of episode 168 = 20.40514\n","Loss of episode 169 = 20.412495\n","Loss of episode 170 = 20.44801\n","Loss of episode 171 = 20.49013\n","Loss of episode 172 = 20.486853\n","Loss of episode 173 = 20.452208\n","Loss of episode 174 = 20.406368\n","Loss of episode 175 = 20.361027\n","Loss of episode 176 = 20.331833\n","Loss of episode 177 = 20.303509\n","Loss of episode 178 = 20.297205\n","Loss of episode 179 = 20.28418\n","Loss of episode 180 = 20.253435\n","Loss of episode 181 = 20.260595\n","Loss of episode 182 = 20.226763\n","Loss of episode 183 = 20.166342\n","Loss of episode 184 = 20.153318\n","Loss of episode 185 = 20.126385\n","Loss of episode 186 = 20.099152\n","Loss of episode 187 = 20.082594\n","Loss of episode 188 = 20.06571\n","Loss of episode 189 = 20.042484\n","Loss of episode 190 = 20.032402\n","Loss of episode 191 = 20.025604\n","Loss of episode 192 = 20.025375\n","Loss of episode 193 = 20.019815\n","Loss of episode 194 = 20.006807\n","Loss of episode 195 = 20.021252\n","Loss of episode 196 = 20.0139\n","Loss of episode 197 = 19.991756\n","Loss of episode 198 = 20.003551\n","Loss of episode 199 = 19.998344\n","Loss of episode 200 = 19.97227\n","Loss of episode 201 = 19.952942\n","Loss of episode 202 = 19.961834\n","Loss of episode 203 = 19.944551\n","Loss of episode 204 = 19.94144\n","Loss of episode 205 = 19.939548\n","Loss of episode 206 = 19.936316\n","Loss of episode 207 = 19.930092\n","Loss of episode 208 = 19.915138\n","Loss of episode 209 = 19.920738\n","Loss of episode 210 = 19.908772\n","Loss of episode 211 = 19.907574\n","Loss of episode 212 = 19.895144\n","Loss of episode 213 = 19.879232\n","Loss of episode 214 = 19.876837\n","Loss of episode 215 = 19.86166\n","Loss of episode 216 = 19.868967\n","Loss of episode 217 = 19.848331\n","Loss of episode 218 = 19.842794\n","Loss of episode 219 = 19.841099\n","Loss of episode 220 = 19.832516\n","Loss of episode 221 = 19.82292\n","Loss of episode 222 = 19.808979\n","Loss of episode 223 = 19.807335\n","Loss of episode 224 = 19.805937\n","Loss of episode 225 = 19.804441\n","Loss of episode 226 = 19.80492\n","Loss of episode 227 = 19.800133\n","Loss of episode 228 = 19.791649\n","Loss of episode 229 = 19.798162\n","Loss of episode 230 = 19.820724\n","Loss of episode 231 = 19.817741\n","Loss of episode 232 = 19.800129\n","Loss of episode 233 = 19.795794\n","Loss of episode 234 = 19.795622\n","Loss of episode 235 = 19.789864\n","Loss of episode 236 = 19.783644\n","Loss of episode 237 = 19.773481\n","Loss of episode 238 = 19.76979\n","Loss of episode 239 = 19.761883\n","Loss of episode 240 = 19.758175\n","Loss of episode 241 = 19.754494\n","Loss of episode 242 = 19.757986\n","Loss of episode 243 = 19.755016\n","Loss of episode 244 = 19.75613\n","Loss of episode 245 = 19.759693\n","Loss of episode 246 = 19.756939\n","Loss of episode 247 = 19.74908\n","Loss of episode 248 = 19.743118\n","Loss of episode 249 = 19.7523\n","Loss of episode 250 = 19.747742\n","Loss of episode 251 = 19.742977\n","Loss of episode 252 = 19.745092\n","Loss of episode 253 = 19.737366\n","Loss of episode 254 = 19.730503\n","Loss of episode 255 = 19.720509\n","Loss of episode 256 = 19.710243\n","Loss of episode 257 = 19.705555\n","Loss of episode 258 = 19.70871\n","Loss of episode 259 = 19.711887\n","Loss of episode 260 = 19.699492\n","Loss of episode 261 = 19.69574\n","Loss of episode 262 = 19.697876\n","Loss of episode 263 = 19.696472\n","Loss of episode 264 = 19.693747\n","Loss of episode 265 = 19.692379\n","Loss of episode 266 = 19.695227\n","Loss of episode 267 = 19.691563\n","Loss of episode 268 = 19.693466\n","Loss of episode 269 = 19.692324\n","Loss of episode 270 = 19.69011\n","Loss of episode 271 = 19.701012\n","Loss of episode 272 = 19.697468\n","Loss of episode 273 = 19.690983\n","Loss of episode 274 = 19.695578\n","Loss of episode 275 = 19.68051\n","Loss of episode 276 = 19.67963\n","Loss of episode 277 = 19.675037\n","Loss of episode 278 = 19.674093\n","Loss of episode 279 = 19.671202\n","Loss of episode 280 = 19.67297\n","Loss of episode 281 = 19.662342\n","Loss of episode 282 = 19.662792\n","Loss of episode 283 = 19.660765\n","Loss of episode 284 = 19.66347\n","Loss of episode 285 = 19.662977\n","Loss of episode 286 = 19.668541\n","Loss of episode 287 = 19.664564\n","Loss of episode 288 = 19.664694\n","Loss of episode 289 = 19.673542\n","Loss of episode 290 = 19.662132\n","Loss of episode 291 = 19.662056\n","Loss of episode 292 = 19.665424\n","Loss of episode 293 = 19.656181\n","Loss of episode 294 = 19.658894\n","Loss of episode 295 = 19.655624\n","Loss of episode 296 = 19.652554\n","Loss of episode 297 = 19.666637\n","Loss of episode 298 = 19.666166\n","Loss of episode 299 = 19.659203\n","Loss of episode 300 = 19.658352\n","Loss of episode 301 = 19.660072\n","Loss of episode 302 = 19.663559\n","Loss of episode 303 = 19.663227\n","Loss of episode 304 = 19.670706\n","Loss of episode 305 = 19.667654\n","Loss of episode 306 = 19.663265\n","Loss of episode 307 = 19.659298\n","Loss of episode 308 = 19.661484\n","Loss of episode 309 = 19.67118\n","Loss of episode 310 = 19.674147\n","Loss of episode 311 = 19.667782\n","Loss of episode 312 = 19.66917\n","Loss of episode 313 = 19.671848\n","Loss of episode 314 = 19.666615\n","Loss of episode 315 = 19.657127\n","Loss of episode 316 = 19.65627\n","Loss of episode 317 = 19.65504\n","Loss of episode 318 = 19.642212\n","Loss of episode 319 = 19.640564\n","Loss of episode 320 = 19.62998\n","Loss of episode 321 = 19.62987\n","Loss of episode 322 = 19.630413\n","Loss of episode 323 = 19.630165\n","Loss of episode 324 = 19.62553\n","Loss of episode 325 = 19.628368\n","Loss of episode 326 = 19.62015\n","Loss of episode 327 = 19.626787\n","Loss of episode 328 = 19.623608\n","Loss of episode 329 = 19.622952\n","Loss of episode 330 = 19.625402\n","Loss of episode 331 = 19.624825\n","Loss of episode 332 = 19.628439\n","Loss of episode 333 = 19.618593\n","Loss of episode 334 = 19.62637\n","Loss of episode 335 = 19.641722\n","Loss of episode 336 = 19.656363\n","Loss of episode 337 = 19.64515\n","Loss of episode 338 = 19.64283\n","Loss of episode 339 = 19.63882\n","Loss of episode 340 = 19.632053\n","Loss of episode 341 = 19.633204\n","Loss of episode 342 = 19.634212\n","Loss of episode 343 = 19.626278\n","Loss of episode 344 = 19.62089\n","Loss of episode 345 = 19.626417\n","Loss of episode 346 = 19.61879\n","Loss of episode 347 = 19.616257\n","Loss of episode 348 = 19.615707\n","Loss of episode 349 = 19.620169\n","Loss of episode 350 = 19.622555\n","Loss of episode 351 = 19.62124\n","Loss of episode 352 = 19.61948\n","Loss of episode 353 = 19.611507\n","Loss of episode 354 = 19.617527\n","Loss of episode 355 = 19.619114\n","Loss of episode 356 = 19.613556\n","Loss of episode 357 = 19.619488\n","Loss of episode 358 = 19.607006\n","Loss of episode 359 = 19.605068\n","Loss of episode 360 = 19.605442\n","Loss of episode 361 = 19.605083\n","Loss of episode 362 = 19.606184\n","Loss of episode 363 = 19.608849\n","Loss of episode 364 = 19.606007\n","Loss of episode 365 = 19.606302\n","Loss of episode 366 = 19.609383\n","Loss of episode 367 = 19.60583\n","Loss of episode 368 = 19.606255\n","Loss of episode 369 = 19.603872\n","Loss of episode 370 = 19.603527\n","Loss of episode 371 = 19.60007\n","Loss of episode 372 = 19.602476\n","Loss of episode 373 = 19.59634\n","Loss of episode 374 = 19.602184\n","Loss of episode 375 = 19.600964\n","Loss of episode 376 = 19.601234\n","Loss of episode 377 = 19.602802\n","Loss of episode 378 = 19.60379\n","Loss of episode 379 = 19.605204\n","Loss of episode 380 = 19.604198\n","Loss of episode 381 = 19.60252\n","Loss of episode 382 = 19.602753\n","Loss of episode 383 = 19.602966\n","Loss of episode 384 = 19.594547\n","Loss of episode 385 = 19.592758\n","Loss of episode 386 = 19.592543\n","Loss of episode 387 = 19.596365\n","Loss of episode 388 = 19.593454\n","Loss of episode 389 = 19.59583\n","Loss of episode 390 = 19.589973\n","Loss of episode 391 = 19.588919\n","Loss of episode 392 = 19.588436\n","Loss of episode 393 = 19.590483\n","Loss of episode 394 = 19.593594\n","Loss of episode 395 = 19.585793\n","Loss of episode 396 = 19.587528\n","Loss of episode 397 = 19.594118\n","Loss of episode 398 = 19.592863\n","Loss of episode 399 = 19.592892\n","Loss of episode 400 = 19.596231\n","Loss of episode 401 = 19.593025\n","Loss of episode 402 = 19.591732\n","Loss of episode 403 = 19.59031\n","Loss of episode 404 = 19.586964\n","Loss of episode 405 = 19.590782\n","Loss of episode 406 = 19.590847\n","Loss of episode 407 = 19.590958\n","Loss of episode 408 = 19.589695\n","Loss of episode 409 = 19.59706\n","Loss of episode 410 = 19.600082\n","Loss of episode 411 = 19.593746\n","Loss of episode 412 = 19.596195\n","Loss of episode 413 = 19.586378\n","Loss of episode 414 = 19.58863\n","Loss of episode 415 = 19.59339\n","Loss of episode 416 = 19.592937\n","Loss of episode 417 = 19.592308\n","Loss of episode 418 = 19.59132\n","Loss of episode 419 = 19.587484\n","Loss of episode 420 = 19.586721\n","Loss of episode 421 = 19.584375\n","Loss of episode 422 = 19.586882\n","Loss of episode 423 = 19.584326\n","Loss of episode 424 = 19.59259\n","Loss of episode 425 = 19.586874\n","Loss of episode 426 = 19.587786\n","Loss of episode 427 = 19.581734\n","Loss of episode 428 = 19.589859\n","Loss of episode 429 = 19.60006\n","Loss of episode 430 = 19.608648\n","Loss of episode 431 = 19.612713\n","Loss of episode 432 = 19.605492\n","Loss of episode 433 = 19.60121\n","Loss of episode 434 = 19.61165\n","Loss of episode 435 = 19.611279\n","Loss of episode 436 = 19.602554\n","Loss of episode 437 = 19.60476\n","Loss of episode 438 = 19.598476\n","Loss of episode 439 = 19.596529\n","Loss of episode 440 = 19.592049\n","Loss of episode 441 = 19.596756\n","Loss of episode 442 = 19.592068\n","Loss of episode 443 = 19.602058\n","Loss of episode 444 = 19.603525\n","Loss of episode 445 = 19.60395\n","Loss of episode 446 = 19.597519\n","Loss of episode 447 = 19.591919\n","Loss of episode 448 = 19.588825\n","Loss of episode 449 = 19.593601\n","Loss of episode 450 = 19.591206\n","Loss of episode 451 = 19.588707\n","Loss of episode 452 = 19.591192\n","Loss of episode 453 = 19.58516\n","Loss of episode 454 = 19.587254\n","Loss of episode 455 = 19.598267\n","Loss of episode 456 = 19.603575\n","Loss of episode 457 = 19.596571\n","Loss of episode 458 = 19.597012\n","Loss of episode 459 = 19.58686\n","Loss of episode 460 = 19.589355\n","Loss of episode 461 = 19.589046\n","Loss of episode 462 = 19.58868\n","Loss of episode 463 = 19.588682\n","Loss of episode 464 = 19.589087\n","Loss of episode 465 = 19.589333\n","Loss of episode 466 = 19.587685\n","Loss of episode 467 = 19.582426\n","Loss of episode 468 = 19.58097\n","Loss of episode 469 = 19.579777\n","Loss of episode 470 = 19.583298\n","Loss of episode 471 = 19.583563\n","Loss of episode 472 = 19.582878\n","Loss of episode 473 = 19.577986\n","Loss of episode 474 = 19.573143\n","Loss of episode 475 = 19.572187\n","Loss of episode 476 = 19.575127\n","Loss of episode 477 = 19.575203\n","Loss of episode 478 = 19.577599\n","Loss of episode 479 = 19.590952\n","Loss of episode 480 = 19.601673\n","Loss of episode 481 = 19.60457\n","Loss of episode 482 = 19.611225\n","Loss of episode 483 = 19.60907\n","Loss of episode 484 = 19.60339\n","Loss of episode 485 = 19.599003\n","Loss of episode 486 = 19.587444\n","Loss of episode 487 = 19.587967\n","Loss of episode 488 = 19.582638\n","Loss of episode 489 = 19.58598\n","Loss of episode 490 = 19.586227\n","Loss of episode 491 = 19.592995\n","Loss of episode 492 = 19.589567\n","Loss of episode 493 = 19.580984\n","Loss of episode 494 = 19.586586\n","Loss of episode 495 = 19.58905\n","Loss of episode 496 = 19.583347\n","Loss of episode 497 = 19.58556\n","Loss of episode 498 = 19.59145\n","Loss of episode 499 = 19.58504\n","Loss of episode 500 = 19.601894\n","Loss of episode 501 = 19.592237\n","Loss of episode 502 = 19.588512\n","Loss of episode 503 = 19.595297\n","Loss of episode 504 = 19.586517\n","Loss of episode 505 = 19.576916\n","Loss of episode 506 = 19.595987\n","Loss of episode 507 = 19.597252\n","Loss of episode 508 = 19.595427\n","Loss of episode 509 = 19.590183\n","Loss of episode 510 = 19.583115\n","Loss of episode 511 = 19.580143\n","Loss of episode 512 = 19.571693\n","Loss of episode 513 = 19.570501\n","Loss of episode 514 = 19.571892\n","Loss of episode 515 = 19.579689\n","Loss of episode 516 = 19.595715\n","Loss of episode 517 = 19.590504\n","Loss of episode 518 = 19.582619\n","Loss of episode 519 = 19.577816\n","Loss of episode 520 = 19.574444\n","Loss of episode 521 = 19.574717\n","Loss of episode 522 = 19.569168\n","Loss of episode 523 = 19.572275\n","Loss of episode 524 = 19.564407\n","Loss of episode 525 = 19.566616\n","Loss of episode 526 = 19.568907\n","Loss of episode 527 = 19.565744\n","Loss of episode 528 = 19.563255\n","Loss of episode 529 = 19.572079\n","Loss of episode 530 = 19.564142\n","Loss of episode 531 = 19.560322\n","Loss of episode 532 = 19.56349\n","Loss of episode 533 = 19.55983\n","Loss of episode 534 = 19.553253\n","Loss of episode 535 = 19.55434\n","Loss of episode 536 = 19.555264\n","Loss of episode 537 = 19.558075\n","Loss of episode 538 = 19.55894\n","Loss of episode 539 = 19.558119\n","Loss of episode 540 = 19.560625\n","Loss of episode 541 = 19.55647\n","Loss of episode 542 = 19.555733\n","Loss of episode 543 = 19.555735\n","Loss of episode 544 = 19.558609\n","Loss of episode 545 = 19.555933\n","Loss of episode 546 = 19.554163\n","Loss of episode 547 = 19.558115\n","Loss of episode 548 = 19.55893\n","Loss of episode 549 = 19.556496\n","Loss of episode 550 = 19.564762\n","Loss of episode 551 = 19.568323\n","Loss of episode 552 = 19.569172\n","Loss of episode 553 = 19.574062\n","Loss of episode 554 = 19.575562\n","Loss of episode 555 = 19.573866\n","Loss of episode 556 = 19.572615\n","Loss of episode 557 = 19.567516\n","Loss of episode 558 = 19.566925\n","Loss of episode 559 = 19.5702\n","Loss of episode 560 = 19.560966\n","Loss of episode 561 = 19.566654\n","Loss of episode 562 = 19.56147\n","Loss of episode 563 = 19.560797\n","Loss of episode 564 = 19.564108\n","Loss of episode 565 = 19.563244\n","Loss of episode 566 = 19.557447\n","Loss of episode 567 = 19.559631\n","Loss of episode 568 = 19.55523\n","Loss of episode 569 = 19.563887\n","Loss of episode 570 = 19.568958\n","Loss of episode 571 = 19.569057\n","Loss of episode 572 = 19.568079\n","Loss of episode 573 = 19.566317\n","Loss of episode 574 = 19.566399\n","Loss of episode 575 = 19.561178\n","Loss of episode 576 = 19.555878\n","Loss of episode 577 = 19.555113\n","Loss of episode 578 = 19.555805\n","Loss of episode 579 = 19.555456\n","Loss of episode 580 = 19.549028\n","Loss of episode 581 = 19.55184\n","Loss of episode 582 = 19.55372\n","Loss of episode 583 = 19.553764\n","Loss of episode 584 = 19.550755\n","Loss of episode 585 = 19.552149\n","Loss of episode 586 = 19.55029\n","Loss of episode 587 = 19.5463\n","Loss of episode 588 = 19.548687\n","Loss of episode 589 = 19.550764\n","Loss of episode 590 = 19.54634\n","Loss of episode 591 = 19.542812\n","Loss of episode 592 = 19.5455\n","Loss of episode 593 = 19.548208\n","Loss of episode 594 = 19.543968\n","Loss of episode 595 = 19.543945\n","Loss of episode 596 = 19.54676\n","Loss of episode 597 = 19.551804\n","Loss of episode 598 = 19.553925\n","Loss of episode 599 = 19.554174\n","Loss of episode 600 = 19.554884\n","Loss of episode 601 = 19.55151\n","Loss of episode 602 = 19.55114\n","Loss of episode 603 = 19.552551\n","Loss of episode 604 = 19.55721\n","Loss of episode 605 = 19.555298\n","Loss of episode 606 = 19.552055\n","Loss of episode 607 = 19.55132\n","Loss of episode 608 = 19.543953\n","Loss of episode 609 = 19.544556\n","Loss of episode 610 = 19.54689\n","Loss of episode 611 = 19.54811\n","Loss of episode 612 = 19.5475\n","Loss of episode 613 = 19.550215\n","Loss of episode 614 = 19.55672\n","Loss of episode 615 = 19.56919\n","Loss of episode 616 = 19.569284\n","Loss of episode 617 = 19.555523\n","Loss of episode 618 = 19.552853\n","Loss of episode 619 = 19.551317\n","Loss of episode 620 = 19.552805\n","Loss of episode 621 = 19.551718\n","Loss of episode 622 = 19.54992\n","Loss of episode 623 = 19.550743\n","Loss of episode 624 = 19.551443\n","Loss of episode 625 = 19.542845\n","Loss of episode 626 = 19.547106\n","Loss of episode 627 = 19.541702\n","Loss of episode 628 = 19.545292\n","Loss of episode 629 = 19.544935\n","Loss of episode 630 = 19.545242\n","Loss of episode 631 = 19.544138\n","Loss of episode 632 = 19.539467\n","Loss of episode 633 = 19.537624\n","Loss of episode 634 = 19.539131\n","Loss of episode 635 = 19.545704\n","Loss of episode 636 = 19.537231\n","Loss of episode 637 = 19.544897\n","Loss of episode 638 = 19.54655\n","Loss of episode 639 = 19.554428\n","Loss of episode 640 = 19.558838\n","Loss of episode 641 = 19.552536\n","Loss of episode 642 = 19.546772\n","Loss of episode 643 = 19.568535\n","Loss of episode 644 = 19.574963\n","Loss of episode 645 = 19.566475\n","Loss of episode 646 = 19.562706\n","Loss of episode 647 = 19.556492\n","Loss of episode 648 = 19.55706\n","Loss of episode 649 = 19.55594\n","Loss of episode 650 = 19.558159\n","Loss of episode 651 = 19.554644\n","Loss of episode 652 = 19.556479\n","Loss of episode 653 = 19.567593\n","Loss of episode 654 = 19.559225\n","Loss of episode 655 = 19.558302\n","Loss of episode 656 = 19.558834\n","Loss of episode 657 = 19.556824\n","Loss of episode 658 = 19.556824\n","Loss of episode 659 = 19.557245\n","Loss of episode 660 = 19.550503\n","Loss of episode 661 = 19.551222\n","Loss of episode 662 = 19.550179\n","Loss of episode 663 = 19.555378\n","Loss of episode 664 = 19.556547\n","Loss of episode 665 = 19.55768\n","Loss of episode 666 = 19.547516\n","Loss of episode 667 = 19.547562\n","Loss of episode 668 = 19.551857\n","Loss of episode 669 = 19.55104\n","Loss of episode 670 = 19.554438\n","Loss of episode 671 = 19.550762\n","Loss of episode 672 = 19.549477\n","Loss of episode 673 = 19.546268\n","Loss of episode 674 = 19.547781\n","Loss of episode 675 = 19.55378\n","Loss of episode 676 = 19.555529\n","Loss of episode 677 = 19.552887\n","Loss of episode 678 = 19.54895\n","Loss of episode 679 = 19.548788\n","Loss of episode 680 = 19.548544\n","Loss of episode 681 = 19.547985\n","Loss of episode 682 = 19.552784\n","Loss of episode 683 = 19.549911\n","Loss of episode 684 = 19.54815\n","Loss of episode 685 = 19.548697\n","Loss of episode 686 = 19.541428\n","Loss of episode 687 = 19.543266\n","Loss of episode 688 = 19.547785\n","Loss of episode 689 = 19.555267\n","Loss of episode 690 = 19.557394\n","Loss of episode 691 = 19.551388\n","Loss of episode 692 = 19.553942\n","Loss of episode 693 = 19.56496\n","Loss of episode 694 = 19.567348\n","Loss of episode 695 = 19.562893\n","Loss of episode 696 = 19.560669\n","Loss of episode 697 = 19.557583\n","Loss of episode 698 = 19.551819\n","Loss of episode 699 = 19.549744\n","Loss of episode 700 = 19.546476\n","Loss of episode 701 = 19.546455\n","Loss of episode 702 = 19.544956\n","Loss of episode 703 = 19.544708\n","Loss of episode 704 = 19.543837\n","Loss of episode 705 = 19.544464\n","Loss of episode 706 = 19.544884\n","Loss of episode 707 = 19.540585\n","Loss of episode 708 = 19.54034\n","Loss of episode 709 = 19.542616\n","Loss of episode 710 = 19.541868\n","Loss of episode 711 = 19.543789\n","Loss of episode 712 = 19.537365\n","Loss of episode 713 = 19.535435\n","Loss of episode 714 = 19.538834\n","Loss of episode 715 = 19.539736\n","Loss of episode 716 = 19.537745\n","Loss of episode 717 = 19.53976\n","Loss of episode 718 = 19.545681\n","Loss of episode 719 = 19.538395\n","Loss of episode 720 = 19.541677\n","Loss of episode 721 = 19.5401\n","Loss of episode 722 = 19.536098\n","Loss of episode 723 = 19.535366\n","Loss of episode 724 = 19.534603\n","Loss of episode 725 = 19.534252\n","Loss of episode 726 = 19.532185\n","Loss of episode 727 = 19.536001\n","Loss of episode 728 = 19.535942\n","Loss of episode 729 = 19.534733\n","Loss of episode 730 = 19.535938\n","Loss of episode 731 = 19.53427\n","Loss of episode 732 = 19.538502\n","Loss of episode 733 = 19.537205\n","Loss of episode 734 = 19.531685\n","Loss of episode 735 = 19.531036\n","Loss of episode 736 = 19.531807\n","Loss of episode 737 = 19.535154\n","Loss of episode 738 = 19.537113\n","Loss of episode 739 = 19.539133\n","Loss of episode 740 = 19.53858\n","Loss of episode 741 = 19.543077\n","Loss of episode 742 = 19.546326\n","Loss of episode 743 = 19.541119\n","Loss of episode 744 = 19.540369\n","Loss of episode 745 = 19.536236\n","Loss of episode 746 = 19.539951\n","Loss of episode 747 = 19.538692\n","Loss of episode 748 = 19.535896\n","Loss of episode 749 = 19.53912\n","Loss of episode 750 = 19.536352\n","Loss of episode 751 = 19.533802\n","Loss of episode 752 = 19.532272\n","Loss of episode 753 = 19.53685\n","Loss of episode 754 = 19.530834\n","Loss of episode 755 = 19.532177\n","Loss of episode 756 = 19.531612\n","Loss of episode 757 = 19.536774\n","Loss of episode 758 = 19.536865\n","Loss of episode 759 = 19.53429\n","Loss of episode 760 = 19.538607\n","Loss of episode 761 = 19.540112\n","Loss of episode 762 = 19.53482\n","Loss of episode 763 = 19.535118\n","Loss of episode 764 = 19.53531\n","Loss of episode 765 = 19.533508\n","Loss of episode 766 = 19.531067\n","Loss of episode 767 = 19.567581\n","Loss of episode 768 = 19.575132\n","Loss of episode 769 = 19.57813\n","Loss of episode 770 = 19.577923\n","Loss of episode 771 = 19.566582\n","Loss of episode 772 = 19.564247\n","Loss of episode 773 = 19.560753\n","Loss of episode 774 = 19.555983\n","Loss of episode 775 = 19.557163\n","Loss of episode 776 = 19.56404\n","Loss of episode 777 = 19.563759\n","Loss of episode 778 = 19.559309\n","Loss of episode 779 = 19.562479\n","Loss of episode 780 = 19.560173\n","Loss of episode 781 = 19.559586\n","Loss of episode 782 = 19.549625\n","Loss of episode 783 = 19.54824\n","Loss of episode 784 = 19.548487\n","Loss of episode 785 = 19.545462\n","Loss of episode 786 = 19.558208\n","Loss of episode 787 = 19.556175\n","Loss of episode 788 = 19.552687\n","Loss of episode 789 = 19.554562\n","Loss of episode 790 = 19.539715\n","Loss of episode 791 = 19.542217\n","Loss of episode 792 = 19.539234\n","Loss of episode 793 = 19.539234\n","Loss of episode 794 = 19.538546\n","Loss of episode 795 = 19.541424\n","Loss of episode 796 = 19.543068\n","Loss of episode 797 = 19.537437\n","Loss of episode 798 = 19.54152\n","Loss of episode 799 = 19.54359\n","\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# feature_size, hidden_size, num_layers, output_size, (number_heads)\n","gru = GRU(state_feature,512,2,3).to(device)\n","num_epoch = 800     # 800\n","Lambda = 1.5        # 1.5\n","Mu = 0.08           # 0.08\n","\n","# optimizer = torch.optim.SGD(net.parameters(),lr=0.5)\n","# optimizer = torch.optim.Adam(gru.parameters(),lr=0.001)\n","optimizer = torch.optim.AdamW(gru.parameters(),lr=0.001)\n","\n","# loss_func = torch.nn.MSELoss()            #MSE for regression\n","loss_func = torch.nn.CrossEntropyLoss()     #CE for classification\n","Loss=[]\n","\n","for t in range(num_epoch):\n","    aver_loss = 0\n","    gru.train()\n","    for batch, data in enumerate(data_train_loader):\n","        x, y ,z = data\n","        action_prediction, click_prediction = gru(x)\n","        # print(action_prediction.shape)\n","        action_prediction = torch.transpose(action_prediction, dim0=0, dim1=1)\n","        click_prediction = torch.transpose(click_prediction, dim0=0, dim1=1)\n","        Entropy = torch.mean(shannon_entropy(action_prediction))\n","        # loss = loss_func(action_prediction,y) - Lambda * Entropy #//2-loss\n","        loss = loss_func(action_prediction,y) + Mu * loss_func(click_prediction,z) \\\n","        - Lambda * Entropy \n","        optimizer.zero_grad() \n","        loss.backward()    \n","        optimizer.step() \n","        aver_loss += loss\n","    aver_loss /= batch\n","    aver_loss=aver_loss.cpu().detach().numpy()\n","    print('Loss of episode %s ='%t,aver_loss)\n","    Loss.append(aver_loss)\n","plt.plot(Loss,color='r')\n","print()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690978204313,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"lxxPbtYyOBS6","outputId":"d2b4bdae-3248-45d5-9a2c-25bb1d8fc299"},"outputs":[{"name":"stdout","output_type":"stream","text":["3460628\n"]}],"source":["device = torch.device(\"cuda\")\n","gru_test = GRU(state_feature,512,2,3).to(device)\n","total = sum([param.nelement() for param in gru_test.parameters()])\n","print(total)\n","# gru_test"]},{"cell_type":"markdown","metadata":{"id":"Qiqn4DAJOBS7"},"source":["## Training accuracy"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690978204313,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"BUoPP1KjOBS7","outputId":"4ec4f842-dd61-42d5-e7f4-01bea8ab6944"},"outputs":[{"name":"stdout","output_type":"stream","text":["acc = 0.9835937500000002\n"]}],"source":["Truth = []\n","Pred = []\n","aver_loss = 0\n","total_acc = 0\n","Entropy = 0\n","for batch, data in enumerate(data_train_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    x, y, z = data\n","    # print(x)\n","    # prediction =net(x)\n","    test, _ = gru(x)\n","    test = torch.transpose(test, dim0=0, dim1=1)\n","    # print(test)\n","    # print(y)\n","    # print(shannon_entropy(test).shape)\n","    aver_entropy = sum(torch.mean(shannon_entropy(test),dim=1))\n","    # print(sum(aver_entropy))\n","    Entropy += aver_entropy\n","\n","    a = torch.argmax(test,dim = 2).cpu().data.numpy()\n","    b = torch.argmax(y,dim = 2).cpu().data.numpy()\n","    # print(a)\n","    # print(b)\n","    # print(sum(a==b))\n","    aver_acc = sum(sum(a==b))/length\n","    # print(aver_acc)\n","    total_acc += aver_acc\n","    # print(total_acc)\n","# print(batch)\n","print('acc =', total_acc/(0.8*max_number_traj))\n","# print('Entropy = ', Entropy/(0.8*max_number_traj))"]},{"cell_type":"markdown","metadata":{"id":"6Hi558JBOBS8"},"source":["## Inference accuracy"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1541,"status":"ok","timestamp":1690978205851,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"dH6BC5O2OBS8","outputId":"f0746245-4215-490b-c3b9-ee67948cc407"},"outputs":[{"name":"stdout","output_type":"stream","text":["Action acc = 0.6536874999999998\n","CTR acc = 0.7885000000000006\n","Entropy =  tensor(0.9904, grad_fn=<DivBackward0>)\n"]}],"source":["from sklearn.metrics import roc_curve, auc\n","\n","\n","Loss=[]\n","Truth = []\n","Pred = []\n","aver_loss = 0\n","total_acc = 0\n","total_ctr = 0\n","total_auc = 0\n","Entropy = 0\n","num = 0\n","\n","# Click_pred = np.zeros((int(0.2*max_number_traj),length))\n","# Click_true = np.zeros((int(0.2*max_number_traj),length))\n","\n","for batch, data in enumerate(data_test_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    gru.eval()\n","    x, y, z = data\n","    test, click = gru(x)\n","    click_pred = torch.squeeze(click).cpu().data.numpy()\n","    click_true = torch.squeeze(z).cpu().data.numpy()\n","    click_pred[click_pred<1/2] = 0\n","    click_pred[click_pred>=1/2] = 1\n","    # Click_pred[batch] = click_pred\n","    # Click_true[batch] = click_true\n","\n","    p = test[:,0].cpu()\n","    aver_entropy = torch.mean(shannon_entropy(p))\n","    # print('aver_entropy =', aver_entropy)\n","    Entropy += aver_entropy\n","    # print(test)\n","    a = torch.argmax(test,dim = 2).cpu().data.numpy().T[0]\n","    b = torch.argmax(y,dim = 2).cpu().data.numpy()[0]\n","    # print('-----------')\n","    # print('pred =',a.T[0])\n","    # print('true =',b[0])\n","    aver_acc = sum(a==b)/length\n","    total_acc += aver_acc\n","\n","    # print('-----------')\n","    # print('pred =', click_pred)\n","    # print('true =', click_true) \n","    aver_ctr = sum(click_pred==click_true)/length\n","    total_ctr += aver_ctr\n","\n","\n","# fpr, tpr, thresholds = roc_curve(Click_true.reshape(-1), Click_pred.reshape(-1),pos_label=1)\n","# total_auc = auc(fpr, tpr)\n","# print('CTR AUC =', total_auc)\n","print('Action acc =', total_acc/(0.2*max_number_traj))\n","print('CTR acc =', total_ctr/(0.2*max_number_traj))\n","print('Entropy = ', Entropy/(0.2*max_number_traj))\n"]},{"cell_type":"markdown","metadata":{"id":"n6ecXL0POBS8"},"source":["## AUC"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":736,"status":"ok","timestamp":1690978206585,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"45g3DhluOBS8","outputId":"9e567364-e555-4ef1-be01-fdcdc6de971b"},"outputs":[{"name":"stdout","output_type":"stream","text":["step 1 AUC = {0: 0.9944434909465374, 1: 0.9913067784765898, 2: 0.991867201426025}\n","step 2 AUC = {0: 0.9379973790366861, 1: 0.9289000246447057, 2: 0.9163080099610071}\n","step 3 AUC = {0: 0.6734168870162163, 1: 0.6724081221681489, 2: 0.6720649251986911}\n","step 4 AUC = {0: 0.639830717686475, 1: 0.6843447915099139, 2: 0.6819027536591417}\n","step 5 AUC = {0: 0.7087915874197195, 1: 0.7465586634969037, 2: 0.7021891780256093}\n","step 6 AUC = {0: 0.7073130281791697, 1: 0.7019236205336951, 2: 0.6990196883813905}\n","step 7 AUC = {0: 0.7210512235645293, 1: 0.7119940072365445, 2: 0.6889867225716282}\n","step 8 AUC = {0: 0.742235870314688, 1: 0.7524378776819941, 2: 0.719528278738805}\n","step 9 AUC = {0: 0.742230583639706, 1: 0.7314853498217468, 2: 0.7313739416221033}\n","step 10 AUC = {0: 0.7796516080977578, 1: 0.7568021243954624, 2: 0.7433373712901272}\n","step 11 AUC = {0: 0.7944512928883801, 1: 0.7483647798742138, 2: 0.7348782618189759}\n","step 12 AUC = {0: 0.8010691823899371, 1: 0.7980338835367611, 2: 0.7737394764605505}\n","step 13 AUC = {0: 0.7782215726893498, 1: 0.7486048519945718, 2: 0.7644178196167928}\n","step 14 AUC = {0: 0.7662299405229491, 1: 0.79064541196732, 2: 0.7620140054237385}\n","step 15 AUC = {0: 0.7847439916405434, 1: 0.7851743179085925, 2: 0.8021231606998029}\n","step 16 AUC = {0: 0.807320807843308, 1: 0.8052142622113008, 2: 0.8272390109890111}\n","step 17 AUC = {0: 0.8039715833631975, 1: 0.7981283422459893, 2: 0.8377369756680102}\n","step 18 AUC = {0: 0.8030510642185074, 1: 0.7964233235598832, 2: 0.7713984550064865}\n","step 19 AUC = {0: 0.7783760683760683, 1: 0.8079682066567312, 2: 0.7955337977781962}\n","step 20 AUC = {0: 0.7495129140230803, 1: 0.7483159451244559, 2: 0.7669338205382569}\n","Average AUC = [0.77569554 0.77525173 0.76912964]\n"]}],"source":["from sklearn.metrics import precision_score, roc_curve, auc\n","n_classes = 3\n","# length = 8\n","aver_auc = np.zeros(n_classes)\n","total_precision = 0\n","Y_score = np.zeros(((length, int(0.2*max_number_traj), n_classes)))\n","# print(Y_score.shape)\n","Y_label = np.zeros(((length, int(0.2*max_number_traj), n_classes)))\n","for batch, data in enumerate(data_test_loader):\n","    gru.eval()\n","    x, y, z = data\n","    # print(x)\n","    test, _ = gru(x)\n","    y_score = test[:,0,:].cpu().data.numpy()\n","    y_label = y[0].cpu().data.numpy()\n","    # print(batch)\n","    for i in range(length):\n","        Y_score[i][batch] = y_score[i]\n","        Y_label[i][batch] = y_label[i]\n","        # Y_score[i][batch] = [1/3,1/3,1/3]\n","        # Y_label[i][batch] = y_label[i]\n","        # print(y_score[i])\n","        # print(y_label[i])\n","fpr = dict()\n","tpr = dict()\n","roc_auc = dict()\n","aver_auc = np.zeros(n_classes)\n","for t in range(length):\n","    for i in range(n_classes):\n","        fpr[i], tpr[i], _ = roc_curve(Y_label[t][:, i], Y_score[t][:, i])\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","        # aver_auc[i] += auc(fpr[i], tpr[i])\n","        aver_auc[i] += roc_auc[i]/length\n","    print('step %s AUC ='%(t+1),roc_auc)\n","print('Average AUC =', aver_auc)\n","    # precision = precision_score(a,b, average=\"micro\")\n","    # total_precision += precision\n","# aver_precesion = total_precision/batch\n","# print(aver_precesion)\n","# print(aver_auc)"]},{"cell_type":"code","execution_count":60,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690978206585,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"YVkh1qOGOBS8"},"outputs":[],"source":["# aver_acc = np.zeros(length)\n","# for batch, data in enumerate(data_test_loader):\n","# # for batch, data in enumerate(data_full_loader):\n","# # for x,y in data_train_loader:\n","#     x, y = data\n","#     # print(x)\n","#     # prediction =net(x)\n","#     test = gru(x)\n","#     # print(test)\n","#     # print(y)\n","#     a = torch.argmax(test,dim = 2).cpu().data.numpy()\n","#     b = torch.argmax(y,dim = 2).cpu().data.numpy()\n","#     # print('-----------')\n","#     # print('predict =',a.T[0])\n","#     # print('true =',b[0])\n","#     for i in range(length):\n","#         aver_acc[i] += a.T[0][i]==b[0][i]\n","# aver_acc /= batch\n","# print('step acc =', aver_acc)\n","# print('aver acc =', sum(aver_acc)/length)\n"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1690978206585,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"Q-5eCoFCOBS8","outputId":"1fe76151-c1a2-4ced-8acf-4a37d2df9e6e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Step 1: F1 = 0.9588 Precision = 0.9588 Recall = 0.959\n","Step 2: F1 = 0.8341 Precision = 0.8359 Recall = 0.8333\n","Step 3: F1 = 0.534 Precision = 0.5357 Recall = 0.5342\n","Step 4: F1 = 0.5243 Precision = 0.5243 Recall = 0.5244\n","Step 5: F1 = 0.5665 Precision = 0.5667 Recall = 0.5666\n","Step 6: F1 = 0.5334 Precision = 0.5346 Recall = 0.5342\n","Step 7: F1 = 0.5641 Precision = 0.5659 Recall = 0.5653\n","Step 8: F1 = 0.6007 Precision = 0.6019 Recall = 0.6022\n","Step 9: F1 = 0.6112 Precision = 0.6117 Recall = 0.6122\n","Step 10: F1 = 0.6393 Precision = 0.6393 Recall = 0.6393\n","Step 11: F1 = 0.6454 Precision = 0.6458 Recall = 0.6477\n","Step 12: F1 = 0.6645 Precision = 0.6651 Recall = 0.6647\n","Step 13: F1 = 0.6553 Precision = 0.6558 Recall = 0.6555\n","Step 14: F1 = 0.6674 Precision = 0.6684 Recall = 0.6676\n","Step 15: F1 = 0.6917 Precision = 0.6918 Recall = 0.6917\n","Step 16: F1 = 0.6986 Precision = 0.6989 Recall = 0.6992\n","Step 17: F1 = 0.6977 Precision = 0.699 Recall = 0.6974\n","Step 18: F1 = 0.6747 Precision = 0.6754 Recall = 0.6749\n","Step 19: F1 = 0.6782 Precision = 0.6778 Recall = 0.6796\n","Step 20: F1 = 0.6202 Precision = 0.6207 Recall = 0.6212\n","Aver_F1 = 0.653 Aver_Precision = 0.6537 Aver_Recall = 0.6535\n"]}],"source":["from sklearn.metrics import f1_score, precision_score, recall_score\n","\n","aver_f1 = 0\n","aver_p = 0\n","aver_r = 0\n","\n","# print(y_pred)\n","# print(y_true)\n","for i in range(length):\n","  y_true = np.argmax(Y_label[i],axis = 1)\n","  y_pred = np.argmax(Y_score[i],axis = 1)\n","  f1 = round(f1_score(y_true, y_pred, average='macro' ),4)\n","  p = round(precision_score(y_true, y_pred, average='macro'),4)\n","  r = round(recall_score(y_true, y_pred, average='macro'),4)\n","  aver_f1 += f1/length\n","  aver_p += p/length\n","  aver_r += r/length\n","  print('Step %s:'%(i+1),'F1 =', f1, 'Precision =', p, 'Recall =', r)\n","print('Aver_F1 =', round(aver_f1,4), 'Aver_Precision =', round(aver_p,4), 'Aver_Recall =', round(aver_r,4))"]},{"cell_type":"markdown","metadata":{},"source":["## Budget Allocation"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["# True_CPC = 0\n","# threshold = 0.01\n","# True_Click_number = 0\n","# Policy_click_number = 0\n","# Action_number = 0\n","# True_Action_count = [0] * action_feature\n","# # print(True_Action_count)\n","# Adj_Action_count = [0] * action_feature\n","# Pred_Action_count = [0] * action_feature\n","\n","# for batch, data in enumerate(data_test_loader):\n","# # for batch, data in enumerate(data_full_loader):\n","# # for x,y in data_train_loader:\n","#     gru.eval()\n","#     x, y, z = data\n","#     action_true = torch.squeeze(y).cpu().data.numpy()\n","#     click_true = torch.squeeze(z).cpu().data.numpy()\n","\n","#     True_Action_count += np.sum(action_true, axis = 0)\n","#     # print(True_action)\n","#     # print(Ture_click)\n","#     Batch_click_number = sum(click_true)\n","#     True_Click_number += Batch_click_number\n","#     # print(batch_click_number)\n","\n","#     action, click = gru(x)\n","#     action_pred = torch.squeeze(action).cpu().data.numpy()\n","#     click_pred = torch.squeeze(click).cpu().data.numpy()\n","\n","#     click_pred[click_pred < threshold] = 0\n","#     click_pred[click_pred >= threshold] = 1\n","\n","\n","    \n","#     Adj_action = np.dot(click_pred, action_true)\n","#     # print(new_action)\n","#     Adj_Action_count += Adj_action\n","\n","#     # Action_pred_count += np.sum(np.exp(action_pred) / np.sum(np.exp(action_pred)), axis = 0)\n","#     Pred_Action_count += np.sum(action_pred, axis = 0)\n","\n","\n","#     Bacth_Action_Policy = click_pred.copy()\n","#     Bacth_Action_number = sum(Bacth_Action_Policy)\n","#     Action_number += Bacth_Action_number\n","\n","#     Policy_click = Bacth_Action_Policy * click_true\n","#     Policy_click_number += sum(Policy_click)\n","\n","# print('True click number = ', True_Click_number)\n","# print('Policy click number = ', Policy_click_number)\n","# True_cost = 0.2 * max_number_traj * length\n","# Policy_cost = Action_number\n","\n","# print('True_cost = ', True_cost)\n","# print('Policy_cost = ', Policy_cost)\n","\n","# True_CPC = (0.2 * max_number_traj) * length / True_Click_number\n","# Policy_CPC = Action_number /  Policy_click_number\n","# print('Ture_CPC =',True_CPC)\n","# print('Policy_CPC =',Policy_CPC)"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["test_number = int(0.2*max_number_traj)\n","True_Click_matrix = np.zeros((test_number,length))\n","True_Action_matrix = np.zeros((test_number,length))\n","Click_pred_matrix = np.zeros((test_number,length))\n","Action_pred_matrix = np.zeros((test_number,length))\n","\n","for batch, data in enumerate(data_test_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    gru.eval()\n","    x, y, z = data\n","    action_true = torch.squeeze(torch.argmax(y,dim = 2)).cpu().data.numpy()\n","    click_true = torch.squeeze(z).cpu().data.numpy()\n","    True_Click_matrix[batch] = click_true\n","    # True_Action_count += np.sum(action_true, axis = 0)\n","    \n","    action, click = gru(x)\n","    action_pred = torch.squeeze(torch.argmax(action,dim = 2)).cpu().data.numpy()\n","    # print(action_true == action_pred)\n","    Action_pred_matrix[batch] = (action_true == action_pred)\n","    click_pred = torch.squeeze(click).cpu().data.numpy()\n","    Click_pred_matrix[batch] = click_pred\n","\n","# print(True_Click_matrix)"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True_cost =  16000.0\n","True click number =  2869.0\n","Ture_CPC = 5.576856047403276\n","Policy_cost =  100.0\n","Policy click number =  46.0\n","Policy_CPC = 2.1739130434782608\n"]}],"source":["choose_number = 200     # 50\n","True_cost = 0.2 * max_number_traj * length\n","True_Click_number = sum(sum(True_Click_matrix))\n","User_choose = np.zeros((test_number,length))\n","\n","print('True_cost = ', True_cost)\n","print('True click number = ', True_Click_number)\n","True_CPC = (0.2 * max_number_traj) * length / True_Click_number\n","print('Ture_CPC =',True_CPC)\n","for i in range(length):\n","    tmp = Click_pred_matrix[:,i]\n","    idx = np.argsort(tmp)\n","    # print(idx[400:])\n","    # print(tmp[idx[400:]])\n","    User_choose[:,i][idx[test_number-choose_number:]] = 1\n","\n","Cost = sum(sum(User_choose))\n","Click = sum(sum(User_choose*Action_pred_matrix*True_Click_matrix))\n","cpc = Cost/Click\n","print('Policy_cost = ', Cost)\n","print('Policy click number = ', Click) \n","print('Policy_CPC =', cpc)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
