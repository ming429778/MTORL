{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1481,"status":"ok","timestamp":1690970114664,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"SHePlV70OBS2"},"outputs":[],"source":["# if torch.cuda.is_available():\n","#     print('CUDNN VERSION:',torch.backends.cudnn.version())\n","#     print('Number CUDA Devices:',torch.cuda.device_count())\n","#     print('CUDA Device Name:',torch.cuda.get_device_name(0))\n","#     print('CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6052,"status":"ok","timestamp":1690970120708,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"kUqW9r2qOBS2","outputId":"606cfa3e-2965-43bb-9f91-818746accbc6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on the GPU\n"]}],"source":["import torch\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")  # you can continue going on here, like cuda:1 cuda:2....etc.\n","    print(\"Running on the GPU\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"Running on the CPU\")\n","# torch.cuda.device_count()"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1690973614130,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"iZBy7-2rOBS3"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","Data = pd.read_csv('kuairand_sequence.csv',index_col=0)\n","# Data = pd.read_csv('kuairand_sequence_user=8000.csv',index_col=0)\n","# data['user_id'].value_counts()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1690973619591,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"3HzbfAYVOBS3","outputId":"2e5a6756-eb74-4e81-f782-0caddeae1081"},"outputs":[],"source":["import torch\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import random\n","from torch.utils.data import Dataset, DataLoader,TensorDataset\n","\n","max_length = 100\n","num_user = len(Data['user_id'].unique())\n","state_feature = len(Data.columns) - 1\n","action_feature = 3\n","\n","uid = Data['user_id'].unique()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2991,"status":"ok","timestamp":1690973742240,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"HdF3rl_LOBS4","outputId":"9a25cd7c-7681-4521-a5f4-db7dc0f9d462"},"outputs":[],"source":["State = np.zeros(((num_user,max_length,state_feature)))\n","Action = np.zeros((num_user,max_length,action_feature))\n","Click = np.zeros((num_user,max_length))\n","\n","j = 0\n","for i in uid:\n","    state_sequence = np.array(Data[Data['user_id']==i])[:-1]\n","    # state_sequence = np.array(data[data['user_id']==i].drop(['short','mid','long'],axis=1))\n","    action_sequence = np.array(Data[Data['user_id']==i][['short','mid','long']])[1:]\n","    click_sequence = np.array(Data[Data['user_id']==i]['is_click'])[1:]\n","    # action_sequence = np.array(data[data['user_id']==i][['short','mid','long']])\n","    # print(action_sequence)\n","    state = np.pad(state_sequence,((0,max_length-len(state_sequence)),(0,0)))[:,1:]\n","    action = np.pad(action_sequence,((0,max_length-len(action_sequence)),(0,0)))\n","    click = np.pad(click_sequence,((0,max_length-len(click_sequence))))\n","    # print(click)\n","    State[j] = state\n","    Action[j] = action\n","    Click[j] = click\n","    j += 1\n","\n","# print(len(State))\n","# print(len(Action))\n","# print(len(Click))\n","# print(Click.shape)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":822,"status":"ok","timestamp":1690976535806,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"tPsLnRdtOBS4","outputId":"798634a6-86b6-4797-8422-298b81c046ca"},"outputs":[],"source":["length = 20\n","max_number_traj = 4000\n","\n","number_traj = 0\n","def Select_trajectory(X,Y,Z,number_traj):\n","    Select_states = []\n","    Select_actions = []\n","    Select_clicks = []\n","    # for s in range(number_traj):\n","    while True:\n","        i = random.randint(0,len(X)-1)\n","        select_state = X[i]\n","        select_action = Y[i]\n","        select_click = Z[i]\n","        # print(select_state)\n","        # print(select_action)\n","        for k in range(0,max_length):\n","            # print(select_episode[k])\n","            if select_state[k].any() == 0:\n","                max_episode_length = k\n","                break\n","        # print(k)\n","        # print(max_episode_length)\n","        if max_episode_length >= length + 1:\n","            j = random.randint(0,max_episode_length-length-1)\n","            select_state = select_state[j:j+length]\n","            select_action = select_action[j:j+length]\n","            select_click = select_click[j:j+length]\n","            Select_states.append(select_state)\n","            Select_actions.append(select_action)\n","            Select_clicks.append(select_click)\n","            number_traj += 1\n","        # print(select_trajectory)\n","        if number_traj == max_number_traj:\n","            break\n","    return np.array(Select_states),np.array(Select_actions),np.array(Select_clicks)\n","\n","X , Y , Z = Select_trajectory(State,Action,Click,number_traj)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1690976536378,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"P1xlyTewOBS5"},"outputs":[],"source":["np.random.seed(2023)\n","per = np.random.permutation(X.shape[0])\n","# print(per)\n","X = X[per]\n","Y = Y[per]\n","Z = Z[per]\n","X,Y,Z = torch.from_numpy(X).to(torch.float32).to(device),torch.from_numpy(Y).to(torch.float32).to(device), torch.from_numpy(Z).to(torch.float32).to(device)\n","# print(sum(Y))"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690976536378,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"YB3Wa9SaOBS5","outputId":"c094f597-85a0-49b2-adee-00ee3d42b5d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["3200\n"]}],"source":["def split_data(State, Action, Reward, timestep, input_size, output_size):\n","\n","    # Training set\n","    train_size = int(np.round(0.8 * X.shape[0]))\n","    val_size = int(np.round(0.1 * X.shape[0]))\n","    print(train_size)\n","\n","    # Split training and test set 8:1:1\n","    x_train = X[: train_size, :].reshape(-1, timestep,input_size)\n","    y_train = Y[: train_size].reshape(-1,timestep, output_size)\n","    z_train = Z[: train_size].reshape(-1,timestep, 1)\n","\n","    # x_test = X[train_size:, :].reshape(-1, timestep,input_size)\n","    # y_test = Y[train_size:].reshape(-1,timestep, output_size)\n","    # z_test = Z[train_size:].reshape(-1,timestep, 1)\n","\n","    x_val = X[train_size:train_size+val_size, :].reshape(-1, timestep,input_size)\n","    y_val = Y[train_size:train_size+val_size].reshape(-1,timestep, output_size)\n","    z_val = Z[train_size:train_size+val_size].reshape(-1,timestep, 1) \n","\n","    x_test = X[train_size+val_size:, :].reshape(-1, timestep,input_size)\n","    y_test = Y[train_size+val_size:].reshape(-1,timestep, output_size)\n","    z_test = Z[train_size+val_size:].reshape(-1,timestep, 1)\n","\n","    return [x_train, y_train, z_train, x_val, y_val, z_val, x_test, y_test, z_test]\n","\n","# State, Action, Reward(is_click)\n","X1,Y1,Z1,X2,Y2,Z2,X3,Y3,Z3 = split_data(X,Y,Z,length,state_feature,action_feature)\n","# print(X1.shape)\n","# print(Y1.shape)\n","# print(Z1.shape)\n","# X1,Y1=torch.from_numpy(X1).to(torch.float32).to(device),torch.from_numpy(Y1).to(torch.float32).to(device)\n","# X2,Y2=torch.from_numpy(X2).to(torch.float32).to(device),torch.from_numpy(Y2).to(torch.float32).to(device)\n","# Z1,Z2=torch.from_numpy(Z1).to(torch.float32).to(device),torch.from_numpy(Z2).to(torch.float32).to(device)\n","train_ids = TensorDataset(X1,Y1,Z1)\n","val_ids = TensorDataset(X2,Y2,Z2)\n","test_ids = TensorDataset(X3,Y3,Z3)\n","# print(test_ids[0])"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([512, 512, 20])\n"]}],"source":["\n","from pytorch_tcn.tcn import TCN\n","x = torch.ones(512,state_feature,length)\n","model = TCN(state_feature,[512,512],causal=True)\n","print(model(x).shape)\n","# model = TCN(\n","#     num_inputs: int,\n","#     num_channels: ArrayLike,\n","#     kernel_size: int = 4,\n","#     dilations: Optional[ ArrayLike ] = None,\n","#     dilaton_reset: Optional[ int ] = None,\n","#     dropout: float = 0.1,\n","#     causal: bool = True,\n","#     use_norm: str = 'weight_norm',\n","#     activation: str = 'relu',\n","#     kernel_initializer: str = 'xavier_uniform',\n","#     use_skip_connections: bool = False,\n","#     input_shape: str = 'NCL',\n","# )"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690976536379,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"gxuxFJ5sOBS6"},"outputs":[],"source":["class MTORL(nn.Module):\n","    def __init__(self, feature_size, hidden_size, num_layers, output_size):\n","        super(MTORL, self).__init__()\n","        self.hidden_size = hidden_size  # hiddensize\n","        self.num_layers = num_layers  # gru layer\n","        # self.embedding = nn.Linear(feature_size,hidden_size)\n","        # self.gru = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)\n","        self.gru = nn.GRU(feature_size, hidden_size, num_layers, batch_first=True)\n","        # self.lstm = nn.LSTM(feature_size, hidden_size, num_layers, batch_first=True)\n","        self.hidden = nn.Linear(hidden_size, hidden_size)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","        self.fc_click = nn.Linear(hidden_size, 1)\n","        # self.fc1 = nn.Linear(hidden_size, fc_hidden_size)\n","        # self.fc2 = nn.Linear(fc_hidden_size, output_size)\n","        self.sigmoid = nn.Sigmoid()\n","        self.ReLU = nn.ReLU()\n","        self.LeakyReLU = nn.LeakyReLU()\n","        self.BN = nn.BatchNorm1d(8)\n","        self.LayerNorm = nn.LayerNorm(hidden_size)\n","        self.query = nn.Linear(hidden_size, hidden_size)\n","        self.key = nn.Linear(hidden_size, hidden_size)\n","        self.value = nn.Linear(hidden_size, hidden_size)\n","        self.sqrt_size = np.sqrt(hidden_size)\n","        self.softmax = nn.Softmax(dim=-1)\n","        self.tcn = TCN(state_feature,[hidden_size,hidden_size])\n","        # self.multihead = nn.MultiheadAttention(hidden_size,num_heads,dropout=0.1)\n","    def forward(self, x, hidden=None):\n","        batch_size = x.shape[0] #  batchsize\n","\n","        # # initial hidden state\n","        # if hidden is None:\n","        #     h_0 = x.data.new(self.num_layers, batch_size, self.hidden_size).fill_(0).float()\n","        # else:\n","        #     h_0 = hidden\n","        # # GRU operation\n","        # output, h_0 = self.gru(x, h_0)\n","\n","        # TCN\n","        x = x.permute(0, 2, 1)\n","        output = self.tcn(x)\n","        output = output.permute(0, 2, 1)\n","\n","        # # Batch Normalization\n","        # output = self.BN(output)\n","\n","        # click_prob = self.LeakyReLU(GRU_output)\n","        # click_prob = click_prob.transpose(0,1)\n","        click_prob = output.transpose(0,1)\n","\n","        click_prob = self.hidden(click_prob)\n","        click_prob = self.LeakyReLU(click_prob)\n","\n","        click_prob = self.fc_click(click_prob)\n","        click_prob = self.sigmoid(click_prob)\n","\n","        # Attention Layer\n","        query_layer = self.query(output)\n","        key_layer = self.key(output).permute(0, 2, 1)\n","        value_layer = self.value(output)\n","        attention_scores = torch.matmul(query_layer, key_layer)\n","        attention_scores = attention_scores / self.sqrt_size\n","\n","        # Mask\n","        mask = (torch.triu(torch.ones(length, length)) == 0).transpose(0,1).to(device)\n","        attention_scores = attention_scores.masked_fill(mask, value=torch.tensor(-1e9))\n","        # print(attention_scores)\n","\n","        attention_scores = F.dropout(attention_scores, p=0.2)\n","        attention_probs = self.softmax(attention_scores)\n","        output = torch.matmul(attention_probs, value_layer)\n","\n","        # ReLU/LeakyReLU activation\n","        # output = self.ReLU(output)\n","        output = self.LeakyReLU(output)\n","        # output = self.LeakyReLU(GRU_output)\n","\n","\n","        # # Multihead attention\n","        # query_layer = self.query(GRU_output)\n","        # key_layer = self.key(GRU_output)\n","        # value_layer = self.value(GRU_output)\n","        # # print(query_layer.shape)\n","        # output, _ = self.multihead(query_layer,key_layer,value_layer)\n","\n","        # transpose dim 1,2 to get shape (timestep, batch_size, hidden_dim)\n","        output = output.transpose(0,1)\n","\n","        # # Add&Norm (1)\n","        output = F.dropout(self.LeakyReLU(self.hidden(output)), p=0.2) + output\n","        output = self.LayerNorm(output)\n","        output = F.dropout(self.LeakyReLU(self.hidden(output)), p=0.2) + output\n","        output = self.LayerNorm(output)\n","\n","        # # Add&Norm (2)\n","        # output = self.LeakyReLU(self.hidden(output)) + output\n","        # output = self.LayerNorm(output)\n","        # output = F.dropout(output, p=0.2)\n","        # output = self.LeakyReLU(self.hidden(output)) + output\n","        # output = self.LayerNorm(output)\n","    \n","        # # Add&Norm (3)\n","        # output = F.dropout(output, p=0.2) + GRU_output.transpose(0,1)\n","        # output = self.LayerNorm(output)\n","        # output = F.dropout(self.hidden(self.LeakyReLU(self.hidden(output))), p=0.2) + output\n","        # output = self.LayerNorm(output)\n","\n","        output = self.fc(output)  # (batch_size, timestep, output_size)\n","\n","        # Softmax convert to [0,1]\n","        action_prob = F.softmax(output,dim=2)\n","\n","        # all timestep output\n","        return action_prob, click_prob\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690976536379,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"PwFFlRoiOBS6"},"outputs":[],"source":["Batchsize = 512\n","# data_train_loader = DataLoader(dataset=train_ids, batch_size=Batchsize, shuffle=True)\n","data_train_loader = DataLoader(dataset=train_ids, batch_size=Batchsize, shuffle=False)\n","data_val_loader = DataLoader(dataset=val_ids, batch_size=1, shuffle=False,drop_last=False)\n","data_test_loader = DataLoader(dataset=test_ids, batch_size=1, shuffle=False,drop_last=False)\n","# data_full_loader = DataLoader(dataset=data_ids, batch_size=1, shuffle=True)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690976536379,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"5ulgrq9e6DgS"},"outputs":[],"source":["def shannon_entropy(x):\n","  p = x\n","  logp = torch.log2(p)\n","  # print(p)\n","  # print(logp)\n","  entropy = - torch.sum(p*logp,dim=-1)\n","  return entropy"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":122170,"status":"ok","timestamp":1690978204312,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"3qztBjLjOBS6","outputId":"f7e713e5-0d66-4c29-a488-d4a2910d4512"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss of episode 0 = 22.948284\n","Loss of episode 1 = 22.70519\n","Loss of episode 2 = 22.57649\n","Loss of episode 3 = 22.455292\n","Loss of episode 4 = 21.805698\n","Loss of episode 5 = 21.608814\n","Loss of episode 6 = 21.56767\n","Loss of episode 7 = 21.552605\n","Loss of episode 8 = 21.543865\n","Loss of episode 9 = 21.538795\n","Loss of episode 10 = 21.534088\n","Loss of episode 11 = 21.534536\n","Loss of episode 12 = 21.532272\n","Loss of episode 13 = 21.530256\n","Loss of episode 14 = 21.529053\n","Loss of episode 15 = 21.52822\n","Loss of episode 16 = 21.52683\n","Loss of episode 17 = 21.525959\n","Loss of episode 18 = 21.523664\n","Loss of episode 19 = 21.519955\n","Loss of episode 20 = 21.51924\n","Loss of episode 21 = 21.517612\n","Loss of episode 22 = 21.51827\n","Loss of episode 23 = 21.516994\n","Loss of episode 24 = 21.515821\n","Loss of episode 25 = 21.51575\n","Loss of episode 26 = 21.515625\n","Loss of episode 27 = 21.514105\n","Loss of episode 28 = 21.513083\n","Loss of episode 29 = 21.513426\n","Loss of episode 30 = 21.513037\n","Loss of episode 31 = 21.512512\n","Loss of episode 32 = 21.512001\n","Loss of episode 33 = 21.512997\n","Loss of episode 34 = 21.513351\n","Loss of episode 35 = 21.513592\n","Loss of episode 36 = 21.512457\n","Loss of episode 37 = 21.51226\n","Loss of episode 38 = 21.511044\n","Loss of episode 39 = 21.510693\n","Loss of episode 40 = 21.51068\n","Loss of episode 41 = 21.509695\n","Loss of episode 42 = 21.510145\n","Loss of episode 43 = 21.508581\n","Loss of episode 44 = 21.509153\n","Loss of episode 45 = 21.508705\n","Loss of episode 46 = 21.508442\n","Loss of episode 47 = 21.507498\n","Loss of episode 48 = 21.505451\n","Loss of episode 49 = 21.502798\n","Loss of episode 50 = 21.494888\n","Loss of episode 51 = 21.490868\n","Loss of episode 52 = 21.486496\n","Loss of episode 53 = 21.480522\n","Loss of episode 54 = 21.477087\n","Loss of episode 55 = 21.473486\n","Loss of episode 56 = 21.467693\n","Loss of episode 57 = 21.464363\n","Loss of episode 58 = 21.462738\n","Loss of episode 59 = 21.457134\n","Loss of episode 60 = 21.458723\n","Loss of episode 61 = 21.456274\n","Loss of episode 62 = 21.456005\n","Loss of episode 63 = 21.453466\n","Loss of episode 64 = 21.453194\n","Loss of episode 65 = 21.452269\n","Loss of episode 66 = 21.45058\n","Loss of episode 67 = 21.450743\n","Loss of episode 68 = 21.449734\n","Loss of episode 69 = 21.449377\n","Loss of episode 70 = 21.44661\n","Loss of episode 71 = 21.44843\n","Loss of episode 72 = 21.44777\n","Loss of episode 73 = 21.44754\n","Loss of episode 74 = 21.44471\n","Loss of episode 75 = 21.44506\n","Loss of episode 76 = 21.442402\n","Loss of episode 77 = 21.44563\n","Loss of episode 78 = 21.44293\n","Loss of episode 79 = 21.447826\n","Loss of episode 80 = 21.446491\n","Loss of episode 81 = 21.444067\n","Loss of episode 82 = 21.442284\n","Loss of episode 83 = 21.437763\n","Loss of episode 84 = 21.435848\n","Loss of episode 85 = 21.441711\n","Loss of episode 86 = 21.442696\n","Loss of episode 87 = 21.438831\n","Loss of episode 88 = 21.435064\n","Loss of episode 89 = 21.431812\n","Loss of episode 90 = 21.431541\n","Loss of episode 91 = 21.42958\n","Loss of episode 92 = 21.430424\n","Loss of episode 93 = 21.42834\n","Loss of episode 94 = 21.42577\n","Loss of episode 95 = 21.425865\n","Loss of episode 96 = 21.423819\n","Loss of episode 97 = 21.421272\n","Loss of episode 98 = 21.42326\n","Loss of episode 99 = 21.41917\n","Loss of episode 100 = 21.41784\n","Loss of episode 101 = 21.417042\n","Loss of episode 102 = 21.41325\n","Loss of episode 103 = 21.410488\n","Loss of episode 104 = 21.40766\n","Loss of episode 105 = 21.406012\n","Loss of episode 106 = 21.405767\n","Loss of episode 107 = 21.397\n","Loss of episode 108 = 21.391083\n","Loss of episode 109 = 21.385944\n","Loss of episode 110 = 21.381353\n","Loss of episode 111 = 21.37653\n","Loss of episode 112 = 21.371662\n","Loss of episode 113 = 21.369102\n","Loss of episode 114 = 21.36832\n","Loss of episode 115 = 21.375061\n","Loss of episode 116 = 21.372282\n","Loss of episode 117 = 21.362091\n","Loss of episode 118 = 21.347736\n","Loss of episode 119 = 21.336472\n","Loss of episode 120 = 21.329086\n","Loss of episode 121 = 21.32107\n","Loss of episode 122 = 21.308517\n","Loss of episode 123 = 21.297586\n","Loss of episode 124 = 21.296326\n","Loss of episode 125 = 21.291153\n","Loss of episode 126 = 21.283878\n","Loss of episode 127 = 21.290342\n","Loss of episode 128 = 21.273777\n","Loss of episode 129 = 21.259377\n","Loss of episode 130 = 21.238464\n","Loss of episode 131 = 21.218822\n","Loss of episode 132 = 21.212017\n","Loss of episode 133 = 21.210182\n","Loss of episode 134 = 21.215014\n","Loss of episode 135 = 21.219574\n","Loss of episode 136 = 21.205334\n","Loss of episode 137 = 21.161716\n","Loss of episode 138 = 21.126568\n","Loss of episode 139 = 21.110386\n","Loss of episode 140 = 21.10494\n","Loss of episode 141 = 21.111385\n","Loss of episode 142 = 21.089745\n","Loss of episode 143 = 21.065588\n","Loss of episode 144 = 21.050045\n","Loss of episode 145 = 21.0215\n","Loss of episode 146 = 20.998459\n","Loss of episode 147 = 20.972366\n","Loss of episode 148 = 20.976492\n","Loss of episode 149 = 20.973377\n","Loss of episode 150 = 20.9209\n","Loss of episode 151 = 20.895187\n","Loss of episode 152 = 20.868538\n","Loss of episode 153 = 20.851402\n","Loss of episode 154 = 20.840094\n","Loss of episode 155 = 20.807526\n","Loss of episode 156 = 20.791546\n","Loss of episode 157 = 20.76934\n","Loss of episode 158 = 20.739746\n","Loss of episode 159 = 20.70442\n","Loss of episode 160 = 20.686018\n","Loss of episode 161 = 20.656012\n","Loss of episode 162 = 20.658314\n","Loss of episode 163 = 20.634583\n","Loss of episode 164 = 20.600094\n","Loss of episode 165 = 20.596292\n","Loss of episode 166 = 20.55702\n","Loss of episode 167 = 20.521704\n","Loss of episode 168 = 20.540216\n","Loss of episode 169 = 20.527971\n","Loss of episode 170 = 20.525024\n","Loss of episode 171 = 20.502956\n","Loss of episode 172 = 20.469038\n","Loss of episode 173 = 20.436176\n","Loss of episode 174 = 20.410769\n","Loss of episode 175 = 20.38194\n","Loss of episode 176 = 20.372128\n","Loss of episode 177 = 20.386177\n","Loss of episode 178 = 20.396183\n","Loss of episode 179 = 20.387407\n","Loss of episode 180 = 20.377426\n","Loss of episode 181 = 20.321262\n","Loss of episode 182 = 20.26983\n","Loss of episode 183 = 20.236702\n","Loss of episode 184 = 20.201725\n","Loss of episode 185 = 20.17842\n","Loss of episode 186 = 20.176455\n","Loss of episode 187 = 20.137909\n","Loss of episode 188 = 20.157633\n","Loss of episode 189 = 20.157623\n","Loss of episode 190 = 20.150282\n","Loss of episode 191 = 20.14952\n","Loss of episode 192 = 20.178005\n","Loss of episode 193 = 20.204971\n","Loss of episode 194 = 20.193787\n","Loss of episode 195 = 20.153572\n","Loss of episode 196 = 20.12751\n","Loss of episode 197 = 20.100212\n","Loss of episode 198 = 20.08043\n","Loss of episode 199 = 20.036934\n","Loss of episode 200 = 20.017876\n","Loss of episode 201 = 20.011236\n","Loss of episode 202 = 20.005789\n","Loss of episode 203 = 20.008919\n","Loss of episode 204 = 20.01611\n","Loss of episode 205 = 20.010656\n","Loss of episode 206 = 20.022427\n","Loss of episode 207 = 20.055891\n","Loss of episode 208 = 20.036125\n","Loss of episode 209 = 19.997831\n","Loss of episode 210 = 19.990078\n","Loss of episode 211 = 19.962873\n","Loss of episode 212 = 19.93319\n","Loss of episode 213 = 19.911827\n","Loss of episode 214 = 19.900839\n","Loss of episode 215 = 19.88296\n","Loss of episode 216 = 19.866844\n","Loss of episode 217 = 19.863483\n","Loss of episode 218 = 19.856693\n","Loss of episode 219 = 19.848026\n","Loss of episode 220 = 19.839743\n","Loss of episode 221 = 19.839516\n","Loss of episode 222 = 19.838234\n","Loss of episode 223 = 19.832266\n","Loss of episode 224 = 19.840675\n","Loss of episode 225 = 19.851324\n","Loss of episode 226 = 19.860184\n","Loss of episode 227 = 19.849737\n","Loss of episode 228 = 19.835533\n","Loss of episode 229 = 19.844704\n","Loss of episode 230 = 19.83048\n","Loss of episode 231 = 19.814754\n","Loss of episode 232 = 19.826378\n","Loss of episode 233 = 19.812958\n","Loss of episode 234 = 19.801683\n","Loss of episode 235 = 19.793058\n","Loss of episode 236 = 19.814732\n","Loss of episode 237 = 19.813454\n","Loss of episode 238 = 19.796791\n","Loss of episode 239 = 19.80062\n","Loss of episode 240 = 19.792854\n","Loss of episode 241 = 19.786547\n","Loss of episode 242 = 19.796047\n","Loss of episode 243 = 19.796003\n","Loss of episode 244 = 19.801403\n","Loss of episode 245 = 19.79514\n","Loss of episode 246 = 19.782604\n","Loss of episode 247 = 19.775965\n","Loss of episode 248 = 19.778255\n","Loss of episode 249 = 19.778305\n","Loss of episode 250 = 19.782703\n","Loss of episode 251 = 19.769283\n","Loss of episode 252 = 19.770397\n","Loss of episode 253 = 19.767952\n","Loss of episode 254 = 19.75405\n","Loss of episode 255 = 19.761604\n","Loss of episode 256 = 19.76012\n","Loss of episode 257 = 19.754772\n","Loss of episode 258 = 19.748846\n","Loss of episode 259 = 19.743217\n","Loss of episode 260 = 19.731598\n","Loss of episode 261 = 19.732687\n","Loss of episode 262 = 19.737352\n","Loss of episode 263 = 19.733479\n","Loss of episode 264 = 19.739216\n","Loss of episode 265 = 19.723698\n","Loss of episode 266 = 19.721588\n","Loss of episode 267 = 19.728561\n","Loss of episode 268 = 19.713377\n","Loss of episode 269 = 19.718307\n","Loss of episode 270 = 19.711643\n","Loss of episode 271 = 19.71024\n","Loss of episode 272 = 19.708433\n","Loss of episode 273 = 19.700941\n","Loss of episode 274 = 19.703196\n","Loss of episode 275 = 19.704573\n","Loss of episode 276 = 19.703981\n","Loss of episode 277 = 19.696405\n","Loss of episode 278 = 19.700579\n","Loss of episode 279 = 19.70793\n","Loss of episode 280 = 19.703583\n","Loss of episode 281 = 19.704123\n","Loss of episode 282 = 19.69844\n","Loss of episode 283 = 19.696144\n","Loss of episode 284 = 19.69364\n","Loss of episode 285 = 19.687714\n","Loss of episode 286 = 19.708574\n","Loss of episode 287 = 19.714272\n","Loss of episode 288 = 19.707\n","Loss of episode 289 = 19.699327\n","Loss of episode 290 = 19.696518\n","Loss of episode 291 = 19.692007\n","Loss of episode 292 = 19.691816\n","Loss of episode 293 = 19.683165\n","Loss of episode 294 = 19.6833\n","Loss of episode 295 = 19.676855\n","Loss of episode 296 = 19.673624\n","Loss of episode 297 = 19.67702\n","Loss of episode 298 = 19.681133\n","Loss of episode 299 = 19.674747\n","Loss of episode 300 = 19.678299\n","Loss of episode 301 = 19.67575\n","Loss of episode 302 = 19.66998\n","Loss of episode 303 = 19.671135\n","Loss of episode 304 = 19.66879\n","Loss of episode 305 = 19.66874\n","Loss of episode 306 = 19.676163\n","Loss of episode 307 = 19.6766\n","Loss of episode 308 = 19.680721\n","Loss of episode 309 = 19.675499\n","Loss of episode 310 = 19.67622\n","Loss of episode 311 = 19.67261\n","Loss of episode 312 = 19.667107\n","Loss of episode 313 = 19.661345\n","Loss of episode 314 = 19.660023\n","Loss of episode 315 = 19.663185\n","Loss of episode 316 = 19.670996\n","Loss of episode 317 = 19.668453\n","Loss of episode 318 = 19.659122\n","Loss of episode 319 = 19.658628\n","Loss of episode 320 = 19.66132\n","Loss of episode 321 = 19.659513\n","Loss of episode 322 = 19.657791\n","Loss of episode 323 = 19.650948\n","Loss of episode 324 = 19.649261\n","Loss of episode 325 = 19.654276\n","Loss of episode 326 = 19.656368\n","Loss of episode 327 = 19.653326\n","Loss of episode 328 = 19.650597\n","Loss of episode 329 = 19.651413\n","Loss of episode 330 = 19.646568\n","Loss of episode 331 = 19.64881\n","Loss of episode 332 = 19.650894\n","Loss of episode 333 = 19.651455\n","Loss of episode 334 = 19.647741\n","Loss of episode 335 = 19.649107\n","Loss of episode 336 = 19.655746\n","Loss of episode 337 = 19.64976\n","Loss of episode 338 = 19.657354\n","Loss of episode 339 = 19.667145\n","Loss of episode 340 = 19.666058\n","Loss of episode 341 = 19.656345\n","Loss of episode 342 = 19.655762\n","Loss of episode 343 = 19.65585\n","Loss of episode 344 = 19.652859\n","Loss of episode 345 = 19.651154\n","Loss of episode 346 = 19.645512\n","Loss of episode 347 = 19.651485\n","Loss of episode 348 = 19.671215\n","Loss of episode 349 = 19.666275\n","Loss of episode 350 = 19.663162\n","Loss of episode 351 = 19.662148\n","Loss of episode 352 = 19.652157\n","Loss of episode 353 = 19.664028\n","Loss of episode 354 = 19.66642\n","Loss of episode 355 = 19.664082\n","Loss of episode 356 = 19.649368\n","Loss of episode 357 = 19.64235\n","Loss of episode 358 = 19.640455\n","Loss of episode 359 = 19.642357\n","Loss of episode 360 = 19.644228\n","Loss of episode 361 = 19.641336\n","Loss of episode 362 = 19.638134\n","Loss of episode 363 = 19.640793\n","Loss of episode 364 = 19.63432\n","Loss of episode 365 = 19.63338\n","Loss of episode 366 = 19.632038\n","Loss of episode 367 = 19.63763\n","Loss of episode 368 = 19.635502\n","Loss of episode 369 = 19.635141\n","Loss of episode 370 = 19.624992\n","Loss of episode 371 = 19.629444\n","Loss of episode 372 = 19.632452\n","Loss of episode 373 = 19.627472\n","Loss of episode 374 = 19.626896\n","Loss of episode 375 = 19.633682\n","Loss of episode 376 = 19.635311\n","Loss of episode 377 = 19.630947\n","Loss of episode 378 = 19.624462\n","Loss of episode 379 = 19.633\n","Loss of episode 380 = 19.628635\n","Loss of episode 381 = 19.628859\n","Loss of episode 382 = 19.634758\n","Loss of episode 383 = 19.66314\n","Loss of episode 384 = 19.661457\n","Loss of episode 385 = 19.658281\n","Loss of episode 386 = 19.650814\n","Loss of episode 387 = 19.64266\n","Loss of episode 388 = 19.665861\n","Loss of episode 389 = 19.66994\n","Loss of episode 390 = 19.661839\n","Loss of episode 391 = 19.658895\n","Loss of episode 392 = 19.656433\n","Loss of episode 393 = 19.656258\n","Loss of episode 394 = 19.647053\n","Loss of episode 395 = 19.636204\n","Loss of episode 396 = 19.629946\n","Loss of episode 397 = 19.620964\n","Loss of episode 398 = 19.625576\n","Loss of episode 399 = 19.619295\n","Loss of episode 400 = 19.623642\n","Loss of episode 401 = 19.620003\n","Loss of episode 402 = 19.616383\n","Loss of episode 403 = 19.616608\n","Loss of episode 404 = 19.617493\n","Loss of episode 405 = 19.620087\n","Loss of episode 406 = 19.614048\n","Loss of episode 407 = 19.621677\n","Loss of episode 408 = 19.620518\n","Loss of episode 409 = 19.610725\n","Loss of episode 410 = 19.61699\n","Loss of episode 411 = 19.611528\n","Loss of episode 412 = 19.614487\n","Loss of episode 413 = 19.60709\n","Loss of episode 414 = 19.605038\n","Loss of episode 415 = 19.607824\n","Loss of episode 416 = 19.609257\n","Loss of episode 417 = 19.604267\n","Loss of episode 418 = 19.609653\n","Loss of episode 419 = 19.603834\n","Loss of episode 420 = 19.608625\n","Loss of episode 421 = 19.60492\n","Loss of episode 422 = 19.615513\n","Loss of episode 423 = 19.60288\n","Loss of episode 424 = 19.607244\n","Loss of episode 425 = 19.609415\n","Loss of episode 426 = 19.609299\n","Loss of episode 427 = 19.612946\n","Loss of episode 428 = 19.606274\n","Loss of episode 429 = 19.608898\n","Loss of episode 430 = 19.604887\n","Loss of episode 431 = 19.615763\n","Loss of episode 432 = 19.605896\n","Loss of episode 433 = 19.60806\n","Loss of episode 434 = 19.607395\n","Loss of episode 435 = 19.603481\n","Loss of episode 436 = 19.601955\n","Loss of episode 437 = 19.60535\n","Loss of episode 438 = 19.612581\n","Loss of episode 439 = 19.616442\n","Loss of episode 440 = 19.607428\n","Loss of episode 441 = 19.611328\n","Loss of episode 442 = 19.610102\n","Loss of episode 443 = 19.607204\n","Loss of episode 444 = 19.60001\n","Loss of episode 445 = 19.60333\n","Loss of episode 446 = 19.605782\n","Loss of episode 447 = 19.601334\n","Loss of episode 448 = 19.599836\n","Loss of episode 449 = 19.602755\n","Loss of episode 450 = 19.606613\n","Loss of episode 451 = 19.601585\n","Loss of episode 452 = 19.601635\n","Loss of episode 453 = 19.601501\n","Loss of episode 454 = 19.595673\n","Loss of episode 455 = 19.596617\n","Loss of episode 456 = 19.59805\n","Loss of episode 457 = 19.595978\n","Loss of episode 458 = 19.592426\n","Loss of episode 459 = 19.604242\n","Loss of episode 460 = 19.601912\n","Loss of episode 461 = 19.609772\n","Loss of episode 462 = 19.612532\n","Loss of episode 463 = 19.612686\n","Loss of episode 464 = 19.61377\n","Loss of episode 465 = 19.601364\n","Loss of episode 466 = 19.606552\n","Loss of episode 467 = 19.601963\n","Loss of episode 468 = 19.60235\n","Loss of episode 469 = 19.6015\n","Loss of episode 470 = 19.595852\n","Loss of episode 471 = 19.603867\n","Loss of episode 472 = 19.601223\n","Loss of episode 473 = 19.603867\n","Loss of episode 474 = 19.605453\n","Loss of episode 475 = 19.601646\n","Loss of episode 476 = 19.609425\n","Loss of episode 477 = 19.607141\n","Loss of episode 478 = 19.615309\n","Loss of episode 479 = 19.6193\n","Loss of episode 480 = 19.61306\n","Loss of episode 481 = 19.606636\n","Loss of episode 482 = 19.608479\n","Loss of episode 483 = 19.59999\n","Loss of episode 484 = 19.605621\n","Loss of episode 485 = 19.60035\n","Loss of episode 486 = 19.599554\n","Loss of episode 487 = 19.59812\n","Loss of episode 488 = 19.596586\n","Loss of episode 489 = 19.596289\n","Loss of episode 490 = 19.597614\n","Loss of episode 491 = 19.602688\n","Loss of episode 492 = 19.591444\n","Loss of episode 493 = 19.59734\n","Loss of episode 494 = 19.593124\n","Loss of episode 495 = 19.602468\n","Loss of episode 496 = 19.6249\n","Loss of episode 497 = 19.629066\n","Loss of episode 498 = 19.61571\n","Loss of episode 499 = 19.625189\n","Loss of episode 500 = 19.616804\n","Loss of episode 501 = 19.61005\n","Loss of episode 502 = 19.60022\n","Loss of episode 503 = 19.59917\n","Loss of episode 504 = 19.59537\n","Loss of episode 505 = 19.593916\n","Loss of episode 506 = 19.59883\n","Loss of episode 507 = 19.592587\n","Loss of episode 508 = 19.590736\n","Loss of episode 509 = 19.593725\n","Loss of episode 510 = 19.594564\n","Loss of episode 511 = 19.594297\n","Loss of episode 512 = 19.589785\n","Loss of episode 513 = 19.588797\n","Loss of episode 514 = 19.588654\n","Loss of episode 515 = 19.591011\n","Loss of episode 516 = 19.589184\n","Loss of episode 517 = 19.582573\n","Loss of episode 518 = 19.584827\n","Loss of episode 519 = 19.589369\n","Loss of episode 520 = 19.588871\n","Loss of episode 521 = 19.596355\n","Loss of episode 522 = 19.599556\n","Loss of episode 523 = 19.597023\n","Loss of episode 524 = 19.59309\n","Loss of episode 525 = 19.58916\n","Loss of episode 526 = 19.594402\n","Loss of episode 527 = 19.589256\n","Loss of episode 528 = 19.595821\n","Loss of episode 529 = 19.589745\n","Loss of episode 530 = 19.586956\n","Loss of episode 531 = 19.590572\n","Loss of episode 532 = 19.585201\n","Loss of episode 533 = 19.587105\n","Loss of episode 534 = 19.59629\n","Loss of episode 535 = 19.596846\n","Loss of episode 536 = 19.591276\n","Loss of episode 537 = 19.593216\n","Loss of episode 538 = 19.593975\n","Loss of episode 539 = 19.588469\n","Loss of episode 540 = 19.583607\n","Loss of episode 541 = 19.58186\n","Loss of episode 542 = 19.581272\n","Loss of episode 543 = 19.58535\n","Loss of episode 544 = 19.58042\n","Loss of episode 545 = 19.579363\n","Loss of episode 546 = 19.583965\n","Loss of episode 547 = 19.582157\n","Loss of episode 548 = 19.58638\n","Loss of episode 549 = 19.588514\n","Loss of episode 550 = 19.587477\n","Loss of episode 551 = 19.589338\n","Loss of episode 552 = 19.581865\n","Loss of episode 553 = 19.586315\n","Loss of episode 554 = 19.583294\n","Loss of episode 555 = 19.583858\n","Loss of episode 556 = 19.580847\n","Loss of episode 557 = 19.578896\n","Loss of episode 558 = 19.578522\n","Loss of episode 559 = 19.580204\n","Loss of episode 560 = 19.575926\n","Loss of episode 561 = 19.581194\n","Loss of episode 562 = 19.577148\n","Loss of episode 563 = 19.581358\n","Loss of episode 564 = 19.579737\n","Loss of episode 565 = 19.582218\n","Loss of episode 566 = 19.579386\n","Loss of episode 567 = 19.579418\n","Loss of episode 568 = 19.579514\n","Loss of episode 569 = 19.577642\n","Loss of episode 570 = 19.575\n","Loss of episode 571 = 19.57518\n","Loss of episode 572 = 19.570278\n","Loss of episode 573 = 19.578583\n","Loss of episode 574 = 19.577488\n","Loss of episode 575 = 19.574324\n","Loss of episode 576 = 19.571445\n","Loss of episode 577 = 19.576052\n","Loss of episode 578 = 19.57588\n","Loss of episode 579 = 19.57321\n","Loss of episode 580 = 19.576786\n","Loss of episode 581 = 19.576496\n","Loss of episode 582 = 19.574415\n","Loss of episode 583 = 19.579042\n","Loss of episode 584 = 19.579659\n","Loss of episode 585 = 19.57638\n","Loss of episode 586 = 19.575674\n","Loss of episode 587 = 19.576513\n","Loss of episode 588 = 19.575897\n","Loss of episode 589 = 19.573542\n","Loss of episode 590 = 19.576288\n","Loss of episode 591 = 19.578886\n","Loss of episode 592 = 19.582172\n","Loss of episode 593 = 19.576862\n","Loss of episode 594 = 19.575733\n","Loss of episode 595 = 19.577644\n","Loss of episode 596 = 19.577023\n","Loss of episode 597 = 19.575333\n","Loss of episode 598 = 19.576057\n","Loss of episode 599 = 19.575436\n","Loss of episode 600 = 19.57761\n","Loss of episode 601 = 19.576664\n","Loss of episode 602 = 19.571224\n","Loss of episode 603 = 19.572105\n","Loss of episode 604 = 19.569786\n","Loss of episode 605 = 19.571209\n","Loss of episode 606 = 19.572182\n","Loss of episode 607 = 19.574585\n","Loss of episode 608 = 19.57111\n","Loss of episode 609 = 19.574461\n","Loss of episode 610 = 19.573917\n","Loss of episode 611 = 19.5697\n","Loss of episode 612 = 19.568045\n","Loss of episode 613 = 19.57152\n","Loss of episode 614 = 19.567852\n","Loss of episode 615 = 19.573614\n","Loss of episode 616 = 19.574726\n","Loss of episode 617 = 19.580444\n","Loss of episode 618 = 19.5777\n","Loss of episode 619 = 19.57893\n","Loss of episode 620 = 19.57494\n","Loss of episode 621 = 19.577396\n","Loss of episode 622 = 19.577728\n","Loss of episode 623 = 19.572466\n","Loss of episode 624 = 19.578037\n","Loss of episode 625 = 19.575668\n","Loss of episode 626 = 19.574436\n","Loss of episode 627 = 19.573105\n","Loss of episode 628 = 19.570662\n","Loss of episode 629 = 19.573627\n","Loss of episode 630 = 19.569534\n","Loss of episode 631 = 19.582916\n","Loss of episode 632 = 19.591383\n","Loss of episode 633 = 19.586952\n","Loss of episode 634 = 19.591534\n","Loss of episode 635 = 19.586678\n","Loss of episode 636 = 19.58769\n","Loss of episode 637 = 19.59116\n","Loss of episode 638 = 19.5881\n","Loss of episode 639 = 19.584843\n","Loss of episode 640 = 19.57896\n","Loss of episode 641 = 19.584534\n","Loss of episode 642 = 19.590034\n","Loss of episode 643 = 19.59277\n","Loss of episode 644 = 19.59019\n","Loss of episode 645 = 19.587341\n","Loss of episode 646 = 19.585938\n","Loss of episode 647 = 19.587227\n","Loss of episode 648 = 19.585297\n","Loss of episode 649 = 19.584282\n","Loss of episode 650 = 19.580065\n","Loss of episode 651 = 19.576454\n","Loss of episode 652 = 19.571499\n","Loss of episode 653 = 19.57497\n","Loss of episode 654 = 19.57306\n","Loss of episode 655 = 19.570545\n","Loss of episode 656 = 19.575064\n","Loss of episode 657 = 19.571568\n","Loss of episode 658 = 19.566841\n","Loss of episode 659 = 19.569876\n","Loss of episode 660 = 19.563786\n","Loss of episode 661 = 19.566936\n","Loss of episode 662 = 19.568016\n","Loss of episode 663 = 19.565762\n","Loss of episode 664 = 19.562649\n","Loss of episode 665 = 19.564167\n","Loss of episode 666 = 19.567335\n","Loss of episode 667 = 19.567106\n","Loss of episode 668 = 19.571064\n","Loss of episode 669 = 19.566526\n","Loss of episode 670 = 19.565096\n","Loss of episode 671 = 19.560783\n","Loss of episode 672 = 19.56174\n","Loss of episode 673 = 19.566841\n","Loss of episode 674 = 19.567867\n","Loss of episode 675 = 19.565184\n","Loss of episode 676 = 19.566559\n","Loss of episode 677 = 19.567566\n","Loss of episode 678 = 19.569344\n","Loss of episode 679 = 19.565796\n","Loss of episode 680 = 19.56011\n","Loss of episode 681 = 19.56199\n","Loss of episode 682 = 19.558609\n","Loss of episode 683 = 19.565964\n","Loss of episode 684 = 19.56871\n","Loss of episode 685 = 19.569221\n","Loss of episode 686 = 19.570494\n","Loss of episode 687 = 19.567785\n","Loss of episode 688 = 19.566864\n","Loss of episode 689 = 19.56437\n","Loss of episode 690 = 19.568861\n","Loss of episode 691 = 19.561016\n","Loss of episode 692 = 19.564259\n","Loss of episode 693 = 19.564087\n","Loss of episode 694 = 19.56209\n","Loss of episode 695 = 19.561586\n","Loss of episode 696 = 19.56282\n","Loss of episode 697 = 19.567707\n","Loss of episode 698 = 19.576118\n","Loss of episode 699 = 19.579811\n","Loss of episode 700 = 19.587496\n","Loss of episode 701 = 19.58506\n","Loss of episode 702 = 19.578503\n","Loss of episode 703 = 19.577747\n","Loss of episode 704 = 19.57468\n","Loss of episode 705 = 19.57533\n","Loss of episode 706 = 19.57376\n","Loss of episode 707 = 19.58512\n","Loss of episode 708 = 19.591553\n","Loss of episode 709 = 19.59077\n","Loss of episode 710 = 19.582108\n","Loss of episode 711 = 19.582897\n","Loss of episode 712 = 19.577076\n","Loss of episode 713 = 19.581997\n","Loss of episode 714 = 19.587444\n","Loss of episode 715 = 19.583881\n","Loss of episode 716 = 19.579823\n","Loss of episode 717 = 19.580082\n","Loss of episode 718 = 19.576946\n","Loss of episode 719 = 19.575876\n","Loss of episode 720 = 19.574802\n","Loss of episode 721 = 19.56826\n","Loss of episode 722 = 19.575266\n","Loss of episode 723 = 19.572521\n","Loss of episode 724 = 19.57859\n","Loss of episode 725 = 19.576015\n","Loss of episode 726 = 19.576866\n","Loss of episode 727 = 19.57993\n","Loss of episode 728 = 19.571796\n","Loss of episode 729 = 19.569744\n","Loss of episode 730 = 19.567623\n","Loss of episode 731 = 19.564808\n","Loss of episode 732 = 19.564404\n","Loss of episode 733 = 19.568792\n","Loss of episode 734 = 19.560833\n","Loss of episode 735 = 19.581438\n","Loss of episode 736 = 19.588459\n","Loss of episode 737 = 19.582678\n","Loss of episode 738 = 19.581413\n","Loss of episode 739 = 19.5772\n","Loss of episode 740 = 19.574924\n","Loss of episode 741 = 19.571867\n","Loss of episode 742 = 19.586908\n","Loss of episode 743 = 19.58545\n","Loss of episode 744 = 19.586182\n","Loss of episode 745 = 19.577578\n","Loss of episode 746 = 19.578657\n","Loss of episode 747 = 19.574814\n","Loss of episode 748 = 19.573421\n","Loss of episode 749 = 19.564297\n","Loss of episode 750 = 19.565819\n","Loss of episode 751 = 19.562607\n","Loss of episode 752 = 19.561302\n","Loss of episode 753 = 19.566154\n","Loss of episode 754 = 19.566717\n","Loss of episode 755 = 19.561848\n","Loss of episode 756 = 19.566917\n","Loss of episode 757 = 19.561058\n","Loss of episode 758 = 19.554916\n","Loss of episode 759 = 19.557886\n","Loss of episode 760 = 19.551224\n","Loss of episode 761 = 19.556042\n","Loss of episode 762 = 19.559502\n","Loss of episode 763 = 19.560024\n","Loss of episode 764 = 19.558447\n","Loss of episode 765 = 19.56568\n","Loss of episode 766 = 19.563097\n","Loss of episode 767 = 19.565054\n","Loss of episode 768 = 19.562391\n","Loss of episode 769 = 19.564528\n","Loss of episode 770 = 19.563538\n","Loss of episode 771 = 19.561222\n","Loss of episode 772 = 19.560833\n","Loss of episode 773 = 19.561384\n","Loss of episode 774 = 19.556782\n","Loss of episode 775 = 19.555946\n","Loss of episode 776 = 19.555183\n","Loss of episode 777 = 19.55524\n","Loss of episode 778 = 19.556625\n","Loss of episode 779 = 19.553967\n","Loss of episode 780 = 19.555729\n","Loss of episode 781 = 19.555614\n","Loss of episode 782 = 19.554394\n","Loss of episode 783 = 19.561138\n","Loss of episode 784 = 19.56759\n","Loss of episode 785 = 19.559443\n","Loss of episode 786 = 19.559767\n","Loss of episode 787 = 19.556515\n","Loss of episode 788 = 19.561138\n","Loss of episode 789 = 19.561539\n","Loss of episode 790 = 19.56149\n","Loss of episode 791 = 19.556572\n","Loss of episode 792 = 19.558342\n","Loss of episode 793 = 19.557518\n","Loss of episode 794 = 19.561209\n","Loss of episode 795 = 19.557533\n","Loss of episode 796 = 19.556023\n","Loss of episode 797 = 19.556332\n","Loss of episode 798 = 19.55517\n","Loss of episode 799 = 19.556362\n","\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# feature_size, hidden_size, num_layers, output_size, (number_heads)\n","model = MTORL(state_feature,512,2,3).to(device)\n","num_epoch = 800     # 800\n","Lambda = 1.5        # 1.5\n","Mu = 0.08           # 0.08\n","\n","# optimizer = torch.optim.SGD(net.parameters(),lr=0.5)\n","# optimizer = torch.optim.Adam(gru.parameters(),lr=0.001)\n","optimizer = torch.optim.AdamW(model.parameters(),lr=0.001)\n","\n","# loss_func = torch.nn.MSELoss()            #MSE for regression\n","loss_func = torch.nn.CrossEntropyLoss()     #CE for classification\n","Loss=[]\n","\n","for t in range(num_epoch):\n","    aver_loss = 0\n","    model.train()\n","    for batch, data in enumerate(data_train_loader):\n","        x, y ,z = data\n","        action_prediction, click_prediction = model(x)\n","        # print(action_prediction.shape)\n","        action_prediction = torch.transpose(action_prediction, dim0=0, dim1=1)\n","        click_prediction = torch.transpose(click_prediction, dim0=0, dim1=1)\n","        Entropy = torch.mean(shannon_entropy(action_prediction))\n","        # loss = loss_func(action_prediction,y) - Lambda * Entropy #//2-loss\n","        loss = loss_func(action_prediction,y) + Mu * loss_func(click_prediction,z) \\\n","        - Lambda * Entropy \n","        optimizer.zero_grad() \n","        loss.backward()    \n","        optimizer.step() \n","        aver_loss += loss\n","    aver_loss /= batch\n","    aver_loss=aver_loss.cpu().detach().numpy()\n","    print('Loss of episode %s ='%t,aver_loss)\n","    Loss.append(aver_loss)\n","plt.plot(Loss,color='r')\n","print()"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690978204313,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"lxxPbtYyOBS6","outputId":"d2b4bdae-3248-45d5-9a2c-25bb1d8fc299"},"outputs":[{"name":"stdout","output_type":"stream","text":["6680084\n"]}],"source":["device = torch.device(\"cuda\")\n","model_test = MTORL(state_feature,512,2,3).to(device)\n","total = sum([param.nelement() for param in model_test.parameters()])\n","print(total)\n","# gru_test"]},{"cell_type":"markdown","metadata":{"id":"Qiqn4DAJOBS7"},"source":["## Training accuracy"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690978204313,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"BUoPP1KjOBS7","outputId":"4ec4f842-dd61-42d5-e7f4-01bea8ab6944"},"outputs":[{"name":"stdout","output_type":"stream","text":["acc = 0.9862968750000001\n"]}],"source":["Truth = []\n","Pred = []\n","aver_loss = 0\n","total_acc = 0\n","Entropy = 0\n","for batch, data in enumerate(data_train_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    x, y, z = data\n","    # print(x)\n","    # prediction =net(x)\n","    test, _ = model(x)\n","    test = torch.transpose(test, dim0=0, dim1=1)\n","    # print(test)\n","    # print(y)\n","    # print(shannon_entropy(test).shape)\n","    aver_entropy = sum(torch.mean(shannon_entropy(test),dim=1))\n","    # print(sum(aver_entropy))\n","    Entropy += aver_entropy\n","\n","    a = torch.argmax(test,dim = 2).cpu().data.numpy()\n","    b = torch.argmax(y,dim = 2).cpu().data.numpy()\n","    # print(a)\n","    # print(b)\n","    # print(sum(a==b))\n","    aver_acc = sum(sum(a==b))/length\n","    # print(aver_acc)\n","    total_acc += aver_acc\n","    # print(total_acc)\n","# print(batch)\n","print('acc =', total_acc/(0.8*max_number_traj))\n","# print('Entropy = ', Entropy/(0.8*max_number_traj))"]},{"cell_type":"markdown","metadata":{},"source":["## Validation accuracy (Hyperparameter tuning)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Action acc = 0.6728749999999997\n","CTR acc = 0.7927499999999995\n","Entropy =  tensor(0.9843, grad_fn=<DivBackward0>)\n"]}],"source":["from sklearn.metrics import roc_curve, auc\n","\n","\n","Loss=[]\n","Truth = []\n","Pred = []\n","aver_loss = 0\n","total_acc = 0\n","total_ctr = 0\n","total_auc = 0\n","Entropy = 0\n","num = 0\n","\n","# Click_pred = np.zeros((int(0.2*max_number_traj),length))\n","# Click_true = np.zeros((int(0.2*max_number_traj),length))\n","\n","for batch, data in enumerate(data_val_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    model.eval()\n","    x, y, z = data\n","    test, click = model(x)\n","    click_pred = torch.squeeze(click).cpu().data.numpy()\n","    click_true = torch.squeeze(z).cpu().data.numpy()\n","    click_pred[click_pred<1/2] = 0\n","    click_pred[click_pred>=1/2] = 1\n","    # Click_pred[batch] = click_pred\n","    # Click_true[batch] = click_true\n","\n","    p = test[:,0].cpu()\n","    aver_entropy = torch.mean(shannon_entropy(p))\n","    # print('aver_entropy =', aver_entropy)\n","    Entropy += aver_entropy\n","    # print(test)\n","    a = torch.argmax(test,dim = 2).cpu().data.numpy().T[0]\n","    b = torch.argmax(y,dim = 2).cpu().data.numpy()[0]\n","    # print('-----------')\n","    # print('pred =',a.T[0])\n","    # print('true =',b[0])\n","    aver_acc = sum(a==b)/length\n","    total_acc += aver_acc\n","\n","    # print('-----------')\n","    # print('pred =', click_pred)\n","    # print('true =', click_true) \n","    aver_ctr = sum(click_pred==click_true)/length\n","    total_ctr += aver_ctr\n","\n","\n","# fpr, tpr, thresholds = roc_curve(Click_true.reshape(-1), Click_pred.reshape(-1),pos_label=1)\n","# total_auc = auc(fpr, tpr)\n","# print('CTR AUC =', total_auc)\n","print('Action acc =', total_acc/(0.1*max_number_traj))\n","print('CTR acc =', total_ctr/(0.1*max_number_traj))\n","print('Entropy = ', Entropy/(0.1*max_number_traj))\n"]},{"cell_type":"markdown","metadata":{"id":"6Hi558JBOBS8"},"source":["## Inference accuracy"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1541,"status":"ok","timestamp":1690978205851,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"dH6BC5O2OBS8","outputId":"f0746245-4215-490b-c3b9-ee67948cc407"},"outputs":[{"name":"stdout","output_type":"stream","text":["Action acc = 0.66925\n","CTR acc = 0.7863749999999999\n","Entropy =  tensor(0.9831, grad_fn=<DivBackward0>)\n"]}],"source":["from sklearn.metrics import roc_curve, auc\n","\n","\n","Loss=[]\n","Truth = []\n","Pred = []\n","aver_loss = 0\n","total_acc = 0\n","total_ctr = 0\n","total_auc = 0\n","Entropy = 0\n","num = 0\n","\n","# Click_pred = np.zeros((int(0.2*max_number_traj),length))\n","# Click_true = np.zeros((int(0.2*max_number_traj),length))\n","\n","for batch, data in enumerate(data_test_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    model.eval()\n","    x, y, z = data\n","    test, click = model(x)\n","    click_pred = torch.squeeze(click).cpu().data.numpy()\n","    click_true = torch.squeeze(z).cpu().data.numpy()\n","    click_pred[click_pred<1/2] = 0\n","    click_pred[click_pred>=1/2] = 1\n","    # Click_pred[batch] = click_pred\n","    # Click_true[batch] = click_true\n","\n","    p = test[:,0].cpu()\n","    aver_entropy = torch.mean(shannon_entropy(p))\n","    # print('aver_entropy =', aver_entropy)\n","    Entropy += aver_entropy\n","    # print(test)\n","    a = torch.argmax(test,dim = 2).cpu().data.numpy().T[0]\n","    b = torch.argmax(y,dim = 2).cpu().data.numpy()[0]\n","    # print('-----------')\n","    # print('pred =',a.T[0])\n","    # print('true =',b[0])\n","    aver_acc = sum(a==b)/length\n","    total_acc += aver_acc\n","\n","    # print('-----------')\n","    # print('pred =', click_pred)\n","    # print('true =', click_true) \n","    aver_ctr = sum(click_pred==click_true)/length\n","    total_ctr += aver_ctr\n","\n","\n","# fpr, tpr, thresholds = roc_curve(Click_true.reshape(-1), Click_pred.reshape(-1),pos_label=1)\n","# total_auc = auc(fpr, tpr)\n","# print('CTR AUC =', total_auc)\n","print('Action acc =', total_acc/(0.1*max_number_traj))\n","print('CTR acc =', total_ctr/(0.1*max_number_traj))\n","print('Entropy = ', Entropy/(0.1*max_number_traj))\n"]},{"cell_type":"markdown","metadata":{"id":"n6ecXL0POBS8"},"source":["## AUC"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":736,"status":"ok","timestamp":1690978206585,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"45g3DhluOBS8","outputId":"9e567364-e555-4ef1-be01-fdcdc6de971b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Average AUC = [0.78992309 0.7842347  0.77871874]\n"]}],"source":["from sklearn.metrics import precision_score, roc_curve, auc\n","n_classes = 3\n","# length = 8\n","aver_auc = np.zeros(n_classes)\n","total_precision = 0\n","Y_score = np.zeros(((length, int(0.1*max_number_traj), n_classes)))\n","# print(Y_score.shape)\n","Y_label = np.zeros(((length, int(0.1*max_number_traj), n_classes)))\n","for batch, data in enumerate(data_test_loader):\n","    model.eval()\n","    x, y, z = data\n","    # print(x)\n","    test, _ = model(x)\n","    y_score = test[:,0,:].cpu().data.numpy()\n","    y_label = y[0].cpu().data.numpy()\n","    # print(batch)\n","    for i in range(length):\n","        Y_score[i][batch] = y_score[i]\n","        Y_label[i][batch] = y_label[i]\n","        # Y_score[i][batch] = [1/3,1/3,1/3]\n","        # Y_label[i][batch] = y_label[i]\n","        # print(y_score[i])\n","        # print(y_label[i])\n","fpr = dict()\n","tpr = dict()\n","roc_auc = dict()\n","aver_auc = np.zeros(n_classes)\n","for t in range(length):\n","    for i in range(n_classes):\n","        fpr[i], tpr[i], _ = roc_curve(Y_label[t][:, i], Y_score[t][:, i])\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","        # aver_auc[i] += auc(fpr[i], tpr[i])\n","        aver_auc[i] += roc_auc[i]/length\n","    # print('step %s AUC ='%(t+1),roc_auc)\n","print('Average AUC =', aver_auc)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1690978206585,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"Q-5eCoFCOBS8","outputId":"1fe76151-c1a2-4ced-8acf-4a37d2df9e6e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Aver_F1 = 0.6681 Aver_Precision = 0.6691 Aver_Recall = 0.6689\n"]}],"source":["from sklearn.metrics import f1_score, precision_score, recall_score\n","\n","aver_f1 = 0\n","aver_p = 0\n","aver_r = 0\n","\n","# print(y_pred)\n","# print(y_true)\n","for i in range(length):\n","  y_true = np.argmax(Y_label[i],axis = 1)\n","  y_pred = np.argmax(Y_score[i],axis = 1)\n","  f1 = round(f1_score(y_true, y_pred, average='macro' ),4)\n","  p = round(precision_score(y_true, y_pred, average='macro'),4)\n","  r = round(recall_score(y_true, y_pred, average='macro'),4)\n","  aver_f1 += f1/length\n","  aver_p += p/length\n","  aver_r += r/length\n","print('Aver_F1 =', round(aver_f1,4), 'Aver_Precision =', round(aver_p,4), 'Aver_Recall =', round(aver_r,4))"]},{"cell_type":"markdown","metadata":{},"source":["## Budget Allocation"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["test_number = int(0.1*max_number_traj)\n","True_Click_matrix = np.zeros((test_number,length))\n","True_Action_matrix = np.zeros((test_number,length))\n","Click_pred_matrix = np.zeros((test_number,length))\n","Action_pred_matrix = np.zeros((test_number,length))\n","\n","for batch, data in enumerate(data_test_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    model.eval()\n","    x, y, z = data\n","    action_true = torch.squeeze(torch.argmax(y,dim = 2)).cpu().data.numpy()\n","    click_true = torch.squeeze(z).cpu().data.numpy()\n","    True_Click_matrix[batch] = click_true\n","    # True_Action_count += np.sum(action_true, axis = 0)\n","    \n","    action, click = model(x)\n","    action_pred = torch.squeeze(torch.argmax(action,dim = 2)).cpu().data.numpy()\n","    # print(action_true == action_pred)\n","    Action_pred_matrix[batch] = (action_true == action_pred)\n","    click_pred = torch.squeeze(click).cpu().data.numpy()\n","    Click_pred_matrix[batch] = click_pred\n","\n","# print(True_Click_matrix)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True_cost =  8000.0\n","True click number =  1428.0\n","Ture_CPC = 5.602240896358543\n","Policy_cost =  2000.0\n","Policy click number =  561.0\n","Policy_CPC = 3.5650623885918002\n"]}],"source":["choose_number = 100\n","True_cost = 0.1 * max_number_traj * length\n","True_Click_number = sum(sum(True_Click_matrix))\n","User_choose = np.zeros((test_number,length))\n","\n","print('True_cost = ', True_cost)\n","print('True click number = ', True_Click_number)\n","True_CPC = (0.1 * max_number_traj) * length / True_Click_number\n","print('Ture_CPC =',True_CPC)\n","for i in range(length):\n","    tmp = Click_pred_matrix[:,i]\n","    idx = np.argsort(tmp)\n","    # print(idx[400:])\n","    # print(tmp[idx[400:]])\n","    User_choose[:,i][idx[test_number-choose_number:]] = 1\n","\n","Cost = sum(sum(User_choose))\n","Click = sum(sum(User_choose*Action_pred_matrix*True_Click_matrix))\n","cpc = Cost/Click\n","print('Policy_cost = ', Cost)\n","print('Policy click number = ', Click) \n","print('Policy_CPC =', cpc)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
