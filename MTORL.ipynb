{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1481,"status":"ok","timestamp":1690970114664,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"SHePlV70OBS2"},"outputs":[],"source":["# if torch.cuda.is_available():\n","#     print('CUDNN VERSION:',torch.backends.cudnn.version())\n","#     print('Number CUDA Devices:',torch.cuda.device_count())\n","#     print('CUDA Device Name:',torch.cuda.get_device_name(0))\n","#     print('CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6052,"status":"ok","timestamp":1690970120708,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"kUqW9r2qOBS2","outputId":"606cfa3e-2965-43bb-9f91-818746accbc6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on the GPU\n"]}],"source":["import torch\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")  # you can continue going on here, like cuda:1 cuda:2....etc.\n","    print(\"Running on the GPU\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"Running on the CPU\")\n","# torch.cuda.device_count()"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1690973614130,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"iZBy7-2rOBS3"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","Data = pd.read_csv('kuairand_sequence.csv',index_col=0)\n","# Data = pd.read_csv('kuairand_sequence_user=8000.csv',index_col=0)\n","# data['user_id'].value_counts()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1690973619591,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"3HzbfAYVOBS3","outputId":"2e5a6756-eb74-4e81-f782-0caddeae1081"},"outputs":[],"source":["import torch\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import random\n","from torch.utils.data import Dataset, DataLoader,TensorDataset\n","\n","max_length = 100\n","num_user = len(Data['user_id'].unique())\n","state_feature = len(Data.columns) - 1\n","action_feature = 3\n","\n","uid = Data['user_id'].unique()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2991,"status":"ok","timestamp":1690973742240,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"HdF3rl_LOBS4","outputId":"9a25cd7c-7681-4521-a5f4-db7dc0f9d462"},"outputs":[],"source":["State = np.zeros(((num_user,max_length,state_feature)))\n","Action = np.zeros((num_user,max_length,action_feature))\n","Click = np.zeros((num_user,max_length))\n","\n","j = 0\n","for i in uid:\n","    state_sequence = np.array(Data[Data['user_id']==i])[:-1]\n","    # state_sequence = np.array(data[data['user_id']==i].drop(['short','mid','long'],axis=1))\n","    action_sequence = np.array(Data[Data['user_id']==i][['short','mid','long']])[1:]\n","    click_sequence = np.array(Data[Data['user_id']==i]['is_click'])[1:]\n","    # action_sequence = np.array(data[data['user_id']==i][['short','mid','long']])\n","    # print(action_sequence)\n","    state = np.pad(state_sequence,((0,max_length-len(state_sequence)),(0,0)))[:,1:]\n","    action = np.pad(action_sequence,((0,max_length-len(action_sequence)),(0,0)))\n","    click = np.pad(click_sequence,((0,max_length-len(click_sequence))))\n","    # print(click)\n","    State[j] = state\n","    Action[j] = action\n","    Click[j] = click\n","    j += 1\n","\n","# print(len(State))\n","# print(len(Action))\n","# print(len(Click))\n","# print(Click.shape)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":822,"status":"ok","timestamp":1690976535806,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"tPsLnRdtOBS4","outputId":"798634a6-86b6-4797-8422-298b81c046ca"},"outputs":[],"source":["length = 20\n","max_number_traj = 4000\n","\n","number_traj = 0\n","def Select_trajectory(X,Y,Z,number_traj):\n","    Select_states = []\n","    Select_actions = []\n","    Select_clicks = []\n","    # for s in range(number_traj):\n","    while True:\n","        i = random.randint(0,len(X)-1)\n","        select_state = X[i]\n","        select_action = Y[i]\n","        select_click = Z[i]\n","        # print(select_state)\n","        # print(select_action)\n","        for k in range(0,max_length):\n","            # print(select_episode[k])\n","            if select_state[k].any() == 0:\n","                max_episode_length = k\n","                break\n","        # print(k)\n","        # print(max_episode_length)\n","        if max_episode_length >= length + 1:\n","            j = random.randint(0,max_episode_length-length-1)\n","            select_state = select_state[j:j+length]\n","            select_action = select_action[j:j+length]\n","            select_click = select_click[j:j+length]\n","            Select_states.append(select_state)\n","            Select_actions.append(select_action)\n","            Select_clicks.append(select_click)\n","            number_traj += 1\n","        # print(select_trajectory)\n","        if number_traj == max_number_traj:\n","            break\n","    return np.array(Select_states),np.array(Select_actions),np.array(Select_clicks)\n","\n","X , Y , Z = Select_trajectory(State,Action,Click,number_traj)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1690976536378,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"P1xlyTewOBS5"},"outputs":[],"source":["np.random.seed(2023)\n","per = np.random.permutation(X.shape[0])\n","# print(per)\n","X = X[per]\n","Y = Y[per]\n","Z = Z[per]\n","# print(sum(Y))"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1690976536378,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"GVz4ycfnOBS5","outputId":"cb73aa8e-ee79-4218-bb29-653e5d037856"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["np.array([0.5,0,0]).any()"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690976536378,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"YB3Wa9SaOBS5","outputId":"c094f597-85a0-49b2-adee-00ee3d42b5d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["3200\n"]}],"source":["def split_data(State, Action, Reward, timestep, input_size, output_size):\n","\n","    # Training set\n","    train_size = int(np.round(0.8 * X.shape[0]))\n","    print(train_size)\n","    # Split training and test set\n","    x_train = X[: train_size, :].reshape(-1, timestep,input_size)\n","    y_train = Y[: train_size].reshape(-1,timestep, output_size)\n","    z_train = Z[: train_size].reshape(-1,timestep, 1)\n","\n","    x_test = X[train_size:, :].reshape(-1, timestep,input_size)\n","    y_test = Y[train_size:].reshape(-1,timestep, output_size)\n","    z_test = Z[train_size:].reshape(-1,timestep, 1)\n","\n","    return [x_train, y_train, z_train, x_test, y_test, z_test]\n","\n","# State, Action, Reward(is_click)\n","X1,Y1,Z1,X2,Y2,Z2=split_data(X,Y,Z,length,state_feature,action_feature)\n","# print(len(X2))\n","# print(len(X2))\n","X1,Y1=torch.from_numpy(X1).to(torch.float32).to(device),torch.from_numpy(Y1).to(torch.float32).to(device)\n","# print(Y2)\n","X2,Y2=torch.from_numpy(X2).to(torch.float32).to(device),torch.from_numpy(Y2).to(torch.float32).to(device)\n","Z1,Z2=torch.from_numpy(Z1).to(torch.float32).to(device),torch.from_numpy(Z2).to(torch.float32).to(device)\n","# X,Y=torch.from_numpy(X).to(torch.float32),torch.from_numpy(Y).to(torch.float32)\n","train_ids = TensorDataset(X1,Y1,Z1)\n","test_ids = TensorDataset(X2,Y2,Z2)\n","# data_ids = TensorDataset(X,Y)\n","# print(train_ids[12])\n","# print(test_ids[0])"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690976536379,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"gxuxFJ5sOBS6"},"outputs":[],"source":["class GRU(nn.Module):\n","    def __init__(self, feature_size, hidden_size, num_layers, output_size):\n","        super(GRU, self).__init__()\n","        self.hidden_size = hidden_size  # hiddensize\n","        self.num_layers = num_layers  # gru layer\n","        # self.embedding = nn.Linear(feature_size,hidden_size)\n","        # self.gru = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)\n","        self.gru = nn.GRU(feature_size, hidden_size, num_layers, batch_first=True)\n","        # self.lstm = nn.LSTM(feature_size, hidden_size, num_layers, batch_first=True)\n","        self.hidden = nn.Linear(hidden_size, hidden_size)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","        self.fc_click = nn.Linear(hidden_size, 1)\n","        # self.fc1 = nn.Linear(hidden_size, fc_hidden_size)\n","        # self.fc2 = nn.Linear(fc_hidden_size, output_size)\n","        self.sigmoid = nn.Sigmoid()\n","        self.ReLU = nn.ReLU()\n","        self.LeakyReLU = nn.LeakyReLU()\n","        self.BN = nn.BatchNorm1d(8)\n","        self.LayerNorm = nn.LayerNorm(hidden_size)\n","        self.query = nn.Linear(hidden_size, hidden_size)\n","        self.key = nn.Linear(hidden_size, hidden_size)\n","        self.value = nn.Linear(hidden_size, hidden_size)\n","        self.sqrt_size = np.sqrt(hidden_size)\n","        self.softmax = nn.Softmax(dim=-1)\n","        # self.multihead = nn.MultiheadAttention(hidden_size,num_heads,dropout=0.1)\n","    def forward(self, x, hidden=None):\n","        batch_size = x.shape[0] #  batchsize\n","        # initial hidden state\n","        if hidden is None:\n","            h_0 = x.data.new(self.num_layers, batch_size, self.hidden_size).fill_(0).float()\n","        else:\n","            h_0 = hidden\n","\n","        # GRU operation\n","        GRU_output, h_0 = self.gru(x, h_0)\n","\n","        # # Batch Normalization\n","        # output = self.BN(output)\n","\n","        # click_prob = self.LeakyReLU(GRU_output)\n","        # click_prob = click_prob.transpose(0,1)\n","        click_prob = GRU_output.transpose(0,1)\n","\n","        click_prob = self.hidden(click_prob)\n","        click_prob = self.LeakyReLU(click_prob)\n","\n","        click_prob = self.fc_click(click_prob)\n","        click_prob = self.sigmoid(click_prob)\n","\n","        # Attention Layer\n","        query_layer = self.query(GRU_output)\n","        key_layer = self.key(GRU_output).permute(0, 2, 1)\n","        value_layer = self.value(GRU_output)\n","        attention_scores = torch.matmul(query_layer, key_layer)\n","        attention_scores = attention_scores / self.sqrt_size\n","\n","        # Mask\n","        mask = (torch.triu(torch.ones(length, length)) == 0).transpose(0,1).to(device)\n","        attention_scores = attention_scores.masked_fill(mask, value=torch.tensor(-1e9))\n","        # print(attention_scores)\n","\n","        attention_scores = F.dropout(attention_scores, p=0.2)\n","        attention_probs = self.softmax(attention_scores)\n","        output = torch.matmul(attention_probs, value_layer)\n","\n","        # ReLU/LeakyReLU activation\n","        # output = self.ReLU(output)\n","        output = self.LeakyReLU(output)\n","        # output = self.LeakyReLU(GRU_output)\n","\n","\n","        # # Multihead attention\n","        # query_layer = self.query(GRU_output)\n","        # key_layer = self.key(GRU_output)\n","        # value_layer = self.value(GRU_output)\n","        # # print(query_layer.shape)\n","        # output, _ = self.multihead(query_layer,key_layer,value_layer)\n","\n","        # transpose dim 1,2 to get shape (timestep, batch_size, hidden_dim)\n","        output = output.transpose(0,1)\n","\n","        # # Add&Norm (1)\n","        output = F.dropout(self.LeakyReLU(self.hidden(output)), p=0.2) + output\n","        output = self.LayerNorm(output)\n","        output = F.dropout(self.LeakyReLU(self.hidden(output)), p=0.2) + output\n","        output = self.LayerNorm(output)\n","\n","        # # Add&Norm (2)\n","        # output = self.LeakyReLU(self.hidden(output)) + output\n","        # output = self.LayerNorm(output)\n","        # output = F.dropout(output, p=0.2)\n","        # output = self.LeakyReLU(self.hidden(output)) + output\n","        # output = self.LayerNorm(output)\n","    \n","        # # Add&Norm (3)\n","        # output = F.dropout(output, p=0.2) + GRU_output.transpose(0,1)\n","        # output = self.LayerNorm(output)\n","        # output = F.dropout(self.hidden(self.LeakyReLU(self.hidden(output))), p=0.2) + output\n","        # output = self.LayerNorm(output)\n","\n","        output = self.fc(output)  # (batch_size, timestep, output_size)\n","\n","        # Softmax convert to [0,1]\n","        action_prob = F.softmax(output,dim=2)\n","\n","        # all timestep output\n","        return action_prob, click_prob\n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690976536379,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"PwFFlRoiOBS6"},"outputs":[],"source":["Batchsize = 512\n","# data_train_loader = DataLoader(dataset=train_ids, batch_size=Batchsize, shuffle=True)\n","data_train_loader = DataLoader(dataset=train_ids, batch_size=Batchsize, shuffle=False)\n","data_test_loader = DataLoader(dataset=test_ids, batch_size=1, shuffle=False,drop_last=False)\n","# data_full_loader = DataLoader(dataset=data_ids, batch_size=1, shuffle=True)\n"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690976536379,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"5ulgrq9e6DgS"},"outputs":[],"source":["def shannon_entropy(x):\n","  p = x\n","  logp = torch.log2(p)\n","  # print(p)\n","  # print(logp)\n","  entropy = - torch.sum(p*logp,dim=-1)\n","  return entropy"]},{"cell_type":"code","execution_count":102,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":122170,"status":"ok","timestamp":1690978204312,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"3qztBjLjOBS6","outputId":"f7e713e5-0d66-4c29-a488-d4a2910d4512"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss of episode 0 = 23.339844\n","Loss of episode 1 = 22.671682\n","Loss of episode 2 = 22.564697\n","Loss of episode 3 = 22.40872\n","Loss of episode 4 = 21.744629\n","Loss of episode 5 = 21.574692\n","Loss of episode 6 = 21.54155\n","Loss of episode 7 = 21.524145\n","Loss of episode 8 = 21.516394\n","Loss of episode 9 = 21.51232\n","Loss of episode 10 = 21.511326\n","Loss of episode 11 = 21.509155\n","Loss of episode 12 = 21.5062\n","Loss of episode 13 = 21.506111\n","Loss of episode 14 = 21.505545\n","Loss of episode 15 = 21.50314\n","Loss of episode 16 = 21.501968\n","Loss of episode 17 = 21.499844\n","Loss of episode 18 = 21.498356\n","Loss of episode 19 = 21.495731\n","Loss of episode 20 = 21.493267\n","Loss of episode 21 = 21.49421\n","Loss of episode 22 = 21.491829\n","Loss of episode 23 = 21.491684\n","Loss of episode 24 = 21.490255\n","Loss of episode 25 = 21.489555\n","Loss of episode 26 = 21.489204\n","Loss of episode 27 = 21.488415\n","Loss of episode 28 = 21.48724\n","Loss of episode 29 = 21.48689\n","Loss of episode 30 = 21.486126\n","Loss of episode 31 = 21.485456\n","Loss of episode 32 = 21.484707\n","Loss of episode 33 = 21.485104\n","Loss of episode 34 = 21.48574\n","Loss of episode 35 = 21.484795\n","Loss of episode 36 = 21.484846\n","Loss of episode 37 = 21.484356\n","Loss of episode 38 = 21.482254\n","Loss of episode 39 = 21.481085\n","Loss of episode 40 = 21.479666\n","Loss of episode 41 = 21.478285\n","Loss of episode 42 = 21.477821\n","Loss of episode 43 = 21.474998\n","Loss of episode 44 = 21.471172\n","Loss of episode 45 = 21.46653\n","Loss of episode 46 = 21.464096\n","Loss of episode 47 = 21.461645\n","Loss of episode 48 = 21.459778\n","Loss of episode 49 = 21.458609\n","Loss of episode 50 = 21.457212\n","Loss of episode 51 = 21.455814\n","Loss of episode 52 = 21.455181\n","Loss of episode 53 = 21.453558\n","Loss of episode 54 = 21.448606\n","Loss of episode 55 = 21.443718\n","Loss of episode 56 = 21.439476\n","Loss of episode 57 = 21.436583\n","Loss of episode 58 = 21.43607\n","Loss of episode 59 = 21.433868\n","Loss of episode 60 = 21.434889\n","Loss of episode 61 = 21.431622\n","Loss of episode 62 = 21.429052\n","Loss of episode 63 = 21.432468\n","Loss of episode 64 = 21.427847\n","Loss of episode 65 = 21.431435\n","Loss of episode 66 = 21.428106\n","Loss of episode 67 = 21.426117\n","Loss of episode 68 = 21.427418\n","Loss of episode 69 = 21.42548\n","Loss of episode 70 = 21.42481\n","Loss of episode 71 = 21.422756\n","Loss of episode 72 = 21.424429\n","Loss of episode 73 = 21.423382\n","Loss of episode 74 = 21.423782\n","Loss of episode 75 = 21.420546\n","Loss of episode 76 = 21.419752\n","Loss of episode 77 = 21.416927\n","Loss of episode 78 = 21.417616\n","Loss of episode 79 = 21.41693\n","Loss of episode 80 = 21.416683\n","Loss of episode 81 = 21.429325\n","Loss of episode 82 = 21.42207\n","Loss of episode 83 = 21.418919\n","Loss of episode 84 = 21.41319\n","Loss of episode 85 = 21.41035\n","Loss of episode 86 = 21.41066\n","Loss of episode 87 = 21.414715\n","Loss of episode 88 = 21.412216\n","Loss of episode 89 = 21.411644\n","Loss of episode 90 = 21.408257\n","Loss of episode 91 = 21.40369\n","Loss of episode 92 = 21.404547\n","Loss of episode 93 = 21.404343\n","Loss of episode 94 = 21.405752\n","Loss of episode 95 = 21.402565\n","Loss of episode 96 = 21.400784\n","Loss of episode 97 = 21.397831\n","Loss of episode 98 = 21.394163\n","Loss of episode 99 = 21.391647\n","Loss of episode 100 = 21.391455\n","Loss of episode 101 = 21.390038\n","Loss of episode 102 = 21.39177\n","Loss of episode 103 = 21.38787\n","Loss of episode 104 = 21.382017\n","Loss of episode 105 = 21.378654\n","Loss of episode 106 = 21.374365\n","Loss of episode 107 = 21.37705\n","Loss of episode 108 = 21.37843\n","Loss of episode 109 = 21.374474\n","Loss of episode 110 = 21.369442\n","Loss of episode 111 = 21.36283\n","Loss of episode 112 = 21.357748\n","Loss of episode 113 = 21.359365\n","Loss of episode 114 = 21.354612\n","Loss of episode 115 = 21.348112\n","Loss of episode 116 = 21.341084\n","Loss of episode 117 = 21.333294\n","Loss of episode 118 = 21.32679\n","Loss of episode 119 = 21.334578\n","Loss of episode 120 = 21.330677\n","Loss of episode 121 = 21.318834\n","Loss of episode 122 = 21.31087\n","Loss of episode 123 = 21.316605\n","Loss of episode 124 = 21.308125\n","Loss of episode 125 = 21.306114\n","Loss of episode 126 = 21.300188\n","Loss of episode 127 = 21.289684\n","Loss of episode 128 = 21.276638\n","Loss of episode 129 = 21.263895\n","Loss of episode 130 = 21.252699\n","Loss of episode 131 = 21.250877\n","Loss of episode 132 = 21.247326\n","Loss of episode 133 = 21.232407\n","Loss of episode 134 = 21.219734\n","Loss of episode 135 = 21.20569\n","Loss of episode 136 = 21.197248\n","Loss of episode 137 = 21.17437\n","Loss of episode 138 = 21.16911\n","Loss of episode 139 = 21.155743\n","Loss of episode 140 = 21.142418\n","Loss of episode 141 = 21.128542\n","Loss of episode 142 = 21.115725\n","Loss of episode 143 = 21.115133\n","Loss of episode 144 = 21.121653\n","Loss of episode 145 = 21.098442\n","Loss of episode 146 = 21.069355\n","Loss of episode 147 = 21.055386\n","Loss of episode 148 = 21.033384\n","Loss of episode 149 = 21.010319\n","Loss of episode 150 = 20.98007\n","Loss of episode 151 = 20.979996\n","Loss of episode 152 = 20.975763\n","Loss of episode 153 = 20.96136\n","Loss of episode 154 = 20.93403\n","Loss of episode 155 = 20.90521\n","Loss of episode 156 = 20.903818\n","Loss of episode 157 = 20.89286\n","Loss of episode 158 = 20.887905\n","Loss of episode 159 = 20.88628\n","Loss of episode 160 = 20.889425\n","Loss of episode 161 = 20.904533\n","Loss of episode 162 = 20.897793\n","Loss of episode 163 = 20.869186\n","Loss of episode 164 = 20.828861\n","Loss of episode 165 = 20.794659\n","Loss of episode 166 = 20.716291\n","Loss of episode 167 = 20.67279\n","Loss of episode 168 = 20.645588\n","Loss of episode 169 = 20.613396\n","Loss of episode 170 = 20.600616\n","Loss of episode 171 = 20.586958\n","Loss of episode 172 = 20.58382\n","Loss of episode 173 = 20.568201\n","Loss of episode 174 = 20.547386\n","Loss of episode 175 = 20.540684\n","Loss of episode 176 = 20.545744\n","Loss of episode 177 = 20.528458\n","Loss of episode 178 = 20.529903\n","Loss of episode 179 = 20.524498\n","Loss of episode 180 = 20.485428\n","Loss of episode 181 = 20.49007\n","Loss of episode 182 = 20.487463\n","Loss of episode 183 = 20.43724\n","Loss of episode 184 = 20.425512\n","Loss of episode 185 = 20.407629\n","Loss of episode 186 = 20.371792\n","Loss of episode 187 = 20.340254\n","Loss of episode 188 = 20.291462\n","Loss of episode 189 = 20.278332\n","Loss of episode 190 = 20.26104\n","Loss of episode 191 = 20.249434\n","Loss of episode 192 = 20.246525\n","Loss of episode 193 = 20.21999\n","Loss of episode 194 = 20.216202\n","Loss of episode 195 = 20.221733\n","Loss of episode 196 = 20.1714\n","Loss of episode 197 = 20.188412\n","Loss of episode 198 = 20.168343\n","Loss of episode 199 = 20.12788\n","Loss of episode 200 = 20.104385\n","Loss of episode 201 = 20.075481\n","Loss of episode 202 = 20.057743\n","Loss of episode 203 = 20.046593\n","Loss of episode 204 = 20.022753\n","Loss of episode 205 = 20.002516\n","Loss of episode 206 = 19.986372\n","Loss of episode 207 = 19.980492\n","Loss of episode 208 = 20.034744\n","Loss of episode 209 = 20.030622\n","Loss of episode 210 = 20.013489\n","Loss of episode 211 = 19.98007\n","Loss of episode 212 = 19.975153\n","Loss of episode 213 = 19.96014\n","Loss of episode 214 = 19.937263\n","Loss of episode 215 = 19.937462\n","Loss of episode 216 = 19.93015\n","Loss of episode 217 = 19.907885\n","Loss of episode 218 = 19.92187\n","Loss of episode 219 = 19.908772\n","Loss of episode 220 = 19.902678\n","Loss of episode 221 = 19.880882\n","Loss of episode 222 = 19.877224\n","Loss of episode 223 = 19.864021\n","Loss of episode 224 = 19.864693\n","Loss of episode 225 = 19.85411\n","Loss of episode 226 = 19.85371\n","Loss of episode 227 = 19.837646\n","Loss of episode 228 = 19.847794\n","Loss of episode 229 = 19.852142\n","Loss of episode 230 = 19.857649\n","Loss of episode 231 = 19.86526\n","Loss of episode 232 = 19.90622\n","Loss of episode 233 = 19.886318\n","Loss of episode 234 = 19.838095\n","Loss of episode 235 = 19.817768\n","Loss of episode 236 = 19.798267\n","Loss of episode 237 = 19.807575\n","Loss of episode 238 = 19.802576\n","Loss of episode 239 = 19.803074\n","Loss of episode 240 = 19.814533\n","Loss of episode 241 = 19.802977\n","Loss of episode 242 = 19.807041\n","Loss of episode 243 = 19.79124\n","Loss of episode 244 = 19.791075\n","Loss of episode 245 = 19.783648\n","Loss of episode 246 = 19.779867\n","Loss of episode 247 = 19.774082\n","Loss of episode 248 = 19.77678\n","Loss of episode 249 = 19.770847\n","Loss of episode 250 = 19.759232\n","Loss of episode 251 = 19.753506\n","Loss of episode 252 = 19.750263\n","Loss of episode 253 = 19.75142\n","Loss of episode 254 = 19.74852\n","Loss of episode 255 = 19.743477\n","Loss of episode 256 = 19.737936\n","Loss of episode 257 = 19.732235\n","Loss of episode 258 = 19.731422\n","Loss of episode 259 = 19.727987\n","Loss of episode 260 = 19.730186\n","Loss of episode 261 = 19.739758\n","Loss of episode 262 = 19.7332\n","Loss of episode 263 = 19.7317\n","Loss of episode 264 = 19.721333\n","Loss of episode 265 = 19.715569\n","Loss of episode 266 = 19.718195\n","Loss of episode 267 = 19.719288\n","Loss of episode 268 = 19.711529\n","Loss of episode 269 = 19.7123\n","Loss of episode 270 = 19.721014\n","Loss of episode 271 = 19.709442\n","Loss of episode 272 = 19.714592\n","Loss of episode 273 = 19.704243\n","Loss of episode 274 = 19.695147\n","Loss of episode 275 = 19.699406\n","Loss of episode 276 = 19.69835\n","Loss of episode 277 = 19.696918\n","Loss of episode 278 = 19.696087\n","Loss of episode 279 = 19.696495\n","Loss of episode 280 = 19.69102\n","Loss of episode 281 = 19.688196\n","Loss of episode 282 = 19.683163\n","Loss of episode 283 = 19.681528\n","Loss of episode 284 = 19.690075\n","Loss of episode 285 = 19.68277\n","Loss of episode 286 = 19.694542\n","Loss of episode 287 = 19.687996\n","Loss of episode 288 = 19.698452\n","Loss of episode 289 = 19.690887\n","Loss of episode 290 = 19.686184\n","Loss of episode 291 = 19.684233\n","Loss of episode 292 = 19.678158\n","Loss of episode 293 = 19.6777\n","Loss of episode 294 = 19.674023\n","Loss of episode 295 = 19.68504\n","Loss of episode 296 = 19.683088\n","Loss of episode 297 = 19.679003\n","Loss of episode 298 = 19.691933\n","Loss of episode 299 = 19.689314\n","Loss of episode 300 = 19.683495\n","Loss of episode 301 = 19.679773\n","Loss of episode 302 = 19.673317\n","Loss of episode 303 = 19.666676\n","Loss of episode 304 = 19.672321\n","Loss of episode 305 = 19.67358\n","Loss of episode 306 = 19.680626\n","Loss of episode 307 = 19.673334\n","Loss of episode 308 = 19.665274\n","Loss of episode 309 = 19.658665\n","Loss of episode 310 = 19.658398\n","Loss of episode 311 = 19.656046\n","Loss of episode 312 = 19.655457\n","Loss of episode 313 = 19.643658\n","Loss of episode 314 = 19.652258\n","Loss of episode 315 = 19.65419\n","Loss of episode 316 = 19.648048\n","Loss of episode 317 = 19.651287\n","Loss of episode 318 = 19.649822\n","Loss of episode 319 = 19.64899\n","Loss of episode 320 = 19.646458\n","Loss of episode 321 = 19.647856\n","Loss of episode 322 = 19.646412\n","Loss of episode 323 = 19.637796\n","Loss of episode 324 = 19.638775\n","Loss of episode 325 = 19.639944\n","Loss of episode 326 = 19.635605\n","Loss of episode 327 = 19.640438\n","Loss of episode 328 = 19.64289\n","Loss of episode 329 = 19.639662\n","Loss of episode 330 = 19.631407\n","Loss of episode 331 = 19.632887\n","Loss of episode 332 = 19.629143\n","Loss of episode 333 = 19.635015\n","Loss of episode 334 = 19.640034\n","Loss of episode 335 = 19.63435\n","Loss of episode 336 = 19.633932\n","Loss of episode 337 = 19.634026\n","Loss of episode 338 = 19.649342\n","Loss of episode 339 = 19.661549\n","Loss of episode 340 = 19.664309\n","Loss of episode 341 = 19.65327\n","Loss of episode 342 = 19.655157\n","Loss of episode 343 = 19.639385\n","Loss of episode 344 = 19.64435\n","Loss of episode 345 = 19.643084\n","Loss of episode 346 = 19.63389\n","Loss of episode 347 = 19.630873\n","Loss of episode 348 = 19.63422\n","Loss of episode 349 = 19.633263\n","Loss of episode 350 = 19.633125\n","Loss of episode 351 = 19.629812\n","Loss of episode 352 = 19.62972\n","Loss of episode 353 = 19.625874\n","Loss of episode 354 = 19.630375\n","Loss of episode 355 = 19.625969\n","Loss of episode 356 = 19.62521\n","Loss of episode 357 = 19.633009\n","Loss of episode 358 = 19.629124\n","Loss of episode 359 = 19.622189\n","Loss of episode 360 = 19.623558\n","Loss of episode 361 = 19.619286\n","Loss of episode 362 = 19.616972\n","Loss of episode 363 = 19.621288\n","Loss of episode 364 = 19.622377\n","Loss of episode 365 = 19.621914\n","Loss of episode 366 = 19.623451\n","Loss of episode 367 = 19.631268\n","Loss of episode 368 = 19.622948\n","Loss of episode 369 = 19.622715\n","Loss of episode 370 = 19.620605\n","Loss of episode 371 = 19.624397\n","Loss of episode 372 = 19.644855\n","Loss of episode 373 = 19.644615\n","Loss of episode 374 = 19.641771\n","Loss of episode 375 = 19.637184\n","Loss of episode 376 = 19.636486\n","Loss of episode 377 = 19.627121\n","Loss of episode 378 = 19.632397\n","Loss of episode 379 = 19.619497\n","Loss of episode 380 = 19.618465\n","Loss of episode 381 = 19.614475\n","Loss of episode 382 = 19.618763\n","Loss of episode 383 = 19.618366\n","Loss of episode 384 = 19.615128\n","Loss of episode 385 = 19.61723\n","Loss of episode 386 = 19.613064\n","Loss of episode 387 = 19.611431\n","Loss of episode 388 = 19.613575\n","Loss of episode 389 = 19.609715\n","Loss of episode 390 = 19.611496\n","Loss of episode 391 = 19.62622\n","Loss of episode 392 = 19.627792\n","Loss of episode 393 = 19.62579\n","Loss of episode 394 = 19.617807\n","Loss of episode 395 = 19.618965\n","Loss of episode 396 = 19.610718\n","Loss of episode 397 = 19.605877\n","Loss of episode 398 = 19.607975\n","Loss of episode 399 = 19.635904\n","Loss of episode 400 = 19.641628\n","Loss of episode 401 = 19.643684\n","Loss of episode 402 = 19.641968\n","Loss of episode 403 = 19.657547\n","Loss of episode 404 = 19.648304\n","Loss of episode 405 = 19.639038\n","Loss of episode 406 = 19.628422\n","Loss of episode 407 = 19.62714\n","Loss of episode 408 = 19.623951\n","Loss of episode 409 = 19.623959\n","Loss of episode 410 = 19.620468\n","Loss of episode 411 = 19.617908\n","Loss of episode 412 = 19.611298\n","Loss of episode 413 = 19.60922\n","Loss of episode 414 = 19.605846\n","Loss of episode 415 = 19.603783\n","Loss of episode 416 = 19.607937\n","Loss of episode 417 = 19.613405\n","Loss of episode 418 = 19.608906\n","Loss of episode 419 = 19.604664\n","Loss of episode 420 = 19.59975\n","Loss of episode 421 = 19.597061\n","Loss of episode 422 = 19.59512\n","Loss of episode 423 = 19.593857\n","Loss of episode 424 = 19.593159\n","Loss of episode 425 = 19.591282\n","Loss of episode 426 = 19.59528\n","Loss of episode 427 = 19.591377\n","Loss of episode 428 = 19.591625\n","Loss of episode 429 = 19.591782\n","Loss of episode 430 = 19.591566\n","Loss of episode 431 = 19.59483\n","Loss of episode 432 = 19.59982\n","Loss of episode 433 = 19.598019\n","Loss of episode 434 = 19.597271\n","Loss of episode 435 = 19.599897\n","Loss of episode 436 = 19.597921\n","Loss of episode 437 = 19.60233\n","Loss of episode 438 = 19.599049\n","Loss of episode 439 = 19.590986\n","Loss of episode 440 = 19.589764\n","Loss of episode 441 = 19.588593\n","Loss of episode 442 = 19.596176\n","Loss of episode 443 = 19.589195\n","Loss of episode 444 = 19.590834\n","Loss of episode 445 = 19.593086\n","Loss of episode 446 = 19.593021\n","Loss of episode 447 = 19.603657\n","Loss of episode 448 = 19.597372\n","Loss of episode 449 = 19.598215\n","Loss of episode 450 = 19.59108\n","Loss of episode 451 = 19.592909\n","Loss of episode 452 = 19.589985\n","Loss of episode 453 = 19.587816\n","Loss of episode 454 = 19.586756\n","Loss of episode 455 = 19.599129\n","Loss of episode 456 = 19.60134\n","Loss of episode 457 = 19.598282\n","Loss of episode 458 = 19.594746\n","Loss of episode 459 = 19.598454\n","Loss of episode 460 = 19.594894\n","Loss of episode 461 = 19.59759\n","Loss of episode 462 = 19.59946\n","Loss of episode 463 = 19.590225\n","Loss of episode 464 = 19.591654\n","Loss of episode 465 = 19.588589\n","Loss of episode 466 = 19.586866\n","Loss of episode 467 = 19.584812\n","Loss of episode 468 = 19.57908\n","Loss of episode 469 = 19.583525\n","Loss of episode 470 = 19.57911\n","Loss of episode 471 = 19.58759\n","Loss of episode 472 = 19.589764\n","Loss of episode 473 = 19.591154\n","Loss of episode 474 = 19.586212\n","Loss of episode 475 = 19.585087\n","Loss of episode 476 = 19.584526\n","Loss of episode 477 = 19.58888\n","Loss of episode 478 = 19.579777\n","Loss of episode 479 = 19.582983\n","Loss of episode 480 = 19.590725\n","Loss of episode 481 = 19.585941\n","Loss of episode 482 = 19.58794\n","Loss of episode 483 = 19.584513\n","Loss of episode 484 = 19.583435\n","Loss of episode 485 = 19.583103\n","Loss of episode 486 = 19.583118\n","Loss of episode 487 = 19.582409\n","Loss of episode 488 = 19.580961\n","Loss of episode 489 = 19.580006\n","Loss of episode 490 = 19.576977\n","Loss of episode 491 = 19.577759\n","Loss of episode 492 = 19.584938\n","Loss of episode 493 = 19.580948\n","Loss of episode 494 = 19.576601\n","Loss of episode 495 = 19.579382\n","Loss of episode 496 = 19.57944\n","Loss of episode 497 = 19.588108\n","Loss of episode 498 = 19.588184\n","Loss of episode 499 = 19.580452\n","Loss of episode 500 = 19.592136\n","Loss of episode 501 = 19.593874\n","Loss of episode 502 = 19.591856\n","Loss of episode 503 = 19.586327\n","Loss of episode 504 = 19.58012\n","Loss of episode 505 = 19.583303\n","Loss of episode 506 = 19.582558\n","Loss of episode 507 = 19.571173\n","Loss of episode 508 = 19.572357\n","Loss of episode 509 = 19.579128\n","Loss of episode 510 = 19.57677\n","Loss of episode 511 = 19.573277\n","Loss of episode 512 = 19.573357\n","Loss of episode 513 = 19.570347\n","Loss of episode 514 = 19.571661\n","Loss of episode 515 = 19.580133\n","Loss of episode 516 = 19.587456\n","Loss of episode 517 = 19.586777\n","Loss of episode 518 = 19.581924\n","Loss of episode 519 = 19.577255\n","Loss of episode 520 = 19.577724\n","Loss of episode 521 = 19.577747\n","Loss of episode 522 = 19.576313\n","Loss of episode 523 = 19.596724\n","Loss of episode 524 = 19.599009\n","Loss of episode 525 = 19.598507\n","Loss of episode 526 = 19.592972\n","Loss of episode 527 = 19.589056\n","Loss of episode 528 = 19.584913\n","Loss of episode 529 = 19.580856\n","Loss of episode 530 = 19.570045\n","Loss of episode 531 = 19.573223\n","Loss of episode 532 = 19.571033\n","Loss of episode 533 = 19.57019\n","Loss of episode 534 = 19.565914\n","Loss of episode 535 = 19.567749\n","Loss of episode 536 = 19.568005\n","Loss of episode 537 = 19.569334\n","Loss of episode 538 = 19.57534\n","Loss of episode 539 = 19.565903\n","Loss of episode 540 = 19.573898\n","Loss of episode 541 = 19.579641\n","Loss of episode 542 = 19.57964\n","Loss of episode 543 = 19.57977\n","Loss of episode 544 = 19.573132\n","Loss of episode 545 = 19.574223\n","Loss of episode 546 = 19.57118\n","Loss of episode 547 = 19.573437\n","Loss of episode 548 = 19.581871\n","Loss of episode 549 = 19.580679\n","Loss of episode 550 = 19.578129\n","Loss of episode 551 = 19.576778\n","Loss of episode 552 = 19.576906\n","Loss of episode 553 = 19.58503\n","Loss of episode 554 = 19.591434\n","Loss of episode 555 = 19.59508\n","Loss of episode 556 = 19.589066\n","Loss of episode 557 = 19.581032\n","Loss of episode 558 = 19.58772\n","Loss of episode 559 = 19.579933\n","Loss of episode 560 = 19.576668\n","Loss of episode 561 = 19.574928\n","Loss of episode 562 = 19.578293\n","Loss of episode 563 = 19.591421\n","Loss of episode 564 = 19.593557\n","Loss of episode 565 = 19.58721\n","Loss of episode 566 = 19.582466\n","Loss of episode 567 = 19.57626\n","Loss of episode 568 = 19.578074\n","Loss of episode 569 = 19.573822\n","Loss of episode 570 = 19.567429\n","Loss of episode 571 = 19.566887\n","Loss of episode 572 = 19.576725\n","Loss of episode 573 = 19.573141\n","Loss of episode 574 = 19.57172\n","Loss of episode 575 = 19.569092\n","Loss of episode 576 = 19.567675\n","Loss of episode 577 = 19.566422\n","Loss of episode 578 = 19.561897\n","Loss of episode 579 = 19.566826\n","Loss of episode 580 = 19.572687\n","Loss of episode 581 = 19.566727\n","Loss of episode 582 = 19.566565\n","Loss of episode 583 = 19.564459\n","Loss of episode 584 = 19.566875\n","Loss of episode 585 = 19.57169\n","Loss of episode 586 = 19.570606\n","Loss of episode 587 = 19.573795\n","Loss of episode 588 = 19.574835\n","Loss of episode 589 = 19.566929\n","Loss of episode 590 = 19.570099\n","Loss of episode 591 = 19.57436\n","Loss of episode 592 = 19.579659\n","Loss of episode 593 = 19.571009\n","Loss of episode 594 = 19.562141\n","Loss of episode 595 = 19.567802\n","Loss of episode 596 = 19.566257\n","Loss of episode 597 = 19.571877\n","Loss of episode 598 = 19.565248\n","Loss of episode 599 = 19.569443\n","Loss of episode 600 = 19.563549\n","Loss of episode 601 = 19.558764\n","Loss of episode 602 = 19.560791\n","Loss of episode 603 = 19.561573\n","Loss of episode 604 = 19.561853\n","Loss of episode 605 = 19.562143\n","Loss of episode 606 = 19.558966\n","Loss of episode 607 = 19.560362\n","Loss of episode 608 = 19.564783\n","Loss of episode 609 = 19.562702\n","Loss of episode 610 = 19.557636\n","Loss of episode 611 = 19.556664\n","Loss of episode 612 = 19.557676\n","Loss of episode 613 = 19.555069\n","Loss of episode 614 = 19.55353\n","Loss of episode 615 = 19.555523\n","Loss of episode 616 = 19.557484\n","Loss of episode 617 = 19.557959\n","Loss of episode 618 = 19.56148\n","Loss of episode 619 = 19.559164\n","Loss of episode 620 = 19.55406\n","Loss of episode 621 = 19.561686\n","Loss of episode 622 = 19.560799\n","Loss of episode 623 = 19.555927\n","Loss of episode 624 = 19.573303\n","Loss of episode 625 = 19.573345\n","Loss of episode 626 = 19.569067\n","Loss of episode 627 = 19.56799\n","Loss of episode 628 = 19.56195\n","Loss of episode 629 = 19.568748\n","Loss of episode 630 = 19.567371\n","Loss of episode 631 = 19.562937\n","Loss of episode 632 = 19.572699\n","Loss of episode 633 = 19.575993\n","Loss of episode 634 = 19.577404\n","Loss of episode 635 = 19.582035\n","Loss of episode 636 = 19.57754\n","Loss of episode 637 = 19.57186\n","Loss of episode 638 = 19.573383\n","Loss of episode 639 = 19.568153\n","Loss of episode 640 = 19.56572\n","Loss of episode 641 = 19.566597\n","Loss of episode 642 = 19.565025\n","Loss of episode 643 = 19.561104\n","Loss of episode 644 = 19.55724\n","Loss of episode 645 = 19.558094\n","Loss of episode 646 = 19.55611\n","Loss of episode 647 = 19.556877\n","Loss of episode 648 = 19.558847\n","Loss of episode 649 = 19.55352\n","Loss of episode 650 = 19.558695\n","Loss of episode 651 = 19.55688\n","Loss of episode 652 = 19.559278\n","Loss of episode 653 = 19.550507\n","Loss of episode 654 = 19.551481\n","Loss of episode 655 = 19.553154\n","Loss of episode 656 = 19.551268\n","Loss of episode 657 = 19.556864\n","Loss of episode 658 = 19.561163\n","Loss of episode 659 = 19.556715\n","Loss of episode 660 = 19.556496\n","Loss of episode 661 = 19.549007\n","Loss of episode 662 = 19.552563\n","Loss of episode 663 = 19.554739\n","Loss of episode 664 = 19.551605\n","Loss of episode 665 = 19.549372\n","Loss of episode 666 = 19.546043\n","Loss of episode 667 = 19.546312\n","Loss of episode 668 = 19.546713\n","Loss of episode 669 = 19.544464\n","Loss of episode 670 = 19.548494\n","Loss of episode 671 = 19.550554\n","Loss of episode 672 = 19.549568\n","Loss of episode 673 = 19.549757\n","Loss of episode 674 = 19.548231\n","Loss of episode 675 = 19.547356\n","Loss of episode 676 = 19.554415\n","Loss of episode 677 = 19.556358\n","Loss of episode 678 = 19.561256\n","Loss of episode 679 = 19.577484\n","Loss of episode 680 = 19.579908\n","Loss of episode 681 = 19.58551\n","Loss of episode 682 = 19.579964\n","Loss of episode 683 = 19.579588\n","Loss of episode 684 = 19.575829\n","Loss of episode 685 = 19.569572\n","Loss of episode 686 = 19.565868\n","Loss of episode 687 = 19.560648\n","Loss of episode 688 = 19.555752\n","Loss of episode 689 = 19.55911\n","Loss of episode 690 = 19.556181\n","Loss of episode 691 = 19.56144\n","Loss of episode 692 = 19.55462\n","Loss of episode 693 = 19.55915\n","Loss of episode 694 = 19.565098\n","Loss of episode 695 = 19.56359\n","Loss of episode 696 = 19.558556\n","Loss of episode 697 = 19.566635\n","Loss of episode 698 = 19.557657\n","Loss of episode 699 = 19.556723\n","Loss of episode 700 = 19.556316\n","Loss of episode 701 = 19.557907\n","Loss of episode 702 = 19.559607\n","Loss of episode 703 = 19.560875\n","Loss of episode 704 = 19.558273\n","Loss of episode 705 = 19.608511\n","Loss of episode 706 = 19.609982\n","Loss of episode 707 = 19.593697\n","Loss of episode 708 = 19.587626\n","Loss of episode 709 = 19.579044\n","Loss of episode 710 = 19.57339\n","Loss of episode 711 = 19.562893\n","Loss of episode 712 = 19.564222\n","Loss of episode 713 = 19.56208\n","Loss of episode 714 = 19.556744\n","Loss of episode 715 = 19.573294\n","Loss of episode 716 = 19.573421\n","Loss of episode 717 = 19.570562\n","Loss of episode 718 = 19.567377\n","Loss of episode 719 = 19.564236\n","Loss of episode 720 = 19.561018\n","Loss of episode 721 = 19.556793\n","Loss of episode 722 = 19.55891\n","Loss of episode 723 = 19.555372\n","Loss of episode 724 = 19.554173\n","Loss of episode 725 = 19.553226\n","Loss of episode 726 = 19.555948\n","Loss of episode 727 = 19.557236\n","Loss of episode 728 = 19.56184\n","Loss of episode 729 = 19.554836\n","Loss of episode 730 = 19.555342\n","Loss of episode 731 = 19.553835\n","Loss of episode 732 = 19.553556\n","Loss of episode 733 = 19.552544\n","Loss of episode 734 = 19.551422\n","Loss of episode 735 = 19.544662\n","Loss of episode 736 = 19.547384\n","Loss of episode 737 = 19.548033\n","Loss of episode 738 = 19.546822\n","Loss of episode 739 = 19.544067\n","Loss of episode 740 = 19.544506\n","Loss of episode 741 = 19.54224\n","Loss of episode 742 = 19.542591\n","Loss of episode 743 = 19.5406\n","Loss of episode 744 = 19.54113\n","Loss of episode 745 = 19.541016\n","Loss of episode 746 = 19.54185\n","Loss of episode 747 = 19.545809\n","Loss of episode 748 = 19.54512\n","Loss of episode 749 = 19.543121\n","Loss of episode 750 = 19.54183\n","Loss of episode 751 = 19.54314\n","Loss of episode 752 = 19.549248\n","Loss of episode 753 = 19.560303\n","Loss of episode 754 = 19.556036\n","Loss of episode 755 = 19.557468\n","Loss of episode 756 = 19.557201\n","Loss of episode 757 = 19.55657\n","Loss of episode 758 = 19.551598\n","Loss of episode 759 = 19.548409\n","Loss of episode 760 = 19.552181\n","Loss of episode 761 = 19.559687\n","Loss of episode 762 = 19.550442\n","Loss of episode 763 = 19.549423\n","Loss of episode 764 = 19.544197\n","Loss of episode 765 = 19.543169\n","Loss of episode 766 = 19.540047\n","Loss of episode 767 = 19.54254\n","Loss of episode 768 = 19.54153\n","Loss of episode 769 = 19.538588\n","Loss of episode 770 = 19.53896\n","Loss of episode 771 = 19.535479\n","Loss of episode 772 = 19.541496\n","Loss of episode 773 = 19.540535\n","Loss of episode 774 = 19.541733\n","Loss of episode 775 = 19.540913\n","Loss of episode 776 = 19.547014\n","Loss of episode 777 = 19.545895\n","Loss of episode 778 = 19.543098\n","Loss of episode 779 = 19.544184\n","Loss of episode 780 = 19.546743\n","Loss of episode 781 = 19.543476\n","Loss of episode 782 = 19.541979\n","Loss of episode 783 = 19.539768\n","Loss of episode 784 = 19.538975\n","Loss of episode 785 = 19.546257\n","Loss of episode 786 = 19.544767\n","Loss of episode 787 = 19.543175\n","Loss of episode 788 = 19.541712\n","Loss of episode 789 = 19.543325\n","Loss of episode 790 = 19.544445\n","Loss of episode 791 = 19.540745\n","Loss of episode 792 = 19.542673\n","Loss of episode 793 = 19.539886\n","Loss of episode 794 = 19.542473\n","Loss of episode 795 = 19.548613\n","Loss of episode 796 = 19.553637\n","Loss of episode 797 = 19.551073\n","Loss of episode 798 = 19.569344\n","Loss of episode 799 = 19.561623\n","\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# feature_size, hidden_size, num_layers, output_size, (number_heads)\n","gru = GRU(state_feature,512,2,3).to(device)\n","num_epoch = 800     # 800\n","Lambda = 1.5        # 1.5\n","Mu = 0.08           # 0.08\n","\n","# optimizer = torch.optim.SGD(net.parameters(),lr=0.5)\n","# optimizer = torch.optim.Adam(gru.parameters(),lr=0.001)\n","optimizer = torch.optim.AdamW(gru.parameters(),lr=0.001)\n","\n","# loss_func = torch.nn.MSELoss()            #MSE for regression\n","loss_func = torch.nn.CrossEntropyLoss()     #CE for classification\n","Loss=[]\n","\n","for t in range(num_epoch):\n","    aver_loss = 0\n","    gru.train()\n","    for batch, data in enumerate(data_train_loader):\n","        x, y ,z = data\n","        action_prediction, click_prediction = gru(x)\n","        # print(action_prediction.shape)\n","        action_prediction = torch.transpose(action_prediction, dim0=0, dim1=1)\n","        click_prediction = torch.transpose(click_prediction, dim0=0, dim1=1)\n","        Entropy = torch.mean(shannon_entropy(action_prediction))\n","        # loss = loss_func(action_prediction,y) - Lambda * Entropy #//2-loss\n","        loss = loss_func(action_prediction,y) + Mu * loss_func(click_prediction,z) \\\n","        - Lambda * Entropy \n","        optimizer.zero_grad() \n","        loss.backward()    \n","        optimizer.step() \n","        aver_loss += loss\n","    aver_loss /= batch\n","    aver_loss=aver_loss.cpu().detach().numpy()\n","    print('Loss of episode %s ='%t,aver_loss)\n","    Loss.append(aver_loss)\n","plt.plot(Loss,color='r')\n","print()"]},{"cell_type":"code","execution_count":103,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690978204313,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"lxxPbtYyOBS6","outputId":"d2b4bdae-3248-45d5-9a2c-25bb1d8fc299"},"outputs":[{"name":"stdout","output_type":"stream","text":["3460628\n"]}],"source":["device = torch.device(\"cuda\")\n","gru_test = GRU(state_feature,512,2,3).to(device)\n","total = sum([param.nelement() for param in gru_test.parameters()])\n","print(total)\n","# gru_test"]},{"cell_type":"markdown","metadata":{"id":"Qiqn4DAJOBS7"},"source":["## Training accuracy"]},{"cell_type":"code","execution_count":104,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690978204313,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"BUoPP1KjOBS7","outputId":"4ec4f842-dd61-42d5-e7f4-01bea8ab6944"},"outputs":[{"name":"stdout","output_type":"stream","text":["acc = 0.982\n"]}],"source":["Truth = []\n","Pred = []\n","aver_loss = 0\n","total_acc = 0\n","Entropy = 0\n","for batch, data in enumerate(data_train_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    x, y, z = data\n","    # print(x)\n","    # prediction =net(x)\n","    test, _ = gru(x)\n","    test = torch.transpose(test, dim0=0, dim1=1)\n","    # print(test)\n","    # print(y)\n","    # print(shannon_entropy(test).shape)\n","    aver_entropy = sum(torch.mean(shannon_entropy(test),dim=1))\n","    # print(sum(aver_entropy))\n","    Entropy += aver_entropy\n","\n","    a = torch.argmax(test,dim = 2).cpu().data.numpy()\n","    b = torch.argmax(y,dim = 2).cpu().data.numpy()\n","    # print(a)\n","    # print(b)\n","    # print(sum(a==b))\n","    aver_acc = sum(sum(a==b))/length\n","    # print(aver_acc)\n","    total_acc += aver_acc\n","    # print(total_acc)\n","# print(batch)\n","print('acc =', total_acc/(0.8*max_number_traj))\n","# print('Entropy = ', Entropy/(0.8*max_number_traj))"]},{"cell_type":"markdown","metadata":{"id":"6Hi558JBOBS8"},"source":["## Inference accuracy"]},{"cell_type":"code","execution_count":105,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1541,"status":"ok","timestamp":1690978205851,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"dH6BC5O2OBS8","outputId":"f0746245-4215-490b-c3b9-ee67948cc407"},"outputs":[{"name":"stdout","output_type":"stream","text":["Action acc = 0.6567499999999998\n","CTR acc = 0.7876875000000002\n","Entropy =  tensor(0.9858, grad_fn=<DivBackward0>)\n"]}],"source":["from sklearn.metrics import roc_curve, auc\n","\n","\n","Loss=[]\n","Truth = []\n","Pred = []\n","aver_loss = 0\n","total_acc = 0\n","total_ctr = 0\n","total_auc = 0\n","Entropy = 0\n","num = 0\n","\n","# Click_pred = np.zeros((int(0.2*max_number_traj),length))\n","# Click_true = np.zeros((int(0.2*max_number_traj),length))\n","\n","for batch, data in enumerate(data_test_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    gru.eval()\n","    x, y, z = data\n","    test, click = gru(x)\n","    click_pred = torch.squeeze(click).cpu().data.numpy()\n","    click_true = torch.squeeze(z).cpu().data.numpy()\n","    click_pred[click_pred<1/2] = 0\n","    click_pred[click_pred>=1/2] = 1\n","    # Click_pred[batch] = click_pred\n","    # Click_true[batch] = click_true\n","\n","    p = test[:,0].cpu()\n","    aver_entropy = torch.mean(shannon_entropy(p))\n","    # print('aver_entropy =', aver_entropy)\n","    Entropy += aver_entropy\n","    # print(test)\n","    a = torch.argmax(test,dim = 2).cpu().data.numpy().T[0]\n","    b = torch.argmax(y,dim = 2).cpu().data.numpy()[0]\n","    # print('-----------')\n","    # print('pred =',a.T[0])\n","    # print('true =',b[0])\n","    aver_acc = sum(a==b)/length\n","    total_acc += aver_acc\n","\n","    # print('-----------')\n","    # print('pred =', click_pred)\n","    # print('true =', click_true) \n","    aver_ctr = sum(click_pred==click_true)/length\n","    total_ctr += aver_ctr\n","\n","\n","# fpr, tpr, thresholds = roc_curve(Click_true.reshape(-1), Click_pred.reshape(-1),pos_label=1)\n","# total_auc = auc(fpr, tpr)\n","# print('CTR AUC =', total_auc)\n","print('Action acc =', total_acc/(0.2*max_number_traj))\n","print('CTR acc =', total_ctr/(0.2*max_number_traj))\n","print('Entropy = ', Entropy/(0.2*max_number_traj))\n"]},{"cell_type":"markdown","metadata":{"id":"n6ecXL0POBS8"},"source":["## AUC"]},{"cell_type":"code","execution_count":111,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":736,"status":"ok","timestamp":1690978206585,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"45g3DhluOBS8","outputId":"9e567364-e555-4ef1-be01-fdcdc6de971b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Average AUC = [0.77058565 0.77890747 0.77571306]\n"]}],"source":["from sklearn.metrics import precision_score, roc_curve, auc\n","n_classes = 3\n","# length = 8\n","aver_auc = np.zeros(n_classes)\n","total_precision = 0\n","Y_score = np.zeros(((length, int(0.2*max_number_traj), n_classes)))\n","# print(Y_score.shape)\n","Y_label = np.zeros(((length, int(0.2*max_number_traj), n_classes)))\n","for batch, data in enumerate(data_test_loader):\n","    gru.eval()\n","    x, y, z = data\n","    # print(x)\n","    test, _ = gru(x)\n","    y_score = test[:,0,:].cpu().data.numpy()\n","    y_label = y[0].cpu().data.numpy()\n","    # print(batch)\n","    for i in range(length):\n","        Y_score[i][batch] = y_score[i]\n","        Y_label[i][batch] = y_label[i]\n","        # Y_score[i][batch] = [1/3,1/3,1/3]\n","        # Y_label[i][batch] = y_label[i]\n","        # print(y_score[i])\n","        # print(y_label[i])\n","fpr = dict()\n","tpr = dict()\n","roc_auc = dict()\n","aver_auc = np.zeros(n_classes)\n","for t in range(length):\n","    for i in range(n_classes):\n","        fpr[i], tpr[i], _ = roc_curve(Y_label[t][:, i], Y_score[t][:, i])\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","        # aver_auc[i] += auc(fpr[i], tpr[i])\n","        aver_auc[i] += roc_auc[i]/length\n","    # print('step %s AUC ='%(t+1),roc_auc)\n","print('Average AUC =', aver_auc)"]},{"cell_type":"code","execution_count":112,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1690978206585,"user":{"displayName":"langming liu","userId":"10823962825404054022"},"user_tz":-480},"id":"Q-5eCoFCOBS8","outputId":"1fe76151-c1a2-4ced-8acf-4a37d2df9e6e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Aver_F1 = 0.6576 Aver_Precision = 0.6584 Aver_Recall = 0.6581\n"]}],"source":["from sklearn.metrics import f1_score, precision_score, recall_score\n","\n","aver_f1 = 0\n","aver_p = 0\n","aver_r = 0\n","\n","# print(y_pred)\n","# print(y_true)\n","for i in range(length):\n","  y_true = np.argmax(Y_label[i],axis = 1)\n","  y_pred = np.argmax(Y_score[i],axis = 1)\n","  f1 = round(f1_score(y_true, y_pred, average='macro' ),4)\n","  p = round(precision_score(y_true, y_pred, average='macro'),4)\n","  r = round(recall_score(y_true, y_pred, average='macro'),4)\n","  aver_f1 += f1/length\n","  aver_p += p/length\n","  aver_r += r/length\n","print('Aver_F1 =', round(aver_f1,4), 'Aver_Precision =', round(aver_p,4), 'Aver_Recall =', round(aver_r,4))"]},{"cell_type":"markdown","metadata":{},"source":["## Budget Allocation"]},{"cell_type":"code","execution_count":113,"metadata":{},"outputs":[],"source":["test_number = int(0.2*max_number_traj)\n","True_Click_matrix = np.zeros((test_number,length))\n","True_Action_matrix = np.zeros((test_number,length))\n","Click_pred_matrix = np.zeros((test_number,length))\n","Action_pred_matrix = np.zeros((test_number,length))\n","\n","for batch, data in enumerate(data_test_loader):\n","# for batch, data in enumerate(data_full_loader):\n","# for x,y in data_train_loader:\n","    gru.eval()\n","    x, y, z = data\n","    action_true = torch.squeeze(torch.argmax(y,dim = 2)).cpu().data.numpy()\n","    click_true = torch.squeeze(z).cpu().data.numpy()\n","    True_Click_matrix[batch] = click_true\n","    # True_Action_count += np.sum(action_true, axis = 0)\n","    \n","    action, click = gru(x)\n","    action_pred = torch.squeeze(torch.argmax(action,dim = 2)).cpu().data.numpy()\n","    # print(action_true == action_pred)\n","    Action_pred_matrix[batch] = (action_true == action_pred)\n","    click_pred = torch.squeeze(click).cpu().data.numpy()\n","    Click_pred_matrix[batch] = click_pred\n","\n","# print(True_Click_matrix)"]},{"cell_type":"code","execution_count":114,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True_cost =  16000.0\n","True click number =  2869.0\n","Ture_CPC = 5.576856047403276\n","Policy_cost =  4000.0\n","Policy click number =  1033.0\n","Policy_CPC = 3.872216844143272\n"]}],"source":["choose_number = 200     # 50\n","True_cost = 0.2 * max_number_traj * length\n","True_Click_number = sum(sum(True_Click_matrix))\n","User_choose = np.zeros((test_number,length))\n","\n","print('True_cost = ', True_cost)\n","print('True click number = ', True_Click_number)\n","True_CPC = (0.2 * max_number_traj) * length / True_Click_number\n","print('Ture_CPC =',True_CPC)\n","for i in range(length):\n","    tmp = Click_pred_matrix[:,i]\n","    idx = np.argsort(tmp)\n","    # print(idx[400:])\n","    # print(tmp[idx[400:]])\n","    User_choose[:,i][idx[test_number-choose_number:]] = 1\n","\n","Cost = sum(sum(User_choose))\n","Click = sum(sum(User_choose*Action_pred_matrix*True_Click_matrix))\n","cpc = Cost/Click\n","print('Policy_cost = ', Cost)\n","print('Policy click number = ', Click) \n","print('Policy_CPC =', cpc)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
