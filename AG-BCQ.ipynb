{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "# torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6006,)\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "Data = pd.read_csv('kuairand_sequence.csv',index_col=0)\n",
    "max_length = 100\n",
    "num_user = len(Data['user_id'].unique())\n",
    "state_feature = len(Data.columns) - 1\n",
    "action_feature = 3\n",
    "\n",
    "uid = Data['user_id'].unique()\n",
    "uid\n",
    "print(uid.shape)\n",
    "print(state_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6006\n",
      "6006\n",
      "6006\n",
      "(6006, 100)\n"
     ]
    }
   ],
   "source": [
    "State = np.zeros(((num_user,max_length,state_feature)))\n",
    "Action = np.zeros((num_user,max_length,action_feature))\n",
    "Click = np.zeros((num_user,max_length))\n",
    "\n",
    "j = 0\n",
    "for i in uid:\n",
    "    state_sequence = np.array(Data[Data['user_id']==i])[:-1]\n",
    "    # state_sequence = np.array(data[data['user_id']==i].drop(['short','mid','long'],axis=1))\n",
    "    action_sequence = np.array(Data[Data['user_id']==i][['short','mid','long']])[1:]\n",
    "    click_sequence = np.array(Data[Data['user_id']==i]['is_click'])[1:]\n",
    "    # action_sequence = np.array(data[data['user_id']==i][['short','mid','long']])\n",
    "    # print(action_sequence)\n",
    "    state = np.pad(state_sequence,((0,max_length-len(state_sequence)),(0,0)))[:,1:]\n",
    "    action = np.pad(action_sequence,((0,max_length-len(action_sequence)),(0,0)))\n",
    "    click = np.pad(click_sequence,((0,max_length-len(click_sequence))))\n",
    "    # print(click)\n",
    "    State[j] = state\n",
    "    Action[j] = action\n",
    "    Click[j] = click\n",
    "    j += 1\n",
    "print(len(State))\n",
    "print(len(Action))\n",
    "print(len(Click))\n",
    "print(Click.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "[[0.82608696 0.68647892 0.         0.         0.13927778 1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.         0.\n",
      "  0.         0.04081633 0.77346939 0.21428571 0.5        0.17094017\n",
      "  0.76821192 0.66666667 0.75      ]\n",
      " [0.82608696 0.68663705 1.         1.         1.45355556 0.\n",
      "  0.         1.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.         0.\n",
      "  0.         0.04081633 0.77346939 0.21428571 0.5        0.17094017\n",
      "  0.76821192 0.66666667 0.75      ]]\n",
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "length = 2\n",
    "max_number_traj = 40000\n",
    "\n",
    "number_traj = 0\n",
    "def Select_trajectory(X,Y,Z,number_traj):\n",
    "    Select_states = []\n",
    "    Select_actions = []\n",
    "    Select_clicks = []\n",
    "    # for s in range(number_traj):\n",
    "    while True:\n",
    "        i = random.randint(0,len(X)-1)\n",
    "        select_state = X[i]\n",
    "        select_action = Y[i]\n",
    "        select_click = Z[i]\n",
    "        # print(select_state)\n",
    "        # print(select_action)\n",
    "        for k in range(0,max_length):\n",
    "            # print(select_episode[k])\n",
    "            if select_state[k].any() == 0:\n",
    "                max_episode_length = k\n",
    "                break\n",
    "        # print(k)\n",
    "        # print(max_episode_length)\n",
    "        if max_episode_length >= length + 1:\n",
    "            j = random.randint(0,max_episode_length-length-1)\n",
    "            select_state = select_state[j:j+length]\n",
    "            select_action = select_action[j:j+length]\n",
    "            select_click = select_click[j:j+length]\n",
    "            Select_states.append(select_state)\n",
    "            Select_actions.append(select_action)\n",
    "            Select_clicks.append(select_click)\n",
    "            number_traj += 1\n",
    "        # print(select_trajectory)\n",
    "        if number_traj == max_number_traj:\n",
    "            break\n",
    "    return np.array(Select_states),np.array(Select_actions),np.array(Select_clicks)\n",
    "\n",
    "X , Y , Z = Select_trajectory(State,Action,Click,number_traj)\n",
    "print(len(X))\n",
    "print(X[0])\n",
    "print(Y[0])\n",
    "print(Z[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2023)\n",
    "per = np.random.permutation(X.shape[0])\n",
    "# print(per)\n",
    "X = X[per]\n",
    "Y = Y[per]\n",
    "Z = Z[per]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000\n",
      "(tensor([[0.8261, 0.8673, 0.0000, 0.0000, 0.0498, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
      "         0.3333, 0.5918, 0.8816, 0.0714, 0.0000, 0.2650, 0.6600, 1.0000, 0.7500],\n",
      "        [0.6522, 0.9773, 0.0000, 0.0000, 0.2233, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
      "         0.3333, 0.5918, 0.8816, 0.0714, 0.0000, 0.2650, 0.6600, 1.0000, 0.7500]],\n",
      "       device='cuda:0'), tensor([[0., 1., 0.],\n",
      "        [0., 0., 1.]], device='cuda:0'), tensor([[0.],\n",
      "        [0.]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "def split_data(State, Action, Reward, timestep, input_size, output_size):\n",
    "\n",
    "    train_size = int(np.round(0.8 * X.shape[0]))\n",
    "    print(train_size)\n",
    "\n",
    "    x_train = State[: train_size, :].reshape(-1, timestep,input_size)\n",
    "    y_train = Action[: train_size].reshape(-1,timestep, output_size)\n",
    "    z_train = Reward[: train_size].reshape(-1,timestep, 1)\n",
    "\n",
    "    x_test = State[train_size:, :].reshape(-1, timestep,input_size)\n",
    "    y_test = Action[train_size:].reshape(-1,timestep, output_size)\n",
    "    z_test = Reward[train_size:].reshape(-1,timestep, 1)\n",
    "\n",
    "    return [x_train, y_train, z_train, x_test, y_test, z_test]\n",
    "\n",
    "X1,Y1,Z1,X2,Y2,Z2=split_data(X,Y,Z,length,state_feature,action_feature)\n",
    "X1,Y1=torch.from_numpy(X1).to(torch.float32).to(device),torch.from_numpy(Y1).to(torch.float32).to(device)\n",
    "X2,Y2=torch.from_numpy(X2).to(torch.float32).to(device),torch.from_numpy(Y2).to(torch.float32).to(device)\n",
    "Z1,Z2=torch.from_numpy(Z1).to(torch.float32).to(device),torch.from_numpy(Z2).to(torch.float32).to(device)\n",
    "train_ids = TensorDataset(X1,Y1,Z1)\n",
    "test_ids = TensorDataset(X2,Y2,Z2)\n",
    "print(test_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGBCQ(torch.nn.Module):\n",
    "    def __init__(self,feature_size, NUM_STATES,NUM_ACTIONS):\n",
    "        super(AGBCQ, self).__init__()\n",
    "        self.fc1 = nn.Linear(NUM_STATES, feature_size)\n",
    "        self.fc1.weight.data.normal_(0,0.1)\n",
    "        self.fc2 = nn.Linear(feature_size,feature_size)\n",
    "        self.fc2.weight.data.normal_(0,0.1)\n",
    "        self.out = nn.Linear(feature_size,NUM_ACTIONS)\n",
    "        self.out.weight.data.normal_(0,0.1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        action_value = self.out(x)\n",
    "        return action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batchsize = 512\n",
    "# data_train_loader = DataLoader(dataset=train_ids, batch_size=Batchsize, shuffle=True)\n",
    "data_train_loader = DataLoader(dataset=train_ids, batch_size=Batchsize, shuffle=False)\n",
    "data_test_loader = DataLoader(dataset=test_ids, batch_size=1, shuffle=False,drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of episode 0 = 0.7898014\n",
      "Loss of episode 1 = 0.4029603\n",
      "Loss of episode 2 = 0.38723466\n",
      "Loss of episode 3 = 0.37916204\n",
      "Loss of episode 4 = 0.37305775\n",
      "Loss of episode 5 = 0.37418133\n",
      "Loss of episode 6 = 0.3671598\n",
      "Loss of episode 7 = 0.36391413\n",
      "Loss of episode 8 = 0.3618538\n",
      "Loss of episode 9 = 0.36178917\n",
      "Loss of episode 10 = 0.3600831\n",
      "Loss of episode 11 = 0.3575715\n",
      "Loss of episode 12 = 0.3516345\n",
      "Loss of episode 13 = 0.35003555\n",
      "Loss of episode 14 = 0.3514634\n",
      "Loss of episode 15 = 0.3522347\n",
      "Loss of episode 16 = 0.3491905\n",
      "Loss of episode 17 = 0.34892917\n",
      "Loss of episode 18 = 0.34877285\n",
      "Loss of episode 19 = 0.3476217\n",
      "Loss of episode 20 = 0.34665117\n",
      "Loss of episode 21 = 0.348198\n",
      "Loss of episode 22 = 0.348581\n",
      "Loss of episode 23 = 0.34130794\n",
      "Loss of episode 24 = 0.3357677\n",
      "Loss of episode 25 = 0.33368644\n",
      "Loss of episode 26 = 0.33276716\n",
      "Loss of episode 27 = 0.33114234\n",
      "Loss of episode 28 = 0.3287902\n",
      "Loss of episode 29 = 0.32856238\n",
      "Loss of episode 30 = 0.32718015\n",
      "Loss of episode 31 = 0.32589734\n",
      "Loss of episode 32 = 0.32505432\n",
      "Loss of episode 33 = 0.3240248\n",
      "Loss of episode 34 = 0.32294565\n",
      "Loss of episode 35 = 0.32154053\n",
      "Loss of episode 36 = 0.3211458\n",
      "Loss of episode 37 = 0.3203044\n",
      "Loss of episode 38 = 0.32011598\n",
      "Loss of episode 39 = 0.32374224\n",
      "Loss of episode 40 = 0.32266647\n",
      "Loss of episode 41 = 0.3183229\n",
      "Loss of episode 42 = 0.31614235\n",
      "Loss of episode 43 = 0.3133816\n",
      "Loss of episode 44 = 0.3109741\n",
      "Loss of episode 45 = 0.30991387\n",
      "Loss of episode 46 = 0.30788997\n",
      "Loss of episode 47 = 0.30729586\n",
      "Loss of episode 48 = 0.30775115\n",
      "Loss of episode 49 = 0.30654353\n",
      "Loss of episode 50 = 0.30650845\n",
      "Loss of episode 51 = 0.3061027\n",
      "Loss of episode 52 = 0.30470085\n",
      "Loss of episode 53 = 0.30611378\n",
      "Loss of episode 54 = 0.30901748\n",
      "Loss of episode 55 = 0.30534795\n",
      "Loss of episode 56 = 0.30129656\n",
      "Loss of episode 57 = 0.29830006\n",
      "Loss of episode 58 = 0.2977714\n",
      "Loss of episode 59 = 0.2986346\n",
      "Loss of episode 60 = 0.2992493\n",
      "Loss of episode 61 = 0.29752997\n",
      "Loss of episode 62 = 0.2940878\n",
      "Loss of episode 63 = 0.29228285\n",
      "Loss of episode 64 = 0.2927745\n",
      "Loss of episode 65 = 0.29117975\n",
      "Loss of episode 66 = 0.29136902\n",
      "Loss of episode 67 = 0.29356766\n",
      "Loss of episode 68 = 0.29462788\n",
      "Loss of episode 69 = 0.29120848\n",
      "Loss of episode 70 = 0.2880579\n",
      "Loss of episode 71 = 0.28521892\n",
      "Loss of episode 72 = 0.28115132\n",
      "Loss of episode 73 = 0.27654952\n",
      "Loss of episode 74 = 0.27450788\n",
      "Loss of episode 75 = 0.27285102\n",
      "Loss of episode 76 = 0.27134606\n",
      "Loss of episode 77 = 0.27163517\n",
      "Loss of episode 78 = 0.27172437\n",
      "Loss of episode 79 = 0.27153987\n",
      "Loss of episode 80 = 0.27403486\n",
      "Loss of episode 81 = 0.27596727\n",
      "Loss of episode 82 = 0.27383918\n",
      "Loss of episode 83 = 0.26978865\n",
      "Loss of episode 84 = 0.26731193\n",
      "Loss of episode 85 = 0.2647431\n",
      "Loss of episode 86 = 0.26507363\n",
      "Loss of episode 87 = 0.2644818\n",
      "Loss of episode 88 = 0.26403746\n",
      "Loss of episode 89 = 0.2640577\n",
      "Loss of episode 90 = 0.2643574\n",
      "Loss of episode 91 = 0.26268187\n",
      "Loss of episode 92 = 0.25966287\n",
      "Loss of episode 93 = 0.25675538\n",
      "Loss of episode 94 = 0.25505885\n",
      "Loss of episode 95 = 0.25380236\n",
      "Loss of episode 96 = 0.2533721\n",
      "Loss of episode 97 = 0.2526647\n",
      "Loss of episode 98 = 0.25167823\n",
      "Loss of episode 99 = 0.24977864\n",
      "Loss of episode 100 = 0.24694192\n",
      "Loss of episode 101 = 0.24512967\n",
      "Loss of episode 102 = 0.24422199\n",
      "Loss of episode 103 = 0.24409713\n",
      "Loss of episode 104 = 0.24548657\n",
      "Loss of episode 105 = 0.24626747\n",
      "Loss of episode 106 = 0.24627627\n",
      "Loss of episode 107 = 0.24535042\n",
      "Loss of episode 108 = 0.2421177\n",
      "Loss of episode 109 = 0.23756249\n",
      "Loss of episode 110 = 0.23533235\n",
      "Loss of episode 111 = 0.23466608\n",
      "Loss of episode 112 = 0.23436877\n",
      "Loss of episode 113 = 0.23466285\n",
      "Loss of episode 114 = 0.23498496\n",
      "Loss of episode 115 = 0.23550642\n",
      "Loss of episode 116 = 0.23600148\n",
      "Loss of episode 117 = 0.23668854\n",
      "Loss of episode 118 = 0.236874\n",
      "Loss of episode 119 = 0.23844677\n",
      "Loss of episode 120 = 0.240309\n",
      "Loss of episode 121 = 0.23833951\n",
      "Loss of episode 122 = 0.23239578\n",
      "Loss of episode 123 = 0.22707264\n",
      "Loss of episode 124 = 0.22252877\n",
      "Loss of episode 125 = 0.22020915\n",
      "Loss of episode 126 = 0.21911827\n",
      "Loss of episode 127 = 0.21809973\n",
      "Loss of episode 128 = 0.22042464\n",
      "Loss of episode 129 = 0.2220415\n",
      "Loss of episode 130 = 0.22340815\n",
      "Loss of episode 131 = 0.22031651\n",
      "Loss of episode 132 = 0.21681648\n",
      "Loss of episode 133 = 0.21411026\n",
      "Loss of episode 134 = 0.21304327\n",
      "Loss of episode 135 = 0.21180384\n",
      "Loss of episode 136 = 0.21137214\n",
      "Loss of episode 137 = 0.21169955\n",
      "Loss of episode 138 = 0.21146551\n",
      "Loss of episode 139 = 0.21338712\n",
      "Loss of episode 140 = 0.21416578\n",
      "Loss of episode 141 = 0.21257363\n",
      "Loss of episode 142 = 0.21139826\n",
      "Loss of episode 143 = 0.2096596\n",
      "Loss of episode 144 = 0.20955272\n",
      "Loss of episode 145 = 0.20977211\n",
      "Loss of episode 146 = 0.20931062\n",
      "Loss of episode 147 = 0.2089418\n",
      "Loss of episode 148 = 0.20780489\n",
      "Loss of episode 149 = 0.20821825\n",
      "Loss of episode 150 = 0.20879251\n",
      "Loss of episode 151 = 0.20941597\n",
      "Loss of episode 152 = 0.20926358\n",
      "Loss of episode 153 = 0.20725642\n",
      "Loss of episode 154 = 0.20475857\n",
      "Loss of episode 155 = 0.2030027\n",
      "Loss of episode 156 = 0.20051531\n",
      "Loss of episode 157 = 0.19977744\n",
      "Loss of episode 158 = 0.19713114\n",
      "Loss of episode 159 = 0.19426264\n",
      "Loss of episode 160 = 0.19049315\n",
      "Loss of episode 161 = 0.19000438\n",
      "Loss of episode 162 = 0.18925455\n",
      "Loss of episode 163 = 0.1901371\n",
      "Loss of episode 164 = 0.19220161\n",
      "Loss of episode 165 = 0.19495435\n",
      "Loss of episode 166 = 0.19573143\n",
      "Loss of episode 167 = 0.19418378\n",
      "Loss of episode 168 = 0.190101\n",
      "Loss of episode 169 = 0.18717702\n",
      "Loss of episode 170 = 0.18556696\n",
      "Loss of episode 171 = 0.1840389\n",
      "Loss of episode 172 = 0.18421136\n",
      "Loss of episode 173 = 0.18342586\n",
      "Loss of episode 174 = 0.18146122\n",
      "Loss of episode 175 = 0.18108918\n",
      "Loss of episode 176 = 0.18118285\n",
      "Loss of episode 177 = 0.18004449\n",
      "Loss of episode 178 = 0.18131432\n",
      "Loss of episode 179 = 0.18057066\n",
      "Loss of episode 180 = 0.17893213\n",
      "Loss of episode 181 = 0.17795767\n",
      "Loss of episode 182 = 0.17725366\n",
      "Loss of episode 183 = 0.17645468\n",
      "Loss of episode 184 = 0.17554283\n",
      "Loss of episode 185 = 0.17579217\n",
      "Loss of episode 186 = 0.17601024\n",
      "Loss of episode 187 = 0.1756738\n",
      "Loss of episode 188 = 0.17714489\n",
      "Loss of episode 189 = 0.17586802\n",
      "Loss of episode 190 = 0.17486405\n",
      "Loss of episode 191 = 0.17387371\n",
      "Loss of episode 192 = 0.1724823\n",
      "Loss of episode 193 = 0.17430162\n",
      "Loss of episode 194 = 0.17735156\n",
      "Loss of episode 195 = 0.17880817\n",
      "Loss of episode 196 = 0.17916846\n",
      "Loss of episode 197 = 0.17746878\n",
      "Loss of episode 198 = 0.17525\n",
      "Loss of episode 199 = 0.17029001\n",
      "Loss of episode 200 = 0.16769478\n",
      "Loss of episode 201 = 0.16710982\n",
      "Loss of episode 202 = 0.1675635\n",
      "Loss of episode 203 = 0.16853389\n",
      "Loss of episode 204 = 0.16841458\n",
      "Loss of episode 205 = 0.16848408\n",
      "Loss of episode 206 = 0.16870846\n",
      "Loss of episode 207 = 0.16909346\n",
      "Loss of episode 208 = 0.16940904\n",
      "Loss of episode 209 = 0.17078021\n",
      "Loss of episode 210 = 0.170933\n",
      "Loss of episode 211 = 0.16975756\n",
      "Loss of episode 212 = 0.16546844\n",
      "Loss of episode 213 = 0.16067466\n",
      "Loss of episode 214 = 0.15653734\n",
      "Loss of episode 215 = 0.15593523\n",
      "Loss of episode 216 = 0.15577067\n",
      "Loss of episode 217 = 0.15760128\n",
      "Loss of episode 218 = 0.15894692\n",
      "Loss of episode 219 = 0.16181536\n",
      "Loss of episode 220 = 0.16220741\n",
      "Loss of episode 221 = 0.1625909\n",
      "Loss of episode 222 = 0.16111986\n",
      "Loss of episode 223 = 0.15925911\n",
      "Loss of episode 224 = 0.15744413\n",
      "Loss of episode 225 = 0.15698951\n",
      "Loss of episode 226 = 0.15726355\n",
      "Loss of episode 227 = 0.15865901\n",
      "Loss of episode 228 = 0.15889184\n",
      "Loss of episode 229 = 0.15892927\n",
      "Loss of episode 230 = 0.1576869\n",
      "Loss of episode 231 = 0.15629815\n",
      "Loss of episode 232 = 0.15577848\n",
      "Loss of episode 233 = 0.15501294\n",
      "Loss of episode 234 = 0.15666977\n",
      "Loss of episode 235 = 0.15701051\n",
      "Loss of episode 236 = 0.15534024\n",
      "Loss of episode 237 = 0.15133989\n",
      "Loss of episode 238 = 0.14784397\n",
      "Loss of episode 239 = 0.14552237\n",
      "Loss of episode 240 = 0.14500195\n",
      "Loss of episode 241 = 0.14578532\n",
      "Loss of episode 242 = 0.14636703\n",
      "Loss of episode 243 = 0.14714181\n",
      "Loss of episode 244 = 0.14704506\n",
      "Loss of episode 245 = 0.14852025\n",
      "Loss of episode 246 = 0.14872296\n",
      "Loss of episode 247 = 0.15030755\n",
      "Loss of episode 248 = 0.15019383\n",
      "Loss of episode 249 = 0.1501645\n",
      "Loss of episode 250 = 0.15052228\n",
      "Loss of episode 251 = 0.15053444\n",
      "Loss of episode 252 = 0.14908311\n",
      "Loss of episode 253 = 0.14773703\n",
      "Loss of episode 254 = 0.14465873\n",
      "Loss of episode 255 = 0.14317553\n",
      "Loss of episode 256 = 0.14278372\n",
      "Loss of episode 257 = 0.14237379\n",
      "Loss of episode 258 = 0.14249681\n",
      "Loss of episode 259 = 0.14290826\n",
      "Loss of episode 260 = 0.142162\n",
      "Loss of episode 261 = 0.14177723\n",
      "Loss of episode 262 = 0.14468922\n",
      "Loss of episode 263 = 0.14211263\n",
      "Loss of episode 264 = 0.13980506\n",
      "Loss of episode 265 = 0.13695239\n",
      "Loss of episode 266 = 0.13566437\n",
      "Loss of episode 267 = 0.13740712\n",
      "Loss of episode 268 = 0.13957766\n",
      "Loss of episode 269 = 0.13839965\n",
      "Loss of episode 270 = 0.13436812\n",
      "Loss of episode 271 = 0.13249584\n",
      "Loss of episode 272 = 0.13154145\n",
      "Loss of episode 273 = 0.13179962\n",
      "Loss of episode 274 = 0.1317925\n",
      "Loss of episode 275 = 0.13122235\n",
      "Loss of episode 276 = 0.1308207\n",
      "Loss of episode 277 = 0.13198875\n",
      "Loss of episode 278 = 0.13433033\n",
      "Loss of episode 279 = 0.13850825\n",
      "Loss of episode 280 = 0.14482881\n",
      "Loss of episode 281 = 0.14580616\n",
      "Loss of episode 282 = 0.14268301\n",
      "Loss of episode 283 = 0.13950405\n",
      "Loss of episode 284 = 0.13560972\n",
      "Loss of episode 285 = 0.13498567\n",
      "Loss of episode 286 = 0.13329692\n",
      "Loss of episode 287 = 0.13269074\n",
      "Loss of episode 288 = 0.13233894\n",
      "Loss of episode 289 = 0.13167499\n",
      "Loss of episode 290 = 0.13058613\n",
      "Loss of episode 291 = 0.12791836\n",
      "Loss of episode 292 = 0.12573971\n",
      "Loss of episode 293 = 0.1248339\n",
      "Loss of episode 294 = 0.12448209\n",
      "Loss of episode 295 = 0.1242236\n",
      "Loss of episode 296 = 0.12526983\n",
      "Loss of episode 297 = 0.12866105\n",
      "Loss of episode 298 = 0.12970454\n",
      "Loss of episode 299 = 0.13082068\n",
      "Loss of episode 300 = 0.13094956\n",
      "Loss of episode 301 = 0.12920927\n",
      "Loss of episode 302 = 0.12608978\n",
      "Loss of episode 303 = 0.12389125\n",
      "Loss of episode 304 = 0.12167365\n",
      "Loss of episode 305 = 0.1202467\n",
      "Loss of episode 306 = 0.11954164\n",
      "Loss of episode 307 = 0.11954166\n",
      "Loss of episode 308 = 0.119209\n",
      "Loss of episode 309 = 0.12253652\n",
      "Loss of episode 310 = 0.12292092\n",
      "Loss of episode 311 = 0.12192186\n",
      "Loss of episode 312 = 0.11901612\n",
      "Loss of episode 313 = 0.118065\n",
      "Loss of episode 314 = 0.11900927\n",
      "Loss of episode 315 = 0.12004981\n",
      "Loss of episode 316 = 0.12090916\n",
      "Loss of episode 317 = 0.12300733\n",
      "Loss of episode 318 = 0.12513642\n",
      "Loss of episode 319 = 0.13001972\n",
      "Loss of episode 320 = 0.13264802\n",
      "Loss of episode 321 = 0.1291673\n",
      "Loss of episode 322 = 0.12323337\n",
      "Loss of episode 323 = 0.1181724\n",
      "Loss of episode 324 = 0.11248043\n",
      "Loss of episode 325 = 0.10839785\n",
      "Loss of episode 326 = 0.105974264\n",
      "Loss of episode 327 = 0.10540557\n",
      "Loss of episode 328 = 0.10628339\n",
      "Loss of episode 329 = 0.106880866\n",
      "Loss of episode 330 = 0.10804957\n",
      "Loss of episode 331 = 0.10908661\n",
      "Loss of episode 332 = 0.11209615\n",
      "Loss of episode 333 = 0.115264975\n",
      "Loss of episode 334 = 0.11998377\n",
      "Loss of episode 335 = 0.12346776\n",
      "Loss of episode 336 = 0.12548022\n",
      "Loss of episode 337 = 0.12432628\n",
      "Loss of episode 338 = 0.11902417\n",
      "Loss of episode 339 = 0.11502604\n",
      "Loss of episode 340 = 0.11090599\n",
      "Loss of episode 341 = 0.10969185\n",
      "Loss of episode 342 = 0.110216275\n",
      "Loss of episode 343 = 0.11173599\n",
      "Loss of episode 344 = 0.113168895\n",
      "Loss of episode 345 = 0.11442968\n",
      "Loss of episode 346 = 0.11589834\n",
      "Loss of episode 347 = 0.11836799\n",
      "Loss of episode 348 = 0.119730316\n",
      "Loss of episode 349 = 0.11902613\n",
      "Loss of episode 350 = 0.119348414\n",
      "Loss of episode 351 = 0.118014894\n",
      "Loss of episode 352 = 0.11702508\n",
      "Loss of episode 353 = 0.11534171\n",
      "Loss of episode 354 = 0.11336932\n",
      "Loss of episode 355 = 0.11209403\n",
      "Loss of episode 356 = 0.11076807\n",
      "Loss of episode 357 = 0.10939925\n",
      "Loss of episode 358 = 0.10776581\n",
      "Loss of episode 359 = 0.1075215\n",
      "Loss of episode 360 = 0.10710932\n",
      "Loss of episode 361 = 0.10799529\n",
      "Loss of episode 362 = 0.107866146\n",
      "Loss of episode 363 = 0.10801129\n",
      "Loss of episode 364 = 0.10762754\n",
      "Loss of episode 365 = 0.11053198\n",
      "Loss of episode 366 = 0.112700015\n",
      "Loss of episode 367 = 0.11661987\n",
      "Loss of episode 368 = 0.11926703\n",
      "Loss of episode 369 = 0.11898642\n",
      "Loss of episode 370 = 0.120112024\n",
      "Loss of episode 371 = 0.1207643\n",
      "Loss of episode 372 = 0.12079056\n",
      "Loss of episode 373 = 0.118704\n",
      "Loss of episode 374 = 0.118016414\n",
      "Loss of episode 375 = 0.11733091\n",
      "Loss of episode 376 = 0.11903078\n",
      "Loss of episode 377 = 0.116573356\n",
      "Loss of episode 378 = 0.11211866\n",
      "Loss of episode 379 = 0.10891378\n",
      "Loss of episode 380 = 0.10641058\n",
      "Loss of episode 381 = 0.105458304\n",
      "Loss of episode 382 = 0.10418977\n",
      "Loss of episode 383 = 0.10393783\n",
      "Loss of episode 384 = 0.10527136\n",
      "Loss of episode 385 = 0.10624983\n",
      "Loss of episode 386 = 0.10808999\n",
      "Loss of episode 387 = 0.11074609\n",
      "Loss of episode 388 = 0.111801095\n",
      "Loss of episode 389 = 0.11375307\n",
      "Loss of episode 390 = 0.11725337\n",
      "Loss of episode 391 = 0.11969187\n",
      "Loss of episode 392 = 0.117518716\n",
      "Loss of episode 393 = 0.11482119\n",
      "Loss of episode 394 = 0.1141728\n",
      "Loss of episode 395 = 0.11541397\n",
      "Loss of episode 396 = 0.11706371\n",
      "Loss of episode 397 = 0.11697437\n",
      "Loss of episode 398 = 0.11618341\n",
      "Loss of episode 399 = 0.11511447\n",
      "Loss of episode 400 = 0.113680586\n",
      "Loss of episode 401 = 0.1115623\n",
      "Loss of episode 402 = 0.11044807\n",
      "Loss of episode 403 = 0.10906083\n",
      "Loss of episode 404 = 0.104575016\n",
      "Loss of episode 405 = 0.10173429\n",
      "Loss of episode 406 = 0.10071791\n",
      "Loss of episode 407 = 0.09952484\n",
      "Loss of episode 408 = 0.09964794\n",
      "Loss of episode 409 = 0.10109209\n",
      "Loss of episode 410 = 0.104647025\n",
      "Loss of episode 411 = 0.1083247\n",
      "Loss of episode 412 = 0.10968055\n",
      "Loss of episode 413 = 0.110509135\n",
      "Loss of episode 414 = 0.10646497\n",
      "Loss of episode 415 = 0.10189545\n",
      "Loss of episode 416 = 0.09900124\n",
      "Loss of episode 417 = 0.09766701\n",
      "Loss of episode 418 = 0.09764906\n",
      "Loss of episode 419 = 0.09792749\n",
      "Loss of episode 420 = 0.09784791\n",
      "Loss of episode 421 = 0.098762445\n",
      "Loss of episode 422 = 0.10036817\n",
      "Loss of episode 423 = 0.10225327\n",
      "Loss of episode 424 = 0.103620924\n",
      "Loss of episode 425 = 0.10596137\n",
      "Loss of episode 426 = 0.10837928\n",
      "Loss of episode 427 = 0.110583484\n",
      "Loss of episode 428 = 0.11173976\n",
      "Loss of episode 429 = 0.114242464\n",
      "Loss of episode 430 = 0.11379584\n",
      "Loss of episode 431 = 0.11467368\n",
      "Loss of episode 432 = 0.11191577\n",
      "Loss of episode 433 = 0.10802687\n",
      "Loss of episode 434 = 0.10488094\n",
      "Loss of episode 435 = 0.102480516\n",
      "Loss of episode 436 = 0.10053757\n",
      "Loss of episode 437 = 0.10061812\n",
      "Loss of episode 438 = 0.10026625\n",
      "Loss of episode 439 = 0.100621805\n",
      "Loss of episode 440 = 0.101267524\n",
      "Loss of episode 441 = 0.10402204\n",
      "Loss of episode 442 = 0.105993874\n",
      "Loss of episode 443 = 0.10738772\n",
      "Loss of episode 444 = 0.10675011\n",
      "Loss of episode 445 = 0.10640957\n",
      "Loss of episode 446 = 0.10593923\n",
      "Loss of episode 447 = 0.1040071\n",
      "Loss of episode 448 = 0.10425361\n",
      "Loss of episode 449 = 0.10073014\n",
      "Loss of episode 450 = 0.09806839\n",
      "Loss of episode 451 = 0.09738464\n",
      "Loss of episode 452 = 0.098771654\n",
      "Loss of episode 453 = 0.09940357\n",
      "Loss of episode 454 = 0.10125692\n",
      "Loss of episode 455 = 0.1042825\n",
      "Loss of episode 456 = 0.10340031\n",
      "Loss of episode 457 = 0.10117418\n",
      "Loss of episode 458 = 0.0991337\n",
      "Loss of episode 459 = 0.096373476\n",
      "Loss of episode 460 = 0.094687864\n",
      "Loss of episode 461 = 0.09418691\n",
      "Loss of episode 462 = 0.09337015\n",
      "Loss of episode 463 = 0.093673296\n",
      "Loss of episode 464 = 0.09529503\n",
      "Loss of episode 465 = 0.096491165\n",
      "Loss of episode 466 = 0.09577342\n",
      "Loss of episode 467 = 0.09527233\n",
      "Loss of episode 468 = 0.095433\n",
      "Loss of episode 469 = 0.0961883\n",
      "Loss of episode 470 = 0.09728092\n",
      "Loss of episode 471 = 0.0973235\n",
      "Loss of episode 472 = 0.09688824\n",
      "Loss of episode 473 = 0.09436273\n",
      "Loss of episode 474 = 0.09344577\n",
      "Loss of episode 475 = 0.093870185\n",
      "Loss of episode 476 = 0.09497175\n",
      "Loss of episode 477 = 0.09515211\n",
      "Loss of episode 478 = 0.09532804\n",
      "Loss of episode 479 = 0.09533735\n",
      "Loss of episode 480 = 0.09371078\n",
      "Loss of episode 481 = 0.09272005\n",
      "Loss of episode 482 = 0.092737064\n",
      "Loss of episode 483 = 0.09169134\n",
      "Loss of episode 484 = 0.09147061\n",
      "Loss of episode 485 = 0.09250894\n",
      "Loss of episode 486 = 0.09451497\n",
      "Loss of episode 487 = 0.09555886\n",
      "Loss of episode 488 = 0.09709906\n",
      "Loss of episode 489 = 0.09709697\n",
      "Loss of episode 490 = 0.097956434\n",
      "Loss of episode 491 = 0.09833733\n",
      "Loss of episode 492 = 0.09712076\n",
      "Loss of episode 493 = 0.09568526\n",
      "Loss of episode 494 = 0.09314797\n",
      "Loss of episode 495 = 0.08906822\n",
      "Loss of episode 496 = 0.0866652\n",
      "Loss of episode 497 = 0.08526104\n",
      "Loss of episode 498 = 0.08412652\n",
      "Loss of episode 499 = 0.08399617\n",
      "Loss of episode 500 = 0.08392737\n",
      "Loss of episode 501 = 0.08536282\n",
      "Loss of episode 502 = 0.08584994\n",
      "Loss of episode 503 = 0.087907255\n",
      "Loss of episode 504 = 0.08898445\n",
      "Loss of episode 505 = 0.09013925\n",
      "Loss of episode 506 = 0.09014248\n",
      "Loss of episode 507 = 0.08820353\n",
      "Loss of episode 508 = 0.086813085\n",
      "Loss of episode 509 = 0.086192384\n",
      "Loss of episode 510 = 0.08594892\n",
      "Loss of episode 511 = 0.084992215\n",
      "Loss of episode 512 = 0.085249156\n",
      "Loss of episode 513 = 0.08541831\n",
      "Loss of episode 514 = 0.08627219\n",
      "Loss of episode 515 = 0.08633308\n",
      "Loss of episode 516 = 0.08625192\n",
      "Loss of episode 517 = 0.08727394\n",
      "Loss of episode 518 = 0.08741794\n",
      "Loss of episode 519 = 0.086608104\n",
      "Loss of episode 520 = 0.08727528\n",
      "Loss of episode 521 = 0.08643171\n",
      "Loss of episode 522 = 0.084813\n",
      "Loss of episode 523 = 0.084048964\n",
      "Loss of episode 524 = 0.08345223\n",
      "Loss of episode 525 = 0.0840773\n",
      "Loss of episode 526 = 0.08507787\n",
      "Loss of episode 527 = 0.08688972\n",
      "Loss of episode 528 = 0.08741693\n",
      "Loss of episode 529 = 0.0892641\n",
      "Loss of episode 530 = 0.09169066\n",
      "Loss of episode 531 = 0.09394481\n",
      "Loss of episode 532 = 0.096620604\n",
      "Loss of episode 533 = 0.09599345\n",
      "Loss of episode 534 = 0.09247544\n",
      "Loss of episode 535 = 0.08804554\n",
      "Loss of episode 536 = 0.08595965\n",
      "Loss of episode 537 = 0.08268747\n",
      "Loss of episode 538 = 0.0809493\n",
      "Loss of episode 539 = 0.07881941\n",
      "Loss of episode 540 = 0.07798603\n",
      "Loss of episode 541 = 0.077055745\n",
      "Loss of episode 542 = 0.0773476\n",
      "Loss of episode 543 = 0.07877824\n",
      "Loss of episode 544 = 0.07872856\n",
      "Loss of episode 545 = 0.07995677\n",
      "Loss of episode 546 = 0.08074234\n",
      "Loss of episode 547 = 0.081234775\n",
      "Loss of episode 548 = 0.083543964\n",
      "Loss of episode 549 = 0.082980946\n",
      "Loss of episode 550 = 0.083494\n",
      "Loss of episode 551 = 0.0850945\n",
      "Loss of episode 552 = 0.08520906\n",
      "Loss of episode 553 = 0.08418994\n",
      "Loss of episode 554 = 0.08380651\n",
      "Loss of episode 555 = 0.08102517\n",
      "Loss of episode 556 = 0.07938018\n",
      "Loss of episode 557 = 0.07861914\n",
      "Loss of episode 558 = 0.077553645\n",
      "Loss of episode 559 = 0.07648541\n",
      "Loss of episode 560 = 0.07726567\n",
      "Loss of episode 561 = 0.07723715\n",
      "Loss of episode 562 = 0.07769304\n",
      "Loss of episode 563 = 0.078943394\n",
      "Loss of episode 564 = 0.08131075\n",
      "Loss of episode 565 = 0.08419645\n",
      "Loss of episode 566 = 0.08707109\n",
      "Loss of episode 567 = 0.08719267\n",
      "Loss of episode 568 = 0.0853143\n",
      "Loss of episode 569 = 0.08514734\n",
      "Loss of episode 570 = 0.08185534\n",
      "Loss of episode 571 = 0.082126826\n",
      "Loss of episode 572 = 0.08080367\n",
      "Loss of episode 573 = 0.07983518\n",
      "Loss of episode 574 = 0.07916397\n",
      "Loss of episode 575 = 0.07920144\n",
      "Loss of episode 576 = 0.079133466\n",
      "Loss of episode 577 = 0.07864832\n",
      "Loss of episode 578 = 0.07800584\n",
      "Loss of episode 579 = 0.078384966\n",
      "Loss of episode 580 = 0.07828978\n",
      "Loss of episode 581 = 0.07738907\n",
      "Loss of episode 582 = 0.077191375\n",
      "Loss of episode 583 = 0.07583486\n",
      "Loss of episode 584 = 0.07440752\n",
      "Loss of episode 585 = 0.07445682\n",
      "Loss of episode 586 = 0.074930035\n",
      "Loss of episode 587 = 0.07554608\n",
      "Loss of episode 588 = 0.07886928\n",
      "Loss of episode 589 = 0.08093262\n",
      "Loss of episode 590 = 0.08269289\n",
      "Loss of episode 591 = 0.083437234\n",
      "Loss of episode 592 = 0.083095394\n",
      "Loss of episode 593 = 0.08096645\n",
      "Loss of episode 594 = 0.080226876\n",
      "Loss of episode 595 = 0.07796912\n",
      "Loss of episode 596 = 0.07519863\n",
      "Loss of episode 597 = 0.07351314\n",
      "Loss of episode 598 = 0.07216634\n",
      "Loss of episode 599 = 0.07075919\n",
      "Loss of episode 600 = 0.07021831\n",
      "Loss of episode 601 = 0.0699209\n",
      "Loss of episode 602 = 0.06987053\n",
      "Loss of episode 603 = 0.07002587\n",
      "Loss of episode 604 = 0.070881195\n",
      "Loss of episode 605 = 0.072732195\n",
      "Loss of episode 606 = 0.07278587\n",
      "Loss of episode 607 = 0.07327656\n",
      "Loss of episode 608 = 0.07392518\n",
      "Loss of episode 609 = 0.073891126\n",
      "Loss of episode 610 = 0.07300441\n",
      "Loss of episode 611 = 0.0720695\n",
      "Loss of episode 612 = 0.071019426\n",
      "Loss of episode 613 = 0.07077588\n",
      "Loss of episode 614 = 0.070089795\n",
      "Loss of episode 615 = 0.070772894\n",
      "Loss of episode 616 = 0.07096098\n",
      "Loss of episode 617 = 0.071571246\n",
      "Loss of episode 618 = 0.07176496\n",
      "Loss of episode 619 = 0.07321051\n",
      "Loss of episode 620 = 0.07373231\n",
      "Loss of episode 621 = 0.07586494\n",
      "Loss of episode 622 = 0.07792154\n",
      "Loss of episode 623 = 0.07890968\n",
      "Loss of episode 624 = 0.079223126\n",
      "Loss of episode 625 = 0.07804058\n",
      "Loss of episode 626 = 0.07518141\n",
      "Loss of episode 627 = 0.07404186\n",
      "Loss of episode 628 = 0.07258074\n",
      "Loss of episode 629 = 0.07137238\n",
      "Loss of episode 630 = 0.06922146\n",
      "Loss of episode 631 = 0.069118865\n",
      "Loss of episode 632 = 0.0699008\n",
      "Loss of episode 633 = 0.0709988\n",
      "Loss of episode 634 = 0.07116893\n",
      "Loss of episode 635 = 0.07255632\n",
      "Loss of episode 636 = 0.072807714\n",
      "Loss of episode 637 = 0.072819754\n",
      "Loss of episode 638 = 0.07328895\n",
      "Loss of episode 639 = 0.07333833\n",
      "Loss of episode 640 = 0.07468787\n",
      "Loss of episode 641 = 0.076476336\n",
      "Loss of episode 642 = 0.078355715\n",
      "Loss of episode 643 = 0.07880798\n",
      "Loss of episode 644 = 0.076946184\n",
      "Loss of episode 645 = 0.0747933\n",
      "Loss of episode 646 = 0.07180745\n",
      "Loss of episode 647 = 0.070069104\n",
      "Loss of episode 648 = 0.06989222\n",
      "Loss of episode 649 = 0.06934141\n",
      "Loss of episode 650 = 0.06910176\n",
      "Loss of episode 651 = 0.069070265\n",
      "Loss of episode 652 = 0.06885517\n",
      "Loss of episode 653 = 0.06752827\n",
      "Loss of episode 654 = 0.068245225\n",
      "Loss of episode 655 = 0.0684236\n",
      "Loss of episode 656 = 0.06902984\n",
      "Loss of episode 657 = 0.06944859\n",
      "Loss of episode 658 = 0.07115879\n",
      "Loss of episode 659 = 0.07280185\n",
      "Loss of episode 660 = 0.075076975\n",
      "Loss of episode 661 = 0.0768233\n",
      "Loss of episode 662 = 0.07560584\n",
      "Loss of episode 663 = 0.07641607\n",
      "Loss of episode 664 = 0.07799116\n",
      "Loss of episode 665 = 0.07679432\n",
      "Loss of episode 666 = 0.075789995\n",
      "Loss of episode 667 = 0.074949905\n",
      "Loss of episode 668 = 0.071443684\n",
      "Loss of episode 669 = 0.069898315\n",
      "Loss of episode 670 = 0.067808226\n",
      "Loss of episode 671 = 0.06773738\n",
      "Loss of episode 672 = 0.06646943\n",
      "Loss of episode 673 = 0.066617586\n",
      "Loss of episode 674 = 0.06730158\n",
      "Loss of episode 675 = 0.06735048\n",
      "Loss of episode 676 = 0.06685783\n",
      "Loss of episode 677 = 0.06841988\n",
      "Loss of episode 678 = 0.0697497\n",
      "Loss of episode 679 = 0.070666306\n",
      "Loss of episode 680 = 0.07151309\n",
      "Loss of episode 681 = 0.07233548\n",
      "Loss of episode 682 = 0.0726672\n",
      "Loss of episode 683 = 0.073422246\n",
      "Loss of episode 684 = 0.07445467\n",
      "Loss of episode 685 = 0.07557007\n",
      "Loss of episode 686 = 0.07737466\n",
      "Loss of episode 687 = 0.07942815\n",
      "Loss of episode 688 = 0.078430004\n",
      "Loss of episode 689 = 0.07854153\n",
      "Loss of episode 690 = 0.07952811\n",
      "Loss of episode 691 = 0.07968795\n",
      "Loss of episode 692 = 0.079849325\n",
      "Loss of episode 693 = 0.076861106\n",
      "Loss of episode 694 = 0.07619405\n",
      "Loss of episode 695 = 0.0726777\n",
      "Loss of episode 696 = 0.06990264\n",
      "Loss of episode 697 = 0.06765489\n",
      "Loss of episode 698 = 0.067205675\n",
      "Loss of episode 699 = 0.066637784\n",
      "Loss of episode 700 = 0.0662924\n",
      "Loss of episode 701 = 0.06692793\n",
      "Loss of episode 702 = 0.06816976\n",
      "Loss of episode 703 = 0.06817753\n",
      "Loss of episode 704 = 0.06829737\n",
      "Loss of episode 705 = 0.06919862\n",
      "Loss of episode 706 = 0.07038395\n",
      "Loss of episode 707 = 0.072231\n",
      "Loss of episode 708 = 0.0716776\n",
      "Loss of episode 709 = 0.07128938\n",
      "Loss of episode 710 = 0.0737783\n",
      "Loss of episode 711 = 0.07317652\n",
      "Loss of episode 712 = 0.07218556\n",
      "Loss of episode 713 = 0.07266567\n",
      "Loss of episode 714 = 0.07179199\n",
      "Loss of episode 715 = 0.071724474\n",
      "Loss of episode 716 = 0.07071041\n",
      "Loss of episode 717 = 0.071452305\n",
      "Loss of episode 718 = 0.070453435\n",
      "Loss of episode 719 = 0.07044748\n",
      "Loss of episode 720 = 0.07057901\n",
      "Loss of episode 721 = 0.07021693\n",
      "Loss of episode 722 = 0.06937249\n",
      "Loss of episode 723 = 0.069797784\n",
      "Loss of episode 724 = 0.070183024\n",
      "Loss of episode 725 = 0.07076539\n",
      "Loss of episode 726 = 0.071290135\n",
      "Loss of episode 727 = 0.07111382\n",
      "Loss of episode 728 = 0.07048995\n",
      "Loss of episode 729 = 0.0695266\n",
      "Loss of episode 730 = 0.06968665\n",
      "Loss of episode 731 = 0.07056672\n",
      "Loss of episode 732 = 0.07127055\n",
      "Loss of episode 733 = 0.07288243\n",
      "Loss of episode 734 = 0.07382543\n",
      "Loss of episode 735 = 0.074592724\n",
      "Loss of episode 736 = 0.07231459\n",
      "Loss of episode 737 = 0.07167484\n",
      "Loss of episode 738 = 0.07248611\n",
      "Loss of episode 739 = 0.07289999\n",
      "Loss of episode 740 = 0.07336781\n",
      "Loss of episode 741 = 0.0751201\n",
      "Loss of episode 742 = 0.076129854\n",
      "Loss of episode 743 = 0.076182656\n",
      "Loss of episode 744 = 0.07549403\n",
      "Loss of episode 745 = 0.07257366\n",
      "Loss of episode 746 = 0.06998351\n",
      "Loss of episode 747 = 0.06834829\n",
      "Loss of episode 748 = 0.06833054\n",
      "Loss of episode 749 = 0.06701905\n",
      "Loss of episode 750 = 0.066162266\n",
      "Loss of episode 751 = 0.06508131\n",
      "Loss of episode 752 = 0.06469312\n",
      "Loss of episode 753 = 0.0653356\n",
      "Loss of episode 754 = 0.06503387\n",
      "Loss of episode 755 = 0.065730184\n",
      "Loss of episode 756 = 0.0657803\n",
      "Loss of episode 757 = 0.06623044\n",
      "Loss of episode 758 = 0.069433756\n",
      "Loss of episode 759 = 0.07113245\n",
      "Loss of episode 760 = 0.07165276\n",
      "Loss of episode 761 = 0.07213404\n",
      "Loss of episode 762 = 0.07193215\n",
      "Loss of episode 763 = 0.07172063\n",
      "Loss of episode 764 = 0.07145836\n",
      "Loss of episode 765 = 0.070627995\n",
      "Loss of episode 766 = 0.069139495\n",
      "Loss of episode 767 = 0.06860614\n",
      "Loss of episode 768 = 0.06760042\n",
      "Loss of episode 769 = 0.0665003\n",
      "Loss of episode 770 = 0.06546613\n",
      "Loss of episode 771 = 0.06558985\n",
      "Loss of episode 772 = 0.066092364\n",
      "Loss of episode 773 = 0.06728432\n",
      "Loss of episode 774 = 0.06915088\n",
      "Loss of episode 775 = 0.07178015\n",
      "Loss of episode 776 = 0.07355616\n",
      "Loss of episode 777 = 0.07537527\n",
      "Loss of episode 778 = 0.07596622\n",
      "Loss of episode 779 = 0.07453243\n",
      "Loss of episode 780 = 0.07232955\n",
      "Loss of episode 781 = 0.07021951\n",
      "Loss of episode 782 = 0.06893329\n",
      "Loss of episode 783 = 0.06875621\n",
      "Loss of episode 784 = 0.06691567\n",
      "Loss of episode 785 = 0.06613993\n",
      "Loss of episode 786 = 0.06504644\n",
      "Loss of episode 787 = 0.065478764\n",
      "Loss of episode 788 = 0.065278865\n",
      "Loss of episode 789 = 0.06530581\n",
      "Loss of episode 790 = 0.06622062\n",
      "Loss of episode 791 = 0.06553547\n",
      "Loss of episode 792 = 0.06608082\n",
      "Loss of episode 793 = 0.066564806\n",
      "Loss of episode 794 = 0.06629748\n",
      "Loss of episode 795 = 0.067400865\n",
      "Loss of episode 796 = 0.06782899\n",
      "Loss of episode 797 = 0.06737017\n",
      "Loss of episode 798 = 0.06750391\n",
      "Loss of episode 799 = 0.06740944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fc4746aad0>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Net = AGBCQ(512,state_feature,action_feature).to(device)\n",
    "num_epoch = 800\n",
    "# optimizer = torch.optim.SGD(net.parameters(),lr=0.5)\n",
    "optimizer = torch.optim.Adam(Net.parameters(),lr=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "# loss_func = torch.nn.CrossEntropyLoss()\n",
    "Loss=[]\n",
    "for t in range(num_epoch):\n",
    "    aver_loss = 0\n",
    "    Net.train()\n",
    "    for batch, data in enumerate(data_train_loader):\n",
    "    # for x,y in data_train_loader:\n",
    "        x, y, z = data\n",
    "        # print(y.shape)\n",
    "        Value = Net(x)\n",
    "        # print(Value.shape)\n",
    "        Action_Value = torch.sum(torch.mul(Value, y),dim=-1)\n",
    "        max_Action_Value = torch.max(Value,dim=-1).values\n",
    "        # print(max_Action_Value.shape)\n",
    "        Reward = torch.squeeze(z)\n",
    "\n",
    "        Target = Reward[:,0] + max_Action_Value[:,1]\n",
    "        Q_value = Action_Value[:,0]\n",
    "        # loss = loss_func(Q_value,Target) - 0.001 * torch.mean(Q_value)\n",
    "        loss = loss_func(Value,y) + loss_func(Q_value,Target) \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        aver_loss += loss\n",
    "    aver_loss /= batch\n",
    "    aver_loss=aver_loss.cpu().detach().numpy()\n",
    "    print('Loss of episode %s ='%t,aver_loss)\n",
    "    Loss.append(aver_loss)\n",
    "    # Loss.append(loss.detach().numpy())\n",
    "plt.plot(Loss,color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action acc = 0.6015\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "Loss=[]\n",
    "Truth = []\n",
    "Pred = []\n",
    "aver_loss = 0\n",
    "total_acc = 0\n",
    "total_ctr = 0\n",
    "total_auc = 0\n",
    "Entropy = 0\n",
    "num = 0\n",
    "\n",
    "# Click_pred = np.zeros((int(0.2*max_number_traj),length))\n",
    "# Click_true = np.zeros((int(0.2*max_number_traj),length))\n",
    "\n",
    "for batch, data in enumerate(data_test_loader):\n",
    "# for batch, data in enumerate(data_full_loader):\n",
    "# for x,y in data_train_loader:\n",
    "    Net.eval()\n",
    "    x, y, z = data\n",
    "    Q_value = Net(x)\n",
    "    # print(Q_value)\n",
    "    # print(y)\n",
    "    a = torch.argmax(Q_value[0][0],dim = 0).cpu().data.numpy()\n",
    "    b = torch.argmax(y[0][0],dim = 0).cpu().data.numpy()\n",
    "    # # print(test)\n",
    "    # a = torch.argmax(test,dim = 2).cpu().data.numpy().T[0]\n",
    "    # b = torch.argmax(y,dim = 2).cpu().data.numpy()[0]\n",
    "    # print('-----------')\n",
    "    # print('pred =',a)\n",
    "    # print('true =',b)\n",
    "    aver_acc = a==b\n",
    "    total_acc += aver_acc\n",
    "\n",
    "    # aver_ctr = sum(click_pred==click_true)/length\n",
    "    # total_ctr += aver_ctr\n",
    "\n",
    "\n",
    "# fpr, tpr, thresholds = roc_curve(Click_true.reshape(-1), Click_pred.reshape(-1),pos_label=1)\n",
    "# total_auc = auc(fpr, tpr)\n",
    "# print('CTR AUC =', total_auc)\n",
    "print('Action acc =', total_acc/(0.2*max_number_traj))\n",
    "# print('CTR acc=', total_ctr/(0.2*max_number_traj))\n",
    "# print('Entropy = ', Entropy/(0.2*max_number_traj))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 AUC = {0: 0.7331354968067199, 1: 0.7430117184111662, 2: 0.7493950190394122}\n",
      "step 2 AUC = {0: 0.7363913217874829, 1: 0.7555458496032859, 2: 0.7419047122754923}\n",
      "Average AUC = [0.73476341 0.74927878 0.74564987]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, roc_curve, auc\n",
    "n_classes = 3\n",
    "# length = 8\n",
    "aver_auc = np.zeros(n_classes)\n",
    "total_precision = 0\n",
    "Y_score = np.zeros(((length, int(0.2*max_number_traj), n_classes)))\n",
    "# print(Y_score.shape)\n",
    "Y_label = np.zeros(((length, int(0.2*max_number_traj), n_classes)))\n",
    "for batch, data in enumerate(data_test_loader):\n",
    "    Net.eval()\n",
    "    x, y, z = data\n",
    "    # print(x)\n",
    "    Q_value = Net(x)\n",
    "    y_score = Q_value[0].cpu().data.numpy()\n",
    "    y_label = y[0].cpu().data.numpy()\n",
    "    # print(batch)\n",
    "    for i in range(length):\n",
    "        Y_score[i][batch] = y_score[i]\n",
    "        Y_label[i][batch] = y_label[i]\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "aver_auc = np.zeros(n_classes)\n",
    "for t in range(length):\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(Y_label[t][:, i], Y_score[t][:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        # aver_auc[i] += auc(fpr[i], tpr[i])\n",
    "        aver_auc[i] += roc_auc[i]/length\n",
    "    print('step %s AUC ='%(t+1),roc_auc)\n",
    "print('Average AUC =', aver_auc)\n",
    "    # precision = precision_score(a,b, average=\"micro\")\n",
    "    # total_precision += precision\n",
    "# aver_precesion = total_precision/batch\n",
    "# print(aver_precesion)\n",
    "# print(aver_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: F1 = 0.601 Precision = 0.6028 Recall = 0.6013\n",
      "Step 2: F1 = 0.606 Precision = 0.6081 Recall = 0.6065\n",
      "Aver_F1 = 0.6035 Aver_Precision = 0.6054 Aver_Recall = 0.6039\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "aver_f1 = 0\n",
    "aver_p = 0\n",
    "aver_r = 0\n",
    "\n",
    "# print(y_pred)\n",
    "# print(y_true)\n",
    "for i in range(length):\n",
    "  y_true = np.argmax(Y_label[i],axis = 1)\n",
    "  y_pred = np.argmax(Y_score[i],axis = 1)\n",
    "  f1 = round(f1_score(y_true, y_pred, average='macro' ),4)\n",
    "  p = round(precision_score(y_true, y_pred, average='macro'),4)\n",
    "  r = round(recall_score(y_true, y_pred, average='macro'),4)\n",
    "  aver_f1 += f1/length\n",
    "  aver_p += p/length\n",
    "  aver_r += r/length\n",
    "  print('Step %s:'%(i+1),'F1 =', f1, 'Precision =', p, 'Recall =', r)\n",
    "print('Aver_F1 =', round(aver_f1,4), 'Aver_Precision =', round(aver_p,4), 'Aver_Recall =', round(aver_r,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy_cost =  8000\n",
      "Policy click number =  1175.0\n",
      "Policy_CPC = 6.808510638297872\n"
     ]
    }
   ],
   "source": [
    "\n",
    "True_Click_matrix = np.zeros((8000,2))\n",
    "True_Action_matrix = np.zeros((8000,2))\n",
    "Click_pred_matrix = np.zeros((8000,2))\n",
    "Action_pred_matrix = np.zeros((8000,2))\n",
    "\n",
    "# print(Click_pred_matrix)\n",
    "\n",
    "for batch, data in enumerate(data_test_loader):\n",
    "# for batch, data in enumerate(data_full_loader):\n",
    "# for x,y in data_train_loader:\n",
    "    Net.eval()\n",
    "    x, y, z = data\n",
    "    action_true = torch.squeeze(torch.argmax(y,dim = 2)).cpu().data.numpy()\n",
    "    click_true = torch.squeeze(z).cpu().data.numpy()\n",
    "    True_Click_matrix[batch] = click_true\n",
    "    # True_Action_count += np.sum(action_true, axis = 0)\n",
    "    \n",
    "    action = Net(x)\n",
    "    action_pred = torch.squeeze(torch.argmax(action,dim = 2)).cpu().data.numpy()\n",
    "    # print(action_true == action_pred)\n",
    "    Action_pred_matrix[batch] = (action_true == action_pred)\n",
    "    # click_pred = torch.squeeze(click).cpu().data.numpy()\n",
    "    # Click_pred_matrix[batch] = click_pred\n",
    "True_click = sum(sum(True_Click_matrix))\n",
    "Cost = 8000\n",
    "Click = sum(sum(Action_pred_matrix*True_Click_matrix))\n",
    "cpc = Cost/Click\n",
    "print('Policy_cost = ', Cost)\n",
    "print('Policy click number = ', Click) \n",
    "print('Policy_CPC =', cpc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
