{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "# torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('kuairand_sequence.csv',index_col=0)\n",
    "# data['user_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "\n",
    "max_length = 100\n",
    "num_user = len(data['user_id'].unique())\n",
    "state_feature = len(data.columns) - 1\n",
    "action_feature = 3\n",
    "\n",
    "uid = data['user_id'].unique()\n",
    "uid\n",
    "print(state_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6269\n",
      "6269\n"
     ]
    }
   ],
   "source": [
    "State = np.zeros(((num_user,max_length,state_feature)))\n",
    "Action = np.zeros((num_user,max_length,action_feature))\n",
    "# print(Action)\n",
    "# conversion = np.array(conversion_one_hot)\n",
    "\n",
    "j = 0\n",
    "for i in uid:\n",
    "    state_sequence = np.array(data[data['user_id']==i])[:-1]\n",
    "    # state_sequence = np.array(data[data['user_id']==i].drop(['short','mid','long'],axis=1))\n",
    "    action_sequence = np.array(data[data['user_id']==i][['short','mid','long']])[1:]\n",
    "    # action_sequence = np.array(data[data['user_id']==i][['short','mid','long']])\n",
    "    # print(action_sequence)\n",
    "    state = np.pad(state_sequence,((0,max_length-len(state_sequence)),(0,0)))[:,1:]\n",
    "    action = np.pad(action_sequence,((0,max_length-len(action_sequence)),(0,0)))\n",
    "    State[j] = state\n",
    "    Action[j] = action\n",
    "    j += 1\n",
    "print(len(State))\n",
    "print(len(Action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000\n",
      "80000\n"
     ]
    }
   ],
   "source": [
    "# print(len(con_X))\n",
    "# print(len(non_X))\n",
    "length = 1\n",
    "max_number_traj = 80000\n",
    "# number_traj = 10000\n",
    "number_traj = 0\n",
    "def Select_trajectory(X,Y,number_traj):\n",
    "    Select_states = []\n",
    "    Select_actions = []\n",
    "    # for s in range(number_traj):\n",
    "    while True:\n",
    "        i = random.randint(0,len(X)-1)\n",
    "        select_state = X[i]\n",
    "        select_action = Y[i]\n",
    "        # print(select_state)\n",
    "        # print(select_action)\n",
    "        for k in range(0,max_length):\n",
    "            # print(select_episode[k])\n",
    "            if select_state[k].any() == 0:\n",
    "                max_episode_length = k\n",
    "                break\n",
    "        # print(k)\n",
    "        # print(max_episode_length)\n",
    "        if max_episode_length >= length + 1:\n",
    "            j = random.randint(0,max_episode_length-length-1)\n",
    "            select_state = select_state[j:j+length]\n",
    "            select_action = select_action[j:j+length]\n",
    "            Select_states.append(select_state)\n",
    "            Select_actions.append(select_action)\n",
    "            number_traj += 1\n",
    "        # print(select_trajectory)\n",
    "        if number_traj == max_number_traj:\n",
    "            break\n",
    "    return np.array(Select_states),np.array(Select_actions)\n",
    "\n",
    "X , Y = Select_trajectory(State,Action,number_traj)\n",
    "print(len(X))\n",
    "# Y = np.concatenate((con_Y , non_Y),axis=0)\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2023)\n",
    "per = np.random.permutation(X.shape[0])\n",
    "# print(per)\n",
    "X = X[per]\n",
    "Y = Y[per]\n",
    "# print(sum(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([0.5,0,0]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64000\n",
      "(tensor([[0.6522, 0.5555, 0.0000, 0.0000, 0.0020, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000,\n",
      "         0.3333, 0.2041, 0.1367, 0.0000, 0.0000, 0.2564, 0.3311, 0.1667, 1.0000]],\n",
      "       device='cuda:0'), tensor([[1., 0., 0.]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "def split_data(X,Y,timestep, input_size, output_size):\n",
    "    # dataX = []\n",
    "    # dataY = []\n",
    "\n",
    "    # for traj_index in range(number_traj):\n",
    "    #     X = []\n",
    "    #     Y = []\n",
    "    #     for index in range(length):\n",
    "    #     # dataX.append(data[index: index + timestep][:, 0])\n",
    "    #     # dataY.append(data[index + timestep][0])\n",
    "    #         X.append(data[traj_index][6 * index : 6 * index + 6])\n",
    "    #         Y.append(data[traj_index][6 * index + 6])\n",
    "    #     dataX.append(X)\n",
    "    #     dataY.append(Y)\n",
    "    # dataX = np.array(dataX)\n",
    "    # dataY = np.array(dataY)\n",
    "\n",
    "    train_size = int(np.round(0.8 * X.shape[0]))\n",
    "    print(train_size)\n",
    "\n",
    "    # x_train = dataX[: train_size, :].reshape(-1, timestep,input_size)\n",
    "    # y_train = dataY[: train_size].reshape(-1,timestep, 1)\n",
    "    \n",
    "    # x_test = dataX[train_size:, :].reshape(-1,timestep, input_size)\n",
    "    # y_test = dataY[train_size:].reshape(-1,timestep, 1)\n",
    "\n",
    "    x_train = X[: train_size, :].reshape(-1, timestep,input_size)\n",
    "    y_train = Y[: train_size].reshape(-1,timestep, output_size) \n",
    "\n",
    "    x_test = X[train_size:, :].reshape(-1, timestep,input_size)\n",
    "    y_test = Y[train_size:].reshape(-1,timestep, output_size)\n",
    "\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "X1,Y1,X2,Y2=split_data(X,Y,length,state_feature,action_feature)\n",
    "\n",
    "# print(len(X2))\n",
    "# print(len(X2))\n",
    "X1,Y1=torch.from_numpy(X1).to(torch.float32).to(device),torch.from_numpy(Y1).to(torch.float32).to(device)\n",
    "# print(Y2)\n",
    "X2,Y2=torch.from_numpy(X2).to(torch.float32).to(device),torch.from_numpy(Y2).to(torch.float32).to(device)\n",
    "# X,Y=torch.from_numpy(X).to(torch.float32),torch.from_numpy(Y).to(torch.float32)\n",
    "train_ids = TensorDataset(X1,Y1)\n",
    "test_ids = TensorDataset(X2,Y2)\n",
    "# data_ids = TensorDataset(X,Y)\n",
    "# print(train_ids[12])\n",
    "print(test_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64000, 1, 27])\n"
     ]
    }
   ],
   "source": [
    "print(X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, feature_size, hidden_size, output_size):\n",
    "        # super().__init__(state_dim, act_dim, max_length=max_length)\n",
    "        super(MLP,self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc1 = nn.Linear(feature_size, hidden_size)\n",
    "        self.hidden = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.LeakyReLU = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.LeakyReLU(x)     \n",
    "        x = self.hidden(x)\n",
    "        x = self.LeakyReLU(x)\n",
    "        x = self.hidden(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.LeakyReLU(x)\n",
    "        x = self.fc2(x)           \n",
    "        # x = self.softmax(x)\n",
    "        x = F.softmax(x,dim=2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2516, 0.3610, 0.3874],\n",
      "         [0.2516, 0.3611, 0.3873],\n",
      "         [0.2516, 0.3613, 0.3871],\n",
      "         [0.2516, 0.3610, 0.3874],\n",
      "         [0.2516, 0.3609, 0.3875]]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1,5,3).to(device)\n",
    "Net = MLP(3,5,3).to(device)\n",
    "Net\n",
    "print(Net(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batchsize = 512\n",
    "# data_train_loader = DataLoader(dataset=train_ids, batch_size=Batchsize, shuffle=True)\n",
    "data_train_loader = DataLoader(dataset=train_ids, batch_size=Batchsize, shuffle=False)\n",
    "data_test_loader = DataLoader(dataset=test_ids, batch_size=1, shuffle=False,drop_last=False)\n",
    "# data_full_loader = DataLoader(dataset=data_ids, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of episode 0 = 0.22403915\n",
      "Loss of episode 1 = 0.22390714\n",
      "Loss of episode 2 = 0.2237481\n",
      "Loss of episode 3 = 0.22345045\n",
      "Loss of episode 4 = 0.22297457\n",
      "Loss of episode 5 = 0.22227372\n",
      "Loss of episode 6 = 0.22125554\n",
      "Loss of episode 7 = 0.2198283\n",
      "Loss of episode 8 = 0.21793224\n",
      "Loss of episode 9 = 0.21555623\n",
      "Loss of episode 10 = 0.21247943\n",
      "Loss of episode 11 = 0.20884413\n",
      "Loss of episode 12 = 0.20497023\n",
      "Loss of episode 13 = 0.20065372\n",
      "Loss of episode 14 = 0.19648279\n",
      "Loss of episode 15 = 0.19219653\n",
      "Loss of episode 16 = 0.18812454\n",
      "Loss of episode 17 = 0.18423101\n",
      "Loss of episode 18 = 0.18044879\n",
      "Loss of episode 19 = 0.17728887\n",
      "Loss of episode 20 = 0.17437503\n",
      "Loss of episode 21 = 0.17182884\n",
      "Loss of episode 22 = 0.16953452\n",
      "Loss of episode 23 = 0.16665435\n",
      "Loss of episode 24 = 0.16435014\n",
      "Loss of episode 25 = 0.16189373\n",
      "Loss of episode 26 = 0.15963036\n",
      "Loss of episode 27 = 0.15748814\n",
      "Loss of episode 28 = 0.15532894\n",
      "Loss of episode 29 = 0.15270907\n",
      "Loss of episode 30 = 0.15028265\n",
      "Loss of episode 31 = 0.14734098\n",
      "Loss of episode 32 = 0.14447194\n",
      "Loss of episode 33 = 0.14220028\n",
      "Loss of episode 34 = 0.14028758\n",
      "Loss of episode 35 = 0.13882951\n",
      "Loss of episode 36 = 0.13754998\n",
      "Loss of episode 37 = 0.13602306\n",
      "Loss of episode 38 = 0.13357645\n",
      "Loss of episode 39 = 0.13071916\n",
      "Loss of episode 40 = 0.1282645\n",
      "Loss of episode 41 = 0.12686695\n",
      "Loss of episode 42 = 0.12611875\n",
      "Loss of episode 43 = 0.1252921\n",
      "Loss of episode 44 = 0.12431706\n",
      "Loss of episode 45 = 0.12354176\n",
      "Loss of episode 46 = 0.12210739\n",
      "Loss of episode 47 = 0.12066151\n",
      "Loss of episode 48 = 0.12046089\n",
      "Loss of episode 49 = 0.120668255\n",
      "Loss of episode 50 = 0.11795705\n",
      "Loss of episode 51 = 0.114444435\n",
      "Loss of episode 52 = 0.11183526\n",
      "Loss of episode 53 = 0.11045824\n",
      "Loss of episode 54 = 0.10998707\n",
      "Loss of episode 55 = 0.11124036\n",
      "Loss of episode 56 = 0.11141773\n",
      "Loss of episode 57 = 0.10779213\n",
      "Loss of episode 58 = 0.10396805\n",
      "Loss of episode 59 = 0.101268806\n",
      "Loss of episode 60 = 0.098287195\n",
      "Loss of episode 61 = 0.09677352\n",
      "Loss of episode 62 = 0.095559314\n",
      "Loss of episode 63 = 0.09490049\n",
      "Loss of episode 64 = 0.09367006\n",
      "Loss of episode 65 = 0.09179296\n",
      "Loss of episode 66 = 0.091036394\n",
      "Loss of episode 67 = 0.08932645\n",
      "Loss of episode 68 = 0.08918707\n",
      "Loss of episode 69 = 0.08961756\n",
      "Loss of episode 70 = 0.09009924\n",
      "Loss of episode 71 = 0.08992677\n",
      "Loss of episode 72 = 0.08845328\n",
      "Loss of episode 73 = 0.08664332\n",
      "Loss of episode 74 = 0.08599941\n",
      "Loss of episode 75 = 0.08490903\n",
      "Loss of episode 76 = 0.08393213\n",
      "Loss of episode 77 = 0.0843695\n",
      "Loss of episode 78 = 0.084752746\n",
      "Loss of episode 79 = 0.08416477\n",
      "Loss of episode 80 = 0.08385336\n",
      "Loss of episode 81 = 0.082636446\n",
      "Loss of episode 82 = 0.080176756\n",
      "Loss of episode 83 = 0.078742005\n",
      "Loss of episode 84 = 0.0780445\n",
      "Loss of episode 85 = 0.0777702\n",
      "Loss of episode 86 = 0.076011516\n",
      "Loss of episode 87 = 0.07549191\n",
      "Loss of episode 88 = 0.07509485\n",
      "Loss of episode 89 = 0.07491204\n",
      "Loss of episode 90 = 0.07506362\n",
      "Loss of episode 91 = 0.07653504\n",
      "Loss of episode 92 = 0.07649239\n",
      "Loss of episode 93 = 0.07678485\n",
      "Loss of episode 94 = 0.07649046\n",
      "Loss of episode 95 = 0.07587209\n",
      "Loss of episode 96 = 0.07510028\n",
      "Loss of episode 97 = 0.07434798\n",
      "Loss of episode 98 = 0.07317996\n",
      "Loss of episode 99 = 0.07186106\n",
      "Loss of episode 100 = 0.06996609\n",
      "Loss of episode 101 = 0.06860935\n",
      "Loss of episode 102 = 0.067698\n",
      "Loss of episode 103 = 0.06760535\n",
      "Loss of episode 104 = 0.06802911\n",
      "Loss of episode 105 = 0.06708809\n",
      "Loss of episode 106 = 0.06655308\n",
      "Loss of episode 107 = 0.0657985\n",
      "Loss of episode 108 = 0.0656507\n",
      "Loss of episode 109 = 0.06529783\n",
      "Loss of episode 110 = 0.06532213\n",
      "Loss of episode 111 = 0.06528329\n",
      "Loss of episode 112 = 0.0653423\n",
      "Loss of episode 113 = 0.06608536\n",
      "Loss of episode 114 = 0.066364035\n",
      "Loss of episode 115 = 0.0649248\n",
      "Loss of episode 116 = 0.063777946\n",
      "Loss of episode 117 = 0.06167953\n",
      "Loss of episode 118 = 0.05998138\n",
      "Loss of episode 119 = 0.058652468\n",
      "Loss of episode 120 = 0.05817721\n",
      "Loss of episode 121 = 0.058052417\n",
      "Loss of episode 122 = 0.058000375\n",
      "Loss of episode 123 = 0.058322765\n",
      "Loss of episode 124 = 0.05881244\n",
      "Loss of episode 125 = 0.059076265\n",
      "Loss of episode 126 = 0.05965418\n",
      "Loss of episode 127 = 0.060779747\n",
      "Loss of episode 128 = 0.061407167\n",
      "Loss of episode 129 = 0.061653495\n",
      "Loss of episode 130 = 0.05981009\n",
      "Loss of episode 131 = 0.05831551\n",
      "Loss of episode 132 = 0.057457417\n",
      "Loss of episode 133 = 0.056472495\n",
      "Loss of episode 134 = 0.056464214\n",
      "Loss of episode 135 = 0.056554195\n",
      "Loss of episode 136 = 0.0562109\n",
      "Loss of episode 137 = 0.05703397\n",
      "Loss of episode 138 = 0.05679053\n",
      "Loss of episode 139 = 0.056141973\n",
      "Loss of episode 140 = 0.05571821\n",
      "Loss of episode 141 = 0.055145036\n",
      "Loss of episode 142 = 0.054727875\n",
      "Loss of episode 143 = 0.05495235\n",
      "Loss of episode 144 = 0.055088613\n",
      "Loss of episode 145 = 0.05539085\n",
      "Loss of episode 146 = 0.05558923\n",
      "Loss of episode 147 = 0.055654652\n",
      "Loss of episode 148 = 0.055802774\n",
      "Loss of episode 149 = 0.055630963\n",
      "Loss of episode 150 = 0.055458855\n",
      "Loss of episode 151 = 0.054803826\n",
      "Loss of episode 152 = 0.053852856\n",
      "Loss of episode 153 = 0.054014094\n",
      "Loss of episode 154 = 0.05377605\n",
      "Loss of episode 155 = 0.05414182\n",
      "Loss of episode 156 = 0.05443862\n",
      "Loss of episode 157 = 0.053929802\n",
      "Loss of episode 158 = 0.054200247\n",
      "Loss of episode 159 = 0.054140788\n",
      "Loss of episode 160 = 0.054746054\n",
      "Loss of episode 161 = 0.05493527\n",
      "Loss of episode 162 = 0.053878944\n",
      "Loss of episode 163 = 0.054283734\n",
      "Loss of episode 164 = 0.053973168\n",
      "Loss of episode 165 = 0.053998895\n",
      "Loss of episode 166 = 0.053197276\n",
      "Loss of episode 167 = 0.05347168\n",
      "Loss of episode 168 = 0.052640356\n",
      "Loss of episode 169 = 0.052331716\n",
      "Loss of episode 170 = 0.05159987\n",
      "Loss of episode 171 = 0.051039048\n",
      "Loss of episode 172 = 0.050074946\n",
      "Loss of episode 173 = 0.050465137\n",
      "Loss of episode 174 = 0.05011834\n",
      "Loss of episode 175 = 0.04920211\n",
      "Loss of episode 176 = 0.048865754\n",
      "Loss of episode 177 = 0.0486111\n",
      "Loss of episode 178 = 0.04775504\n",
      "Loss of episode 179 = 0.047426715\n",
      "Loss of episode 180 = 0.047193352\n",
      "Loss of episode 181 = 0.047500774\n",
      "Loss of episode 182 = 0.047620654\n",
      "Loss of episode 183 = 0.047035746\n",
      "Loss of episode 184 = 0.0471614\n",
      "Loss of episode 185 = 0.04708249\n",
      "Loss of episode 186 = 0.046504598\n",
      "Loss of episode 187 = 0.04627878\n",
      "Loss of episode 188 = 0.046535753\n",
      "Loss of episode 189 = 0.04692424\n",
      "Loss of episode 190 = 0.04604927\n",
      "Loss of episode 191 = 0.045687795\n",
      "Loss of episode 192 = 0.046218168\n",
      "Loss of episode 193 = 0.0458934\n",
      "Loss of episode 194 = 0.046803545\n",
      "Loss of episode 195 = 0.045661703\n",
      "Loss of episode 196 = 0.045619998\n",
      "Loss of episode 197 = 0.045167007\n",
      "Loss of episode 198 = 0.044890106\n",
      "Loss of episode 199 = 0.04545664\n",
      "Loss of episode 200 = 0.045046225\n",
      "Loss of episode 201 = 0.044530068\n",
      "Loss of episode 202 = 0.043595728\n",
      "Loss of episode 203 = 0.04368989\n",
      "Loss of episode 204 = 0.043329354\n",
      "Loss of episode 205 = 0.04357693\n",
      "Loss of episode 206 = 0.04334283\n",
      "Loss of episode 207 = 0.043327853\n",
      "Loss of episode 208 = 0.04304195\n",
      "Loss of episode 209 = 0.042733457\n",
      "Loss of episode 210 = 0.04240194\n",
      "Loss of episode 211 = 0.04222823\n",
      "Loss of episode 212 = 0.042014595\n",
      "Loss of episode 213 = 0.041581143\n",
      "Loss of episode 214 = 0.04197877\n",
      "Loss of episode 215 = 0.042448044\n",
      "Loss of episode 216 = 0.04194303\n",
      "Loss of episode 217 = 0.0414679\n",
      "Loss of episode 218 = 0.04213477\n",
      "Loss of episode 219 = 0.041525774\n",
      "Loss of episode 220 = 0.04205759\n",
      "Loss of episode 221 = 0.042287786\n",
      "Loss of episode 222 = 0.041635502\n",
      "Loss of episode 223 = 0.04075792\n",
      "Loss of episode 224 = 0.040847413\n",
      "Loss of episode 225 = 0.041428674\n",
      "Loss of episode 226 = 0.040987227\n",
      "Loss of episode 227 = 0.040147956\n",
      "Loss of episode 228 = 0.040007923\n",
      "Loss of episode 229 = 0.040103946\n",
      "Loss of episode 230 = 0.04032936\n",
      "Loss of episode 231 = 0.040832818\n",
      "Loss of episode 232 = 0.040227406\n",
      "Loss of episode 233 = 0.040335834\n",
      "Loss of episode 234 = 0.039427217\n",
      "Loss of episode 235 = 0.039760027\n",
      "Loss of episode 236 = 0.04026238\n",
      "Loss of episode 237 = 0.039767966\n",
      "Loss of episode 238 = 0.040038977\n",
      "Loss of episode 239 = 0.040726826\n",
      "Loss of episode 240 = 0.04068749\n",
      "Loss of episode 241 = 0.040125936\n",
      "Loss of episode 242 = 0.03969575\n",
      "Loss of episode 243 = 0.039417468\n",
      "Loss of episode 244 = 0.039856736\n",
      "Loss of episode 245 = 0.04023638\n",
      "Loss of episode 246 = 0.040538285\n",
      "Loss of episode 247 = 0.03995445\n",
      "Loss of episode 248 = 0.040410783\n",
      "Loss of episode 249 = 0.040020768\n",
      "Loss of episode 250 = 0.03941592\n",
      "Loss of episode 251 = 0.04037125\n",
      "Loss of episode 252 = 0.04012419\n",
      "Loss of episode 253 = 0.039771184\n",
      "Loss of episode 254 = 0.039658412\n",
      "Loss of episode 255 = 0.040004242\n",
      "Loss of episode 256 = 0.039344583\n",
      "Loss of episode 257 = 0.039662786\n",
      "Loss of episode 258 = 0.03894922\n",
      "Loss of episode 259 = 0.0394686\n",
      "Loss of episode 260 = 0.039576143\n",
      "Loss of episode 261 = 0.03911209\n",
      "Loss of episode 262 = 0.039289225\n",
      "Loss of episode 263 = 0.039273586\n",
      "Loss of episode 264 = 0.039732184\n",
      "Loss of episode 265 = 0.03971172\n",
      "Loss of episode 266 = 0.03966193\n",
      "Loss of episode 267 = 0.03966881\n",
      "Loss of episode 268 = 0.039711278\n",
      "Loss of episode 269 = 0.038995873\n",
      "Loss of episode 270 = 0.039220676\n",
      "Loss of episode 271 = 0.03910002\n",
      "Loss of episode 272 = 0.039540082\n",
      "Loss of episode 273 = 0.038838353\n",
      "Loss of episode 274 = 0.039873365\n",
      "Loss of episode 275 = 0.039434876\n",
      "Loss of episode 276 = 0.039583888\n",
      "Loss of episode 277 = 0.039899394\n",
      "Loss of episode 278 = 0.03988723\n",
      "Loss of episode 279 = 0.03994424\n",
      "Loss of episode 280 = 0.039736822\n",
      "Loss of episode 281 = 0.039717957\n",
      "Loss of episode 282 = 0.04027994\n",
      "Loss of episode 283 = 0.039482024\n",
      "Loss of episode 284 = 0.03865248\n",
      "Loss of episode 285 = 0.038700573\n",
      "Loss of episode 286 = 0.038984593\n",
      "Loss of episode 287 = 0.03887898\n",
      "Loss of episode 288 = 0.038488828\n",
      "Loss of episode 289 = 0.037620593\n",
      "Loss of episode 290 = 0.03841969\n",
      "Loss of episode 291 = 0.038025305\n",
      "Loss of episode 292 = 0.0386914\n",
      "Loss of episode 293 = 0.037813272\n",
      "Loss of episode 294 = 0.037857067\n",
      "Loss of episode 295 = 0.037812956\n",
      "Loss of episode 296 = 0.037752956\n",
      "Loss of episode 297 = 0.03778436\n",
      "Loss of episode 298 = 0.037056647\n",
      "Loss of episode 299 = 0.037405614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d6af481cf0>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feature_size, hidden_size, num_layers, output_size\n",
    "# feature_size, hidden_size, output_size\n",
    "Net = MLP(27,512,3).to(device)\n",
    "num_epoch = 300\n",
    "# optimizer = torch.optim.SGD(net.parameters(),lr=0.5)\n",
    "optimizer = torch.optim.Adam(Net.parameters(),lr=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "# loss_func = torch.nn.CrossEntropyLoss()\n",
    "Loss=[]\n",
    "for t in range(num_epoch):\n",
    "    aver_loss = 0\n",
    "    Net.train()\n",
    "    for batch, data in enumerate(data_train_loader):\n",
    "    # for x,y in data_train_loader:\n",
    "        x, y = data\n",
    "        # print(x.shape)\n",
    "        prediction = Net(x)\n",
    "        # prediction = torch.transpose(prediction, dim0=0, dim1=1)\n",
    "        # print(y[0])\n",
    "        # print(prediction[:,0,:].shape)\n",
    "        # print(y[:,0,:].shape)\n",
    "        loss = loss_func(prediction[:,0,:],y[:,0,:])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        aver_loss += loss\n",
    "    # print('Loss of episode %s ='%t,loss.cpu().data.numpy())\n",
    "    aver_loss /= batch\n",
    "    aver_loss=aver_loss.cpu().detach().numpy()\n",
    "    print('Loss of episode %s ='%t,aver_loss)\n",
    "    # print(t)\n",
    "    Loss.append(aver_loss)\n",
    "    # Loss.append(loss.detach().numpy())\n",
    "plt.plot(Loss,color='r')\n",
    "    # plt.show()\n",
    "    # print(loss)\n",
    "    # if t%5==0:\n",
    "    #     plt.cla()\n",
    "    #     plt.scatter(x.data.numpy(),y.data.numpy())\n",
    "    #     plt.plot(x.data.numpy(),prediction.data.numpy(),'r-',lw=5)\n",
    "    #     plt.text(0.5,0,'Loss=%.4f' % loss.data,fontdict={'size':20,'color':'red'})\n",
    "    #     plt.pause(0.1)\n",
    "# print(gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278531\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "from torchvision.models import vgg16\n",
    "device = torch.device(\"cuda\")\n",
    "# myNet = vgg16().to(device)  \n",
    "# summary(myNet, (3, 64, 64)) \n",
    "Net_test = MLP(27,512,3).to(device)\n",
    "# summary(gru_test, (8,11))\n",
    "total = sum([param.nelement() for param in Net_test.parameters()])\n",
    "print(total)\n",
    "# gru_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 0.87821875\n"
     ]
    }
   ],
   "source": [
    "Truth = []\n",
    "Pred = []\n",
    "aver_loss = 0\n",
    "total_acc = 0\n",
    "for batch, data in enumerate(data_train_loader):   \n",
    "# for batch, data in enumerate(data_full_loader):\n",
    "# for x,y in data_train_loader:\n",
    "    x, y = data\n",
    "    # print(x)\n",
    "    # prediction =net(x)\n",
    "    test = Net(x)\n",
    "    # test = torch.transpose(test, dim0=0, dim1=1)\n",
    "    # print(test)\n",
    "    # print(y)\n",
    "    # print(loss.data)\n",
    "    # print(y[0])\n",
    "    # print(test[0])\n",
    "    a = torch.argmax(test,dim = 2).cpu().data.numpy().T[0]\n",
    "    b = torch.argmax(y,dim = 2).cpu().data.numpy().T[0]\n",
    "    # print(a.shape)\n",
    "    # print(b.shape)\n",
    "    # print(a)\n",
    "    # print(sum(a==b))\n",
    "    aver_acc = sum(a==b)\n",
    "    # print(aver_acc)\n",
    "    total_acc += aver_acc\n",
    "    # print(total_acc)\n",
    "    # print(aver_acc)\n",
    "# print(batch)\n",
    "print('acc =', total_acc/(0.8*max_number_traj))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 0.586375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Loss=[]\n",
    "Truth = []\n",
    "Pred = []\n",
    "aver_loss = 0\n",
    "total_acc = 0\n",
    "Net.eval()\n",
    "for batch, data in enumerate(data_test_loader):   \n",
    "    x, y = data\n",
    "    # print(x)\n",
    "    # prediction =net(x)\n",
    "    Net.eval()\n",
    "    test = Net(x)\n",
    "    a = torch.argmax(test,dim = 2).cpu().data.numpy()[0]\n",
    "    b = torch.argmax(y,dim = 2).cpu().data.numpy()[0]\n",
    "    # print('-----------')\n",
    "    # print('pred =',a[0])\n",
    "    # print('true =',b[0])\n",
    "    aver_acc = (a[0]==b[0])\n",
    "    total_acc += aver_acc \n",
    "\n",
    "print('acc =', total_acc/(0.2*max_number_traj))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 AUC = {0: 0.7279610915182508, 1: 0.7296717852286588, 2: 0.7197705326115019}\n",
      "Average AUC = [0.72796109 0.72967179 0.71977053]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, roc_curve, auc\n",
    "n_classes = 3\n",
    "length = 1\n",
    "# number_traj = 8000\n",
    "aver_auc = np.zeros(n_classes)\n",
    "total_precision = 0\n",
    "Y_score = np.zeros(((length, int(0.2*max_number_traj), n_classes)))\n",
    "# print(Y_score.shape)\n",
    "Y_label = np.zeros(((length, int(0.2*max_number_traj), n_classes)))\n",
    "for batch, data in enumerate(data_test_loader):   \n",
    "    x, y = data\n",
    "    # print(x)\n",
    "    Net.eval()\n",
    "    test = Net(x)\n",
    "    y_score = test[0].cpu().data.numpy()\n",
    "    y_label = y[0].cpu().data.numpy()\n",
    "    # print(batch)\n",
    "    for i in range(length):\n",
    "        Y_score[i][batch] = y_score[i]\n",
    "        Y_label[i][batch] = y_label[i]\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "aver_auc = np.zeros(n_classes)\n",
    "for t in range(length):\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(Y_label[t][:, i], Y_score[t][:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        # aver_auc[i] += auc(fpr[i], tpr[i])\n",
    "        aver_auc[i] += roc_auc[i]/length\n",
    "    print('step %s AUC ='%(t+1),roc_auc) \n",
    "print('Average AUC =', aver_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aver_acc = np.zeros(length)\n",
    "# for batch, data in enumerate(data_test_loader):   \n",
    "# # for batch, data in enumerate(data_full_loader):\n",
    "# # for x,y in data_train_loader:\n",
    "#     x, y = data\n",
    "#     # print(x)\n",
    "#     # prediction =net(x)\n",
    "#     test = gru(x)\n",
    "#     # print(test)\n",
    "#     # print(y)\n",
    "#     a = torch.argmax(test,dim = 2).cpu().data.numpy()\n",
    "#     b = torch.argmax(y,dim = 2).cpu().data.numpy()\n",
    "#     # print('-----------')\n",
    "#     # print('predict =',a.T[0])\n",
    "#     # print('true =',b[0])\n",
    "#     for i in range(length):\n",
    "#         aver_acc[i] += a.T[0][i]==b[0][i]\n",
    "# aver_acc /= batch\n",
    "# print('step acc =', aver_acc)\n",
    "# print('aver acc =', sum(aver_acc)/length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: F1 = 0.5862 Precision = 0.5863 Recall = 0.5862\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "\n",
    "# print(y_pred)\n",
    "# print(y_true)\n",
    "for i in range(length):\n",
    "    y_true = np.argmax(Y_label[i],axis = 1)\n",
    "    y_pred = np.argmax(Y_score[i],axis = 1)\n",
    "    f1 = round(f1_score(y_true, y_pred, average='macro' ),4)\n",
    "    p = round(precision_score(y_true, y_pred, average='macro'),4)\n",
    "    r = round(recall_score(y_true, y_pred, average='macro'),4)\n",
    "\n",
    "    print('Step %s:'%(i+1),'F1 =', f1, 'Precision =', p, 'Recall =', r)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
